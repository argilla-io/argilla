{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will show a standard workflow for a image classification task, in this case, using transformers, sentence-transformers and Argilla.\n",
    "\n",
    "We will follow these steps:\n",
    "\n",
    "* Configure the Argilla dataset\n",
    "* Add initial model suggestions\n",
    "* Add vectors for image search\n",
    "* Evaluate with Argilla\n",
    "* Train your model\n",
    "* Update the suggestions with the new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Argilla server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have deployed Argilla, you can skip this step. Otherwise, you can quickly deploy Argilla following [this guide](../getting_started/quickstart.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this tutorial, you need to install the Argilla SDK and a few third-party libraries via `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install argilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers>=3 transformers==4.40.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "import base64\n",
    "import io\n",
    "from datasets import load_dataset, Dataset, Image\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also need to connect to the Argilla server using the `api_url` and `api_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with your url if using Docker\n",
    "# Replace api_key if you configured a custom API key\n",
    "# Uncomment the last line and set your HF_TOKEN if your space is private\n",
    "client = rg.Argilla(\n",
    "    # api_url=\"https://[your-owner-name]-[your_space_name].hf.space\",\n",
    "    # api_url=\n",
    "    api_key=\"argilla.apikey\",\n",
    "    # headers={\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and create the Argilla dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will need to configure the dataset. In the settings, we can specify the guidelines, fields, and questions. If needed, you can also add metadata and vectors. However, for our use case, we just need a text field and a label question.\n",
    "\n",
    "!!! note\n",
    "    Check this [how-to guide](../how_to_guides/dataset.md) to know more about configuring and creating a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [str(x) for x in range(10)]\n",
    "\n",
    "settings = rg.Settings(\n",
    "    guidelines=\"The goal of this task is to classify a given image of a handwritten digit into one of 10 classes representing integer values from 0 to 9, inclusively.\",\n",
    "    fields=[\n",
    "        rg.ImageField(\n",
    "            name=\"image\",\n",
    "            title=\"An image of a handwritten digit.\",\n",
    "        ),\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.LabelQuestion(\n",
    "            name=\"image_label\",\n",
    "            title=\"What digit do you see on the image?\",\n",
    "            labels=labels,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the dataset with the name and the defined settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidberenstein/Documents/programming/argilla/argilla/argilla/src/argilla/datasets/_resource.py:203: UserWarning: Workspace not provided. Using default workspace: argilla id: fcdac196-1f8c-4634-bbc3-d6e490fcb481\n",
      "  warnings.warn(f\"Workspace not provided. Using default workspace: {workspace.name} id: {workspace.id}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(id=UUID('706d637a-9216-4c96-b28c-b273d6f1680f') inserted_at=datetime.datetime(2024, 8, 13, 15, 18, 28, 53048) updated_at=datetime.datetime(2024, 8, 13, 15, 18, 28, 221716) name='image_classification_datasets' status='ready' guidelines='The goal of this task is to classify a given image of a handwritten digit into one of 10 classes representing integer values from 0 to 9, inclusively.' allow_extra_metadata=False distribution=OverlapTaskDistributionModel(strategy='overlap', min_submitted=1) workspace_id=UUID('fcdac196-1f8c-4634-bbc3-d6e490fcb481') last_activity_at=datetime.datetime(2024, 8, 13, 15, 18, 28, 221716))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = rg.Dataset(\n",
    "    name=\"image_classification_dataset\",\n",
    "    settings=settings,\n",
    ")\n",
    "dataset.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we have created the dataset, it still lacks the information to be annotated (you can check it in the UI). We will use the `ylecun/mnist` dataset from [the Hugging Face Hub](https://huggingface.co/datasets/ylecun/mnist). Specifically, we will use the `train` split and get `100` examples. \n",
    "\n",
    "!!! tip\n",
    "    When working with Hugging Face dataset you can set `Image(decode=False)` so that we can get [public image URLs](https://huggingface.co/docs/datasets/en/image_load#local-files), however, this depends on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset = load_dataset(\"ylecun/mnist\", split=\"train[:100]\")\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       " 'label': 5}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can seem, the image is a 28x28 grayscale image. In order to use it in in Argilla, we need to convert it to a base64 string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1203.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       " 'label': 5,\n",
       " 'image_data_uri': 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg=='}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pil_to_data_uri(batch):\n",
    "    data_uri = []\n",
    "    for image in batch[\"image\"]:\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "        data_uri.append(f\"data:image/png;base64,{img_str}\")\n",
    "    batch[\"image_data_uri\"] = data_uri\n",
    "    return batch\n",
    "\n",
    "hf_dataset= hf_dataset.map(pil_to_data_uri, batched=True)\n",
    "hf_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will easily add them to the dataset using `log` and the mapping, where we indicate that the column `text` is the data that should be added to the field `review`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecordsIngestionError",
     "evalue": "Argilla SDK error: RecordsIngestionError: Mapped attribute is not a valid dataset attribute: image.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecordsIngestionError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_data_uri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/programming/argilla/argilla/argilla/src/argilla/records/_dataset_records.py:228\u001b[0m, in \u001b[0;36mDatasetRecords.log\u001b[0;34m(self, records, mapping, user_id, batch_size)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog\u001b[39m(\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    206\u001b[0m     records: Union[List[\u001b[38;5;28mdict\u001b[39m], List[Record], HFDataset],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_BATCH_SIZE,\n\u001b[1;32m    210\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetRecords\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add or update records in a dataset on the server using the provided records.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    If the record includes a known `id` field, the record will be updated.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    If the record does not include a known `id` field, the record will be added as a new record.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m        A list of Record objects representing the updated records.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     record_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ingest_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mme\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_batch_size(\n\u001b[1;32m    230\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    231\u001b[0m         records_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(record_models),\n\u001b[1;32m    232\u001b[0m         max_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api\u001b[38;5;241m.\u001b[39mMAX_RECORDS_PER_UPSERT_BULK,\n\u001b[1;32m    233\u001b[0m     )\n\u001b[1;32m    235\u001b[0m     created_or_updated \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/programming/argilla/argilla/argilla/src/argilla/records/_dataset_records.py:390\u001b[0m, in \u001b[0;36mDatasetRecords._ingest_records\u001b[0;34m(self, records, mapping, user_id)\u001b[0m\n\u001b[1;32m    387\u001b[0m     records \u001b[38;5;241m=\u001b[39m HFDatasetsIO\u001b[38;5;241m.\u001b[39m_record_dicts_from_datasets(dataset\u001b[38;5;241m=\u001b[39mrecords)\n\u001b[1;32m    389\u001b[0m ingested_records \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 390\u001b[0m record_mapper \u001b[38;5;241m=\u001b[39m \u001b[43mIngestedRecordMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/programming/argilla/argilla/argilla/src/argilla/records/_mapping/_mapper.py:66\u001b[0m, in \u001b[0;36mIngestedRecordMapper.__init__\u001b[0;34m(self, dataset, user_id, mapping)\u001b[0m\n\u001b[1;32m     64\u001b[0m mapping \u001b[38;5;241m=\u001b[39m mapping \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m     65\u001b[0m default_mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schematize_default_attributes()\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schematize_mapped_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_mapping\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/programming/argilla/argilla/argilla/src/argilla/records/_mapping/_mapper.py:145\u001b[0m, in \u001b[0;36mIngestedRecordMapper._schematize_mapped_attributes\u001b[0;34m(self, mapping, default_mapping)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m             attr_route \u001b[38;5;241m=\u001b[39m AttributeRoute(\n\u001b[1;32m    140\u001b[0m                 name\u001b[38;5;241m=\u001b[39mattr_name,\n\u001b[1;32m    141\u001b[0m                 source\u001b[38;5;241m=\u001b[39msource_key,\n\u001b[1;32m    142\u001b[0m                 \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mattr_type,\n\u001b[1;32m    143\u001b[0m                 parameters\u001b[38;5;241m=\u001b[39m[parameter],\n\u001b[1;32m    144\u001b[0m             )\n\u001b[0;32m--> 145\u001b[0m             attr_route \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_attribute_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattribute_route\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattr_route\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m             default_mapping\u001b[38;5;241m.\u001b[39madd_route(attr_route)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default_mapping\n",
      "File \u001b[0;32m~/Documents/programming/argilla/argilla/argilla/src/argilla/records/_mapping/_mapper.py:196\u001b[0m, in \u001b[0;36mIngestedRecordMapper._select_attribute_type\u001b[0;34m(self, attribute_route)\u001b[0m\n\u001b[1;32m    194\u001b[0m     attribute_route\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m=\u001b[39m AttributeType\u001b[38;5;241m.\u001b[39mID\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RecordsIngestionError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMapped attribute is not a valid dataset attribute: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattribute_route\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attribute_route\n",
      "\u001b[0;31mRecordsIngestionError\u001b[0m: Argilla SDK error: RecordsIngestionError: Mapped attribute is not a valid dataset attribute: image."
     ]
    }
   ],
   "source": [
    "dataset.records.log(records=hf_dataset, mapping={\"image_data_uri\": \"image\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add initial model suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to add suggestions to the dataset. This will make things easier and faster for the annotation team. Suggestions will appear as preselected options, so annotators will only need to correct them. In our case, we will generate them using a zero-shot SetFit model. However, you can use a framework or technique of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by defining an example training set with the required labels: `positive` and `negative`. Using `get_templated_dataset` will create sentences from the default template: \"This sentence is {label}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_ds = get_templated_dataset(\n",
    "    candidate_labels=labels,\n",
    "    sample_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will prepare a function to train the SetFit model.\n",
    "\n",
    "!!! note\n",
    "    For further customization, you can check the [SetFit documentation](https://huggingface.co/docs/setfit/reference/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, dataset):\n",
    "    model = SetFitModel.from_pretrained(model_name)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model. We will use `TaylorAI/bge-micro-v2`, available in the [Hugging Face Hub](https://huggingface.co/TaylorAI/bge-micro-v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model_name=\"TaylorAI/bge-micro-v2\", dataset=zero_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save it locally or push it to the Hub. And then, load it from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load locally\n",
    "# model.save_pretrained(\"text_classification_model\")\n",
    "# model = SetFitModel.from_pretrained(\"text_classification_model\")\n",
    "\n",
    "# Push and load in HF\n",
    "# model.push_to_hub(\"[username]/text_classification_model\")\n",
    "# model = SetFitModel.from_pretrained(\"[username]/text_classification_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to make the predictions! We will set a function that uses the `predict` method to get the suggested label. The model will infer the label based on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input, labels):\n",
    "    model.labels = labels\n",
    "\n",
    "    prediction = model.predict([input])\n",
    "\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the records, we will need to retrieve them from the server and update them with the new suggestions. The `id` will always need to be provided as it is the records' identifier to update a record and avoid creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.records.to_list(flatten=True)\n",
    "updated_data = [\n",
    "    {\n",
    "        \"sentiment_label\": predict(model, sample[\"review\"], labels),\n",
    "        \"id\": sample[\"id\"],\n",
    "    }\n",
    "    for sample in data\n",
    "]\n",
    "dataset.records.log(records=updated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà! We have added the suggestions to the dataset, and they will appear in the UI marked with a ✨. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Argilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start the annotation process. Just open the dataset in the Argilla UI and start annotating the records. If the suggestions are correct, you can just click on `Submit`. Otherwise, you can select the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note\n",
    "    Check this [how-to guide](../how_to_guides/annotate.md) to know more about annotating in the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the annotation, we will have a robust dataset to train the main model. In our case, we will fine-tune using SetFit. However, you can select the one that best fits your requirements. So, let's start by retrieving the annotated records.\n",
    "\n",
    "!!! note\n",
    "    Check this [how-to guide](../how_to_guides/query.md) to know more about filtering and querying in Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.datasets(\"text_classification_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_filter = rg.Query(filter=rg.Filter((\"response.status\", \"==\", \"submitted\")))\n",
    "\n",
    "submitted = dataset.records(status_filter).to_list(flatten=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a single response per record, we can retrieve the selected label straightforwardly and create the training set with 8 samples per label. We selected 8 samples per label to have a balanced dataset for few-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = [\n",
    "    {\n",
    "        \"text\": r[\"review\"],\n",
    "        \"label\": r[\"sentiment_label.responses\"][0],\n",
    "    }\n",
    "    for r in submitted\n",
    "]\n",
    "train_dataset = Dataset.from_list(train_records)\n",
    "train_dataset = sample_dataset(train_dataset, label_column=\"label\", num_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the model using our previous function, but this time with a high-quality human-annotated training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model_name=\"TaylorAI/bge-micro-v2\", dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the training data had a better-quality, we can expect a better model. So, we can update the remaining non-annotated records with the new model's suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.records.to_list(flatten=True)\n",
    "updated_data = [\n",
    "    {\n",
    "        \"sentiment_label\": predict(model, sample[\"review\"], labels),\n",
    "        \"id\": sample[\"id\"],\n",
    "    }\n",
    "    for sample in data\n",
    "]\n",
    "dataset.records.log(records=updated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we present an end-to-end example of a text classification task. This serves as the base, but it can be performed iteratively and seamlessly integrated into your workflow to ensure high-quality curation of your data and improved results.\n",
    "\n",
    "We started by configuring the dataset, adding records, and training a zero-shot SetFit model, as an example, to add suggestions. After the annotation process, we trained a new model with the annotated data and updated the remaining records with the new suggestions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
