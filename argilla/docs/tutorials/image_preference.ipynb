{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image preference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Goal**: Show a standard workflow for working with complex multi-modal preference datasets like image question answering.\n",
    "- **Dataset**: [RLAIF-V](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset), large-scale multimodal feedback dataset. The dataset provides high-quality feedback with a total number of 83,132 preference pairs for image question answering.\n",
    "- **Libraries**: [datasets](https://github.com/huggingface/datasets), [sentence-transformers](https://github.com/UKPLab/sentence-transformers)\n",
    "- **Components**: [ImageField](https://docs.argilla.io/latest/reference/argilla/settings/fields/#src.argilla.settings._field.ImageField), [TextQuestion](https://docs.argilla.io/latest/reference/argilla/settings/questions/#src.argilla.settings._question.TextQuestion), [VectorField](https://docs.argilla.io/dev/reference/argilla/settings/vectors/#rgvectorfield), [TermsMetadataProperty](https://docs.argilla.io/dev/reference/argilla/settings/metadata_property/?h=#rgtermsmetadataproperty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Argilla server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have deployed Argilla, you can skip this step. Otherwise, you can quickly deploy Argilla following [this guide](../getting_started/quickstart.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this tutorial, you need to install the Argilla SDK and a few third-party libraries via `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install argilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sentence-transformers>3,<4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "import re\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from PIL import Image\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import pipeline, AutoImageProcessor, AutoModelForImageClassification, Trainer, TrainingArguments, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also need to connect to the Argilla server using the `api_url` and `api_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with your url if using Docker\n",
    "# Replace api_key if you configured a custom API key\n",
    "# Uncomment the last line and set your HF_TOKEN if your space is private\n",
    "client = rg.Argilla(\n",
    "    # api_url=\"https://[your-owner-name]-[your_space_name].hf.space\",\n",
    "    # api_url=\n",
    "    api_key=\"argilla.apikey\",\n",
    "    # headers={\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vibe check the dataset\n",
    "\n",
    "We will have a look at [the dataset](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset) to understand its structure and the kind of data it contains. We do this by using [the embedded Hugging Face Dataset Viewer](https://huggingface.co/docs/hub/main/en/datasets-viewer-embed).\n",
    "\n",
    "<iframe\n",
    "  src=\"https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset/embed/viewer/default/train\"\n",
    "  frameborder=\"0\"\n",
    "  width=\"100%\"\n",
    "  height=\"560px\"\n",
    "></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and create the Argilla dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will need to configure the dataset. In the settings, we can specify the guidelines, fields, and questions. Because of the complexity of the task we will also add metadata and vectors. We will add one vector to represent the semantic meaning of the `question`. We will be adding two metadata fields that store the `original_dataset` and the type of task the question belongs to which can be obtained from `origin_split`. \n",
    "\n",
    "!!! note\n",
    "    Check this [how-to guide](../how_to_guides/dataset.md) to know more about configuring and creating a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = rg.Settings(\n",
    "    guidelines=\"The goal is to assess if the answers are correct and update them where needed.\",\n",
    "    fields=[\n",
    "        rg.ImageField(\n",
    "            name=\"image\",\n",
    "            title=\"An image of a certain object, state or action.\",\n",
    "        ),\n",
    "        rg.TextField(\n",
    "            name=\"question\",\n",
    "            title=\"A question about the image, intended to be answered.\",\n",
    "        )\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.TextQuestion(\n",
    "            name=\"chosen\",\n",
    "            title=\"The chosen answer to the question.\",\n",
    "        ),\n",
    "        rg.TextQuestion(\n",
    "            name=\"rejected\",\n",
    "            title=\"The rejected answer to the question.\",\n",
    "        )\n",
    "    ],\n",
    "    metadata=[\n",
    "        rg.TermsMetadataProperty(name=\"origin_dataset\", title=\"Origin dataset\"),\n",
    "        rg.TermsMetadataProperty(name=\"task_type\", title=\"Task type\"),\n",
    "    ],\n",
    "    vectors=[\n",
    "        rg.VectorField(name=\"question_vector\", dimensions=384),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the dataset with the name and the defined settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.Dataset(\n",
    "    name=\"image_preference_dataset\",\n",
    "    settings=settings,\n",
    ")\n",
    "dataset.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we have created the dataset, it still lacks the information to be annotated (you can check it in the UI). We will use the `openbmb/RLAIF-V-Dataset` dataset from [the Hugging Face Hub](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset). Specifically, we will use the `train` split and get `100` examples. Because we are dealing with a large dataset, we will set `streaming=True` to avoid loading the entire dataset into memorym and iterate over the data to lazily load it.\n",
    "\n",
    "!!! tip\n",
    "    When working with Hugging Face dataset you can set `Image(decode=False)` so that we can get [public image URLs](https://huggingface.co/docs/datasets/en/image_load#local-files), however, this depends on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 100\n",
    "hf_dataset = load_dataset(\"openbmb/RLAIF-V-Dataset\", streaming=True)\n",
    "dataset_rows = []\n",
    "count = 0\n",
    "for row in hf_dataset[\"train\"]:\n",
    "    dataset_rows.append(row)\n",
    "    count += 1\n",
    "    if count >= n_rows:\n",
    "        break\n",
    "dataset_rows\n",
    "hf_dataset = Dataset.from_list(dataset_rows)\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ds_name': 'RLAIF-V',\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>,\n",
       " 'question': 'Who is more likely to use these tools a leather crafter or a paper crafter?',\n",
       " 'chosen': 'A leather crafter is more likely to use these tools. The image shows various crafting tools, including scissors and a hole punch, which are commonly used in leatherworking projects. Leather is a material that requires cutting, shaping, and precise hole-punching techniques to create desired designs or patterns. In contrast, paper crafters typically use different types of tools, such as adhesives, decorative papers, or specialized cutting machines like the Silhouette Cameo, for their projects.',\n",
       " 'rejected': 'A leather crafter is more likely to use these tools as they consist of a hole punch, scissors, and a knife. These items are typically used in crafting projects involving fabric or leather materials for various designs and patterns. Paper crafters may also benefit from some of these tools, but their primary focus would be on paper-related projects, which might require different types of tools such as paper cutters or scrapbooking supplies.',\n",
       " 'origin_dataset': 'OK-VQA',\n",
       " 'origin_split': '{\"model\": \"OmniLMM-12B\", \"feedback_model\": \"OmniLMM-12B\", \"type\": \"question_answering\"}',\n",
       " 'idx': 'OmniLMM-12B_OmniLMM-12B_0',\n",
       " 'image_path': 'coco2017/train2017/000000489771.jpg'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert PIL image to base64\n",
    "\n",
    "As we can see, the image is a PIL Image. In order to use it in in Argilla, we need to convert them to a base64 string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_data_uri(batch):\n",
    "    data_uri = []\n",
    "    for image in batch[\"image\"]:\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "        data_uri.append(f\"data:image/png;base64,{img_str}\")\n",
    "    batch[\"image_data_uri\"] = data_uri\n",
    "    return batch\n",
    "\n",
    "hf_dataset_with_base64= hf_dataset.map(pil_to_data_uri, batched=True)\n",
    "hf_dataset_with_base64[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve values nested in JSON\n",
    "\n",
    "The question type values are nested in a JSON object. We can obtain them by looping through the data and getting the `origin_split` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_type_from_json(batch):\n",
    "    loaded_json = [json.loads(x) for x in batch[\"origin_split\"]]\n",
    "    batch[\"task_type\"] = [x[\"type\"] for x in loaded_json]\n",
    "    return batch\n",
    "\n",
    "hf_dataset_with_base64_task= hf_dataset_with_base64.map(retrieve_type_from_json, batched=True)\n",
    "hf_dataset_with_base64_task[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vectors\n",
    "\n",
    "We will use the `sentence-transformers` library to create vectors for the questions. We will use the `TaylorAI/bge-micro-v2` model which strikes a good balance between speed and performance. Note that we also need to convert the vectort to a `list` to store it in the Argilla dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"TaylorAI/bge-micro-v2\")\n",
    "\n",
    "def encode_questions(batch):\n",
    "    vectors_as_numpy = model.encode(batch[\"question\"])\n",
    "    batch[\"question_vector\"] = [x.tolist() for x in vectors_as_numpy]\n",
    "    return batch\n",
    "\n",
    "hf_dataset_with_base64_task_vectors = hf_dataset_with_base64_task.map(encode_questions, batched=True)\n",
    "hf_dataset_with_base64_task_vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log into Argilla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will easily add them to the dataset using `log` and the mapping, where we indicate that the column `text` is the data that should be added to the field `review`. We are also adding an \"id\" column to the record, so we can easily backtrack the record to the external data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = hf_dataset.add_column(\"id\", range(len(hf_dataset)))\n",
    "dataset.records.log(records=hf_dataset[:100], mapping={\n",
    "    \"image_data_uri\": \"image\",\n",
    "    \"idx\": \"id\",\n",
    "    \"question\": \"question\",\n",
    "    \"chosen\": \"chosen\",\n",
    "    \"rejected\": \"rejected\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà! We have added the suggestions to the dataset, and they will appear in the UI marked with a ✨. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Argilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start the annotation process. Just open the dataset in the Argilla UI and start annotating the records. If the suggestions are correct, you can just click on `Submit`. Otherwise, you can select the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note\n",
    "    Check this [how-to guide](../how_to_guides/annotate.md) to know more about annotating in the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the annotation, we will have a robust dataset to train the main model. In our case, we will fine-tune using transformers and the . However, you can select the one that best fits your requirements. So, let's start by retrieving the annotated records.\n",
    "\n",
    "!!! note\n",
    "    Check this [how-to guide](../how_to_guides/query.md) to know more about filtering and querying in Argilla. Also, you can check the Hugging Face docs on [fine-tuning an image classification model](https://huggingface.co/docs/transformers/en/tasks/image_classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.datasets(\"image_classification_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_filter = rg.Query(filter=rg.Filter((\"response.status\", \"==\", \"submitted\")))\n",
    "\n",
    "submitted = dataset.records(status_filter).to_list(flatten=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to convert our base64 images to a format that the model can understand so we will convert them to PIL images again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base64_to_pil(base64_string):\n",
    "    image_data = re.sub('^data:image/.+;base64,', '', base64_string)\n",
    "    image = Image.open(io.BytesIO(base64.b64decode(image_data)))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply that to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted_pil_image = [\n",
    "    {\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"image\": base64_to_pil(sample[\"image\"]),\n",
    "        \"label\": sample[\"image_label.responses\"][0],\n",
    "    }\n",
    "    for sample in submitted\n",
    "]\n",
    "submitted_pil_image[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to ensure our images are forwarded with the correct dimensions. Because the original MNIST dataset is greyscale and the VIT model expects RGB, we need to add a channel dimension to the images. We will do this by stacking the images along the channel axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greyscale_to_rgb(img) -> Image:\n",
    "    return Image.merge('RGB', (img, img, img))\n",
    "\n",
    "submitted_pil_image_rgb = [\n",
    "    {\n",
    "        \"image\": greyscale_to_rgb(sample[\"image\"]),\n",
    "        \"label\": sample[\"label\"],\n",
    "    }\n",
    "    for sample in submitted_pil_image\n",
    "]\n",
    "submitted_pil_image_rgb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the `ImageProcessor` for fine-tuning the model. This processor will handle the image resizing and normalization in order to be compatible with the model we intend to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "submitted_pil_image_rgb_processed = [\n",
    "    {\n",
    "        \"pixel_values\": processor(sample[\"image\"], return_tensors='pt')[\"pixel_values\"],\n",
    "        \"label\": sample[\"label\"],\n",
    "    }\n",
    "    for sample in submitted_pil_image_rgb\n",
    "]\n",
    "submitted_pil_image_rgb_processed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now convert the images to a Hugging Face datasets Dataset that is ready for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ds = Dataset.from_list(submitted_pil_image_rgb_processed)\n",
    "prepared_ds = prepared_ds.train_test_split(test_size=0.2)\n",
    "prepared_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to define our data collator, which will ensure the data is unpacked and stacked correctly for the model. We wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([torch.tensor(x['pixel_values'][0]) for x in batch]),\n",
    "        'labels': torch.tensor([int(x['label']) for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define our training metrics. We will use the accuracy metric to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load our model and configure the labels that we will use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label={int(i): int(c) for i, c in enumerate(labels)},\n",
    "    label2id={int(c): int(i) for i, c in enumerate(labels)}\n",
    ")\n",
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the training arguments and start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./image-classifier\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=1,\n",
    "  fp16=False, # True if you have a GPU with mixed precision support\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=True,\n",
    "  push_to_hub=False,\n",
    "  load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    "    tokenizer=processor,\n",
    ")\n",
    "\n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the training data had a better-quality, we can expect a better model. So, we can update the remainder of our original dataset with the new model's suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"image-classification\", model=model, image_processor=processor)\n",
    "\n",
    "def run_inference(batch):\n",
    "    predictions = pipe(batch[\"image\"])\n",
    "    batch[\"image_label\"] = [prediction[0][\"label\"] for prediction in predictions]\n",
    "    batch[\"image_label.score\"] = [prediction[0][\"score\"] for prediction in predictions]\n",
    "    return batch\n",
    "\n",
    "hf_dataset = hf_dataset.map(run_inference, batched=True)\n",
    "dataset.records.log(records=hf_dataset[:100], mapping={\"image_data_uri\": \"image\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we present an end-to-end example of a image classification task. This serves as the base, but it can be performed iteratively and seamlessly integrated into your workflow to ensure high-quality curation of your data and improved results.\n",
    "\n",
    "We started by configuring the dataset, adding records, as an example, to add suggestions. After the annotation process, we trained a new model with the annotated data and updated the remaining records with the new suggestions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
