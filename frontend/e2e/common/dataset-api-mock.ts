import { Page } from "@playwright/test";
export interface DatasetData {
  datasetId: string;
  workspaceId: string;
}

const fakeDateMonthAgo = (monthAgo) => {
  const now = new Date();
  now.setHours(0, 0, 0, 0);

  now.setMonth(now.getMonth() - monthAgo);

  return now.toISOString();
};

const oldDatasets = [
  {
    tags: {},
    metadata: {},
    name: "span_marker_conll_8_epochs_corr_annot",
    task: "TokenClassification",
    workspace: "argilla",
    id: "argilla.span_marker_conll_8_epochs_corr_annot",
    owner: "argilla",
    created_at: fakeDateMonthAgo(3),
    created_by: "argilla",
    last_updated: fakeDateMonthAgo(3),
  },
  {
    tags: {
      description:
        "This dataset contains text2text records with 10 predictions",
    },
    metadata: {},
    name: "text2text-10-predictions",
    task: "Text2Text",
    workspace: "recognai",
    id: "recognai.text2text-10-predictions",
    owner: "recognai",
    created_at: fakeDateMonthAgo(6),
    created_by: "recognai",
    last_updated: fakeDateMonthAgo(3),
  },
  {
    tags: {},
    metadata: {},
    name: "settings_textclass_with_labels",
    task: "TextClassification",
    workspace: "recognai",
    id: "recognai.settings_textclass_with_labels",
    owner: "recognai",
    created_at: fakeDateMonthAgo(6),
    created_by: "recognai",
    last_updated: fakeDateMonthAgo(6),
  },
];

export const newDatasetsMocked = [
  {
    id: "ff0046f3-5098-40e6-80e1-58945faa185c",
    name: "feedback-dataset-1",
    guidelines:
      "This dataset collects human feedback on replies generated by an LLM. It is based on prompts and responses from the Open Assistant dataset.\nQuestion 1: Select your preferred reply\nSelect which reply you like best. Click 1 for reply_1, click 2 for reply_2.\nQuestion 2: Rate the overall quality of reply no.1\nCheck if reply_1 has typos or errors and select your score, 1 being very bad and 5 being very good.\nQuestion 3: Rate the helpfulness of reply no.1\nCheck if reply_1 is clear and offers a real solution to the prompt and select your score from 1 to 5, where 1 is not helpful and 5 very helpful. \nQuestion 4: Rate the harmlessness of reply no.1\nCheck if reply_1 has any content that could be considered offensive, violent or dangerous. Then select your score, 1 being very harmful and 5 being harmless.\nQuestion 5: Propose a correction for reply no.1 (optional)\nIf needed, provide a corrected or alternative text for reply_1\nQuestion 6: Rate the overall quality of reply no.2\nCheck if reply_2 has typos or errors and select your score, 1 being very bad and 5 being very good.\nQuestion 7: Rate the helpfulness of reply no.2\nCheck if reply_2 is clear and offers a real solution to the prompt and select your score from 1 to 5, where 1 is not helpful and 5 very helpful. \nQuestion 8: Rate the harmlessness of reply no.2\nCheck if reply_2 has any content that could be considered offensive, violent or dangerous. Then select your score, 1 being very harmful and 5 being harmless.\nQuestion 9: Propose a correction for reply no.2 (optional)\nIf needed, provide a corrected or alternative text for reply_2\nQuestion 10: Comments (optional)\nLeave any comments you may have about this record",
    status: "ready",
    workspace_id: "4e70e21a-7533-41e9-8a25-11d6ee3091be",
    inserted_at: fakeDateMonthAgo(4),
    updated_at: fakeDateMonthAgo(4),
    last_activity_at: fakeDateMonthAgo(4),
  },
  {
    id: "25073901-a24d-4416-b331-71a497b38063",
    name: "feedback-dataset-2",
    guidelines:
      "This dataset collects human feedback on replies generated by an LLM. It is based on prompts and responses from the Open Assistant dataset.\nQuestion 1: Select your preferred reply\nSelect which reply you like best. Click 1 for reply_1, click 2 for reply_2.\nQuestion 2: Rate the overall quality of reply no.1\nCheck if reply_1 has typos or errors and select your score, 1 being very bad and 5 being very good.\nQuestion 3: Rate the helpfulness of reply no.1\nCheck if reply_1 is clear and offers a real solution to the prompt and select your score from 1 to 5, where 1 is not helpful and 5 very helpful. \nQuestion 4: Rate the harmlessness of reply no.1\nCheck if reply_1 has any content that could be considered offensive, violent or dangerous. Then select your score, 1 being very harmful and 5 being harmless.\nQuestion 5: Propose a correction for reply no.1 (optional)\nIf needed, provide a corrected or alternative text for reply_1\nQuestion 6: Rate the overall quality of reply no.2\nCheck if reply_2 has typos or errors and select your score, 1 being very bad and 5 being very good.\nQuestion 7: Rate the helpfulness of reply no.2\nCheck if reply_2 is clear and offers a real solution to the prompt and select your score from 1 to 5, where 1 is not helpful and 5 very helpful. \nQuestion 8: Rate the harmlessness of reply no.2\nCheck if reply_2 has any content that could be considered offensive, violent or dangerous. Then select your score, 1 being very harmful and 5 being harmless.\nQuestion 9: Propose a correction for reply no.2 (optional)\nIf needed, provide a corrected or alternative text for reply_2\nQuestion 10: Comments (optional)\nLeave any comments you may have about this record",
    status: "ready",
    workspace_id: "4e70e21a-7533-41e9-8a25-11d6ee3091be",
    inserted_at: fakeDateMonthAgo(4),
    updated_at: fakeDateMonthAgo(4),
    last_activity_at: fakeDateMonthAgo(4),
  },
  {
    id: "25073901-a24d-4416-b331-71a497b38065",
    name: "feedback-dataset-3",
    guidelines:
      "This dataset collects human feedback on replies generated by an LLM. It is based on prompts and responses from the Open Assistant dataset.\nQuestion 1: Select your preferred reply\nSelect which reply you like best. Click 1 for reply_1, click 2 for reply_2.\nQuestion 2: Rate the overall quality of reply no.1\nCheck if reply_1 has typos or errors and select your score, 1 being very bad and 5 being very good.\nQuestion 3: Rate the helpfulness of reply no.1\nCheck if reply_1 is clear and offers a real solution to the prompt and select your score from 1 to 5, where 1 is not helpful and 5 very helpful. \nQuestion 4: Rate the harmlessness of reply no.1\nCheck if reply_1 has any content that could be considered offensive, violent or dangerous. Then select your score, 1 being very harmful and 5 being harmless.\nQuestion 5: Propose a correction for reply no.1 (optional)\nIf needed, provide a corrected or alternative text for reply_1\nQuestion 6: Rate the overall quality of reply no.2\nCheck if reply_2 has typos or errors and select your score, 1 being very bad and 5 being very good.\nQuestion 7: Rate the helpfulness of reply no.2\nCheck if reply_2 is clear and offers a real solution to the prompt and select your score from 1 to 5, where 1 is not helpful and 5 very helpful. \nQuestion 8: Rate the harmlessness of reply no.2\nCheck if reply_2 has any content that could be considered offensive, violent or dangerous. Then select your score, 1 being very harmful and 5 being harmless.\nQuestion 9: Propose a correction for reply no.2 (optional)\nIf needed, provide a corrected or alternative text for reply_2\nQuestion 10: Comments (optional)\nLeave any comments you may have about this record",
    status: "ready",
    workspace_id: "4e70e21a-7533-41e9-8a25-11d6ee3091bd",
    inserted_at: fakeDateMonthAgo(4),
    updated_at: fakeDateMonthAgo(4),
    last_activity_at: fakeDateMonthAgo(4),
  },
];

export const workspacesMocked = [
  {
    id: "4e70e21a-7533-41e9-8a25-11d6ee3091be",
    name: "argilla",
    inserted_at: fakeDateMonthAgo(1),
    updated_at: fakeDateMonthAgo(1),
  },
  {
    id: "9c14ca14-65bb-4c27-ba1f-cf7e4a6398e8",
    name: "recognai",
    inserted_at: fakeDateMonthAgo(1),
    updated_at: fakeDateMonthAgo(1),
  },
  {
    id: "4e70e21a-7533-41e9-8a25-11d6ee3091bd",
    name: "argilla-other",
    inserted_at: fakeDateMonthAgo(1),
    updated_at: fakeDateMonthAgo(1),
  },
];

export const mockWithEmptyDatasets = async (page: Page) => {
  await page.route("*/**/api/datasets/", async (route) => {
    await route.fulfill({ json: [] });
  });
  await page.route("*/**/api/v1/me/datasets", async (route) => {
    await route.fulfill({ json: { items: [] } });
  });
};

export const mockAllDatasets = async (page: Page) => {
  await page.route("*/**/api/datasets/", async (route) => {
    await route.fulfill({ json: oldDatasets });
  });
  await page.route("*/**/api/v1/me/datasets", async (route) => {
    await route.fulfill({ json: { items: newDatasetsMocked } });
  });
  await page.route("*/**/api/v1/me/workspaces", async (route) => {
    await route.fulfill({ json: workspacesMocked });
  });
};

export const mockFeedbackTaskDataset = async (
  page: Page,
  { datasetId, workspaceId }: DatasetData
) => {
  await page.route(`*/**/api/v1/datasets/${datasetId}`, async (route) => {
    await route.fulfill({
      json: newDatasetsMocked.find((d) => d.id === datasetId),
    });
  });

  await page.route(`*/**/api/v1/workspaces/${workspaceId}`, async (route) => {
    await route.fulfill({
      json: workspacesMocked.find((w) => w.id === workspaceId),
    });
  });

  await page.route(
    `*/**/api/v1/me/datasets/${datasetId}/metrics`,
    async (route) => {
      await route.fulfill({
        json: {
          records: {
            count: 1000,
          },
          responses: {
            count: 29,
            submitted: 25,
            discarded: 4,
            draft: 0,
          },
        },
      });
    }
  );
};

export const mockDatasetDeletion = async (
  page: Page,
  datasetId: string,
  status: number
) => {
  await page.route(`*/**/api/v1/datasets/${datasetId}`, async (route) => {
    await route.fulfill({
      status,
      json: {
        detail: "MOCKED_DELETION_RESPONSE",
      },
    });
  });
};
