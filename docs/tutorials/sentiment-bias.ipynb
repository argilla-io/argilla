{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb97919e-2886-47fa-83f3-910ca32a273f",
   "metadata": {},
   "source": [
    "# üë©‚Äçüíª Examining Occupational Gender Stereotypes in Sentiment Analysis with Rubrix\n",
    "\n",
    "This brief tutorial will use Rubrix for reproducing and extending the analysis presented in [Good Secretaries, Bad Truck Drivers? Occupational Gender Stereotypes in Sentiment Analysis](https://aclanthology.org/W19-3809/), a research paper by Jayadev Bhaskaran and Isha Bhallamudi (*Proceedings of the First Workshop on Gender Bias in Natural Language Processing, ACL 2019*). \n",
    "\n",
    "<video width=\"100%\" controls><source src=\"https://github.com/recognai/rubrix-materials/raw/main/tutorials/videos/stereotypes.mp4\" type=\"video/mp4\"></video>\n",
    "\n",
    "## TL;DR\n",
    "\n",
    "\n",
    "\n",
    "## Brief summary of the original paper\n",
    "\n",
    "In the paper, the authors investigate the presence of occupational gender stereotypes in sentiment analysis models. For this research, they've built and released a gendered-balanced dataset of 800 sentences with specific professions. Their research approach is summarized in the following figure (extracted from the paper):\n",
    "\n",
    "<img src=\"img/gender-bias-paper.png\" alt=\"Occupational Gender Stereotypes in Sentiment Analysis\" width=\"500\"/>\n",
    "\n",
    "In the paper, they evaluate three models (a *logistic regression baseline model*, an *LSTM-based model*, and a *pre-trained BERT model*) all trained/fine-tuned on the SST-2 sentiment analysis dataset, a widely-known sentiment analysis dataset. Their main findings are:\n",
    "\n",
    "1. The pre-trained BERT shows a statistically significant **higher predicted positive class probability for sentences with male nouns**.\n",
    "\n",
    "2. The other two models show a **higher predicted positive class probabilities for sentences with female nouns**, which is in line with the distribution of positive examples with female nouns in the training set (SST-2).\n",
    "\n",
    "3. Given the above, the authors hypothesize that: (1) for pre-trained models biases might propagate from the pretraining phase (i.e., the large corpus used for language modeling pre-training), and (2) \"shallower\" models might propagate biases more directly from the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a86e83-b7a0-420a-ac37-cd669a324559",
   "metadata": {},
   "source": [
    "## Setup Rubrix\n",
    "\n",
    "If you are new to Rubrix, check out the ‚≠ê [Github repository](https://github.com/recognai/rubrix).\n",
    "\n",
    "If you have not installed and launched Rubrix, check the [Setup and Installation guide](../getting_started/setup&installation.rst).\n",
    "\n",
    "Once installed, you only need to import Rubrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c46c139-9e20-40ad-9c6d-c0c19ea0a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9e9ef7-c6f7-4a1d-af73-6ae82f6e1375",
   "metadata": {},
   "source": [
    "## Load the Gendered sentiment dataset\n",
    "\n",
    "\n",
    "The dataset in the original paper is available at https://github.com/jayadevbhaskaran/gendered-sentiment, let's load it into Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d647b93-a0b5-4d6c-999c-b9df626739d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27b1d404-9e65-4e33-a1b6-dca5c7e684fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/jayadevbhaskaran/gendered-sentiment/master/data/gender_corpus.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4b71e0e-564d-441f-9efc-a63a858c891d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>noun phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>He is a doctor.</td>\n",
       "      <td>male</td>\n",
       "      <td>doctor</td>\n",
       "      <td>He</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This boy is a doctor.</td>\n",
       "      <td>male</td>\n",
       "      <td>doctor</td>\n",
       "      <td>This boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>This man is a doctor.</td>\n",
       "      <td>male</td>\n",
       "      <td>doctor</td>\n",
       "      <td>This man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>My father is a doctor.</td>\n",
       "      <td>male</td>\n",
       "      <td>doctor</td>\n",
       "      <td>My father</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>My son is a doctor.</td>\n",
       "      <td>male</td>\n",
       "      <td>doctor</td>\n",
       "      <td>My son</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                sentence gender occupation noun phrase\n",
       "0   0         He is a doctor.   male     doctor          He\n",
       "1   1   This boy is a doctor.   male     doctor    This boy\n",
       "2   2   This man is a doctor.   male     doctor    This man\n",
       "3   3  My father is a doctor.   male     doctor   My father\n",
       "4   4     My son is a doctor.   male     doctor      My son"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db01aa5d-0768-4efd-b0f9-4ba79188d05f",
   "metadata": {},
   "source": [
    "## Extend the Gendered sentiment dataset with programmers\n",
    "\n",
    "As today is `#ProgrammersDay` and triggered by tweets like this one:\n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">I am not a &quot;Female Developer&quot;<br><br>I am not a &quot;Girl who can code&quot;<br><br>I am a Developer. <br>That&#39;s it. <br>That&#39;s my tag.<br>Call me nothing else‚ô•Ô∏è</p>&mdash; timpratim (@BhosalePratim) <a href=\"https://twitter.com/BhosalePratim/status/1437088890502873088?ref_src=twsrc%5Etfw\">September 12, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "Let's add \"programmer\" as occupation to the original dataset, so we can \"extend\" the original paper analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1ac35cd0-21c2-4114-9f1b-1c0eedf5676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "programmers = []\n",
    "for i,r in df.query(\"occupation == 'doctor'\").iterrows():\n",
    "    programmers.append(\n",
    "        {\n",
    "            \"sentence\": r.sentence.replace('doctor', 'programmer'),\n",
    "            \"gender\": r.gender,\n",
    "            \"noun phrase\": r['noun phrase'],\n",
    "            \"occupation\": 'programmer'\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "26f79d08-5b93-440b-9870-6832212b0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "programmers_df = pd.DataFrame(programmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "06d672e8-be2f-42fc-bac5-b7d0de642645",
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_occupations = pd.concat([df,programmers_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf472521-cd3e-428e-a3bf-6f47230e64ed",
   "metadata": {},
   "source": [
    "## Log default sentiment analysis pipeline predictions (`distilbert-finetuned-sst-2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa82dda4-b32a-4f8f-99a9-cf975d159df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8728ce18-71b3-4d89-b819-70c80b0b56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_record(row):\n",
    "    prediction = [(p['label'], p['score']) for p in nlp(row.sentence)]\n",
    "\n",
    "    return rb.TextClassificationRecord(\n",
    "        inputs={\"text\": row.sentence},#, \"gender\": row.gender, \"occupation\": row.occupation},\n",
    "        prediction=prediction,\n",
    "        metadata={\"gender\": row.gender, \"occupation\": row.occupation, \"noun_phrase\": row[\"noun phrase\"]},\n",
    "        prediction_agent=prediction_agent\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c379efd6-1177-483e-bf4e-3dc9aa13013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(task=\"sentiment-analysis\")\n",
    "prediction_agent = \"sst2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "efa7dd3e-561e-46d7-8585-447bb8346c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = gendered_occupations.apply(make_record, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8b0df453-881e-43db-9753-b8add6d2d03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='gender_sentiment_base', processed=40, failed=0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records, name=\"gender_sentiment_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694340d2-5550-493d-890d-30dda97d0c96",
   "metadata": {},
   "source": [
    "## Log predictions from twitter sentiment analysis pipeline (CardiffNLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c22a3671-e06a-4b71-84a3-bee98804dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_record(row):\n",
    "    prediction = [(mapping[p['label']], p['score']) for p in nlp(row.sentence)]\n",
    "\n",
    "    return rb.TextClassificationRecord(\n",
    "        inputs={\"text\": row.sentence},#, \"gender\": row.gender, \"occupation\": row.occupation},\n",
    "        prediction=prediction,\n",
    "        metadata={\"gender\": row.gender, \"occupation\": row.occupation, \"noun_phrase\": row[\"noun phrase\"]},\n",
    "        prediction_agent=prediction_agent\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "162d44ba-f461-4d8d-bc27-5338299371c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(task=\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "mapping = {\"LABEL_0\": \"NEGATIVE\", \"LABEL_1\": \"NEUTRAL\", \"LABEL_2\":\"POSITIVE\" }\n",
    "prediction_agent = \"cardiffnlp/twitter-roberta-base-sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b566d9e-55f1-4015-8f64-265c884d2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = gendered_occupations.apply(make_record, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "65dec897-6c60-4b04-a9e1-f523844ebdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='gender_sentiment_base', processed=40, failed=0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records, name=\"gender_sentiment_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8ef77-6674-4f1f-bcfc-aea89bcade6a",
   "metadata": {},
   "source": [
    "## Building an interactive dashboard to analyse and compare the models\n",
    "\n",
    "Rubrix default installation includes Kibana, which can be used to build monitoring and analytical dashboards on top of your model predictions.\n",
    "\n",
    "In this case, we'll be building a Kibana dashboard to examine biases in pre-trained sentiment models.\n",
    "\n",
    "### Setting up Kibana indexes\n",
    "\n",
    "A detailed guide for configuring Kibana with Rubrix indexes is coming soon, stay tuned! For now, let's describe the basic steps to get started:\n",
    "\n",
    "1. If you are running Rubrix locally, open the following URL: https://localhost:5601\n",
    "2. Then go to http://localhost:5601/app/management and go to Kibana / Index Patterns.\n",
    "3. Click create index pattern and input the following pattern `.rubrix.dataset.*`, this will make all your Rubrix dataset available for building visualizations and dashboards.\n",
    "4. Then you can explore Kibana by yourself, you can start creating a Dashboad by going to http://localhost:5601/app/dashboards \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c5945-22fb-4b3c-8d3b-cd1eacc3904e",
   "metadata": {},
   "source": [
    "### Our dashboard\n",
    "\n",
    "<img src=\"img/gender_dashboard.png\" alt=\"Occupational Gender Stereotypes in Sentiment Analysis\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edbd585-88dd-4627-a54b-e037803a697f",
   "metadata": {},
   "source": [
    "## Main findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d9cab-38a7-435f-8950-2ef28c1f2c84",
   "metadata": {},
   "source": [
    "## Appendix: Model explainability with `transformers-interpret`\n",
    "\n",
    "The main idea is to log the token attributions for each prediction to potentially detect/confirm biases associated to certain words. For this type of use case, in Rubrix you can log the token attributions together with the predictions. Later, you can browse this information and consume as another dimension of your Kibana dashboards.\n",
    "\n",
    "Unfortunately, this is a work in progress because we've identified a number of issues in the `transformers-interpret` library and did not manage to get meaningful results (see https://github.com/cdpierse/transformers-interpret/issues/65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116c05e-6399-431a-a009-6c1055affc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers_interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2b6cae3e-a547-4df1-9806-c48aac4263cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
    "\n",
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c7877eaf-a054-431b-b374-a9fe2bbe7d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_attributions = cls_explainer(\"She is a programmer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dbbc877c-c216-4f3f-8687-b075aa0b3e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>POSITIVE (0.95)</b></text></td><td><text style=\"padding-right:2em\"><b>POSITIVE</b></text></td><td><text style=\"padding-right:2em\"><b>0.84</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> she                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 53%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> programmer                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>POSITIVE (0.95)</b></text></td><td><text style=\"padding-right:2em\"><b>POSITIVE</b></text></td><td><text style=\"padding-right:2em\"><b>0.84</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> she                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 53%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> programmer                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_explainer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb773f66-af37-45bc-bcc6-a0015e667fd9",
   "metadata": {},
   "source": [
    "## Log attributions into a Rubrix dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2ae677e3-d2fd-4f1b-ac7f-c48fcc205b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubrix import TextClassificationRecord, TokenAttributions\n",
    "\n",
    "records = []\n",
    "for i,example in programmers_df.iterrows():\n",
    "    word_attributions = cls_explainer(example[\"sentence\"])\n",
    "    \n",
    "    token_attributions = [ \n",
    "        TokenAttributions(\n",
    "            token=token, \n",
    "            attributions={cls_explainer.predicted_class_name: score}\n",
    "        )\n",
    "        for token, score in word_attributions[1:-1] # ignore first (CLS) and last (SEP) tokens\n",
    "    ]\n",
    "    record = TextClassificationRecord(\n",
    "        inputs=example[\"sentence\"],\n",
    "        prediction=[(cls_explainer.predicted_class_name, cls_explainer.pred_probs)],\n",
    "        prediction_agent=\"\",\n",
    "        explanation={\"text\": token_attributions},\n",
    "        metadata={\"gender\": example[\"gender\"], \"occupation\": example[\"occupation\"], \"noun_phrase\": example[\"noun phrase\"]},\n",
    "    )\n",
    "    records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f03787e8-9e54-4621-9058-563a7e610c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='gender_sentiment_sst2_interpret', processed=40, failed=0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records, name=\"gender_sentiment_sst2_interpret\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
