{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a705f134-8360-4c9d-9feb-ac16d662086d",
   "metadata": {},
   "source": [
    "# üßê Find label errors with cleanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360fd61b-ed74-499d-84b6-43c9d52631b0",
   "metadata": {},
   "source": [
    "In this tutorial, we will show you how you can find possible labeling errors in your data set with the help of [*cleanlab*](https://github.com/cgnorthcutt/cleanlab) and *Rubrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8c719-d8f2-417a-b489-28766b2f3d6e",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149f04d-8b00-409e-8e38-cc2f594e15b0",
   "metadata": {},
   "source": [
    "As shown recently by [Curtis G. Northcutt et al.](https://arxiv.org/abs/2103.14749) label errors are pervasive even in the most-cited test sets used to benchmark the progress of the field of machine learning.\n",
    "In the worst-case scenario, these label errors can destabilize benchmarks and tend to favor more complex models with a higher capacity over lower capacity models.\n",
    "\n",
    "They introduce a new principled framework to ‚Äúidentify label errors, characterize label noise, and learn with noisy labels‚Äù called **confident learning**. It is open-sourced as the [cleanlab Python package](https://github.com/cgnorthcutt/cleanlab) that supports finding, quantifying, and learning with label errors in data sets.\n",
    "\n",
    "This tutorial walks you through 5 basic steps to find and correct label errors in your data set:\n",
    "\n",
    "1. üíæ Load the data set you want to check, and a model trained on it;\n",
    "2. üíª Make predictions for the test split of your data set;\n",
    "3. üßê Get label error candidates with *cleanlab*;\n",
    "4. üî¶ Uncover label errors with *Rubrix*;\n",
    "5. üñç Correct label errors and load the corrected data set;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fcc27e-29d7-4648-b7a7-527847306fd6",
   "metadata": {},
   "source": [
    "## Setup Rubrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a9912b",
   "metadata": {},
   "source": [
    "If you are new to Rubrix, visit and star Rubrix for updates: ‚≠ê [Github repository](https://github.com/recognai/rubrix)\n",
    "\n",
    "If you have not installed and launched Rubrix, check the [Setup and Installation guide](../getting_started/setup&installation.rst).\n",
    "\n",
    "Once installed, you only need to import Rubrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee51923",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import rubrix as rb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f079a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install tutorial dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b55b62-0f05-4f17-94b6-e448cd4c5e0a",
   "metadata": {},
   "source": [
    "Apart from [cleanlab](https://github.com/cgnorthcutt/cleanlab), we will also install the Hugging Face libraries [transformers](https://github.com/huggingface/transformers) and [datasets](https://github.com/huggingface/datasets), as well as [PyTorch](https://pytorch.org/), that provide us with the model and the data set we are going to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac0cc7f-762c-4e53-888e-c8821a54f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cleanlab torch transformers datasets\n",
    "exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b222d232-148b-4739-8301-11eb0e2cb832",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d612a62-92d4-4f7b-bee4-775ebb849cb6",
   "metadata": {},
   "source": [
    "Let us import all the necessary stuff in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5341ea5-b6ea-4ab8-b4e6-c76a9e65373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from cleanlab.pruning import get_noise_indices\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9f8ce-b38f-4999-abbf-6ac2a3eb48b8",
   "metadata": {},
   "source": [
    "## 1. Load model and data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e086a-8622-4be8-8e90-b9571ed23df4",
   "metadata": {},
   "source": [
    "For this tutorial we will use the well studied [Microsoft Research Paraphrase Corpus](https://microsoft.com/en-us/download/details.aspx?id=52398) (MRPC) data set that forms part of the [GLUE benchmark](https://gluebenchmark.com/), and a pre-trained model from the Hugging Face Hub that was fine-tuned on this specific data set.\n",
    "\n",
    "Let us first get the model and its corresponding tokenizer to be able to make predictions. For a detailed guide on how to use the ü§ó *transformers* library, please refer to their excellent [documentation](https://huggingface.co/transformers/task_summary.html#sequence-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55dc06-45bd-4622-9284-8f656e5e7e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"textattack/roberta-base-MRPC\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53573589-7e23-4843-bb54-129e74347964",
   "metadata": {},
   "source": [
    "We then get the test split of the MRPC data set, that we will scan for label errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb920c-6d9c-4fbd-a6d3-2c72534e2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"glue\", \"mrpc\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51905b1-5c78-48d5-bb7a-fc36f753d32a",
   "metadata": {},
   "source": [
    "Let us have a quick look at the format of the data set. Label `1` means that both `sentence1` and `sentence2` are *semantically equivalent*, a `0` as label implies that the sentence pair is *not equivalent*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c27f1f5f-c0cd-4ab1-bbf8-fb22368d7159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCCW 's chief operating officer , Mike Butcher...</td>\n",
       "      <td>Current Chief Operating Officer Mike Butcher a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The world 's two largest automakers said their...</td>\n",
       "      <td>Domestic sales at both GM and No. 2 Ford Motor...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>According to the federal Centers for Disease C...</td>\n",
       "      <td>The Centers for Disease Control and Prevention...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A tropical storm rapidly developed in the Gulf...</td>\n",
       "      <td>A tropical storm rapidly developed in the Gulf...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The company didn 't detail the costs of the re...</td>\n",
       "      <td>But company officials expect the costs of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  PCCW 's chief operating officer , Mike Butcher...   \n",
       "1  The world 's two largest automakers said their...   \n",
       "2  According to the federal Centers for Disease C...   \n",
       "3  A tropical storm rapidly developed in the Gulf...   \n",
       "4  The company didn 't detail the costs of the re...   \n",
       "\n",
       "                                           sentence2  label  idx  \n",
       "0  Current Chief Operating Officer Mike Butcher a...      1    0  \n",
       "1  Domestic sales at both GM and No. 2 Ford Motor...      1    1  \n",
       "2  The Centers for Disease Control and Prevention...      1    2  \n",
       "3  A tropical storm rapidly developed in the Gulf...      0    3  \n",
       "4  But company officials expect the costs of the ...      0    4  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277bdacc-7b45-49de-9a04-d6202538e176",
   "metadata": {},
   "source": [
    "## 2. Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76780ac-2710-4189-83da-e4d1820afba3",
   "metadata": {},
   "source": [
    "Now let us use the model to get predictions for our data set, and add those to our dataset instance. We will use the `.map` functionality of the *datasets* library to process our data batch-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a9e1b-d718-4789-9ca3-79014113e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(batch):\n",
    "    # batch is a dictionary of lists\n",
    "    tokenized_input = tokenizer(\n",
    "        batch[\"sentence1\"], batch[\"sentence2\"], padding=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    # get logits of the model prediction\n",
    "    logits = model(**tokenized_input).logits\n",
    "    # convert logits to probabilities\n",
    "    probabilities = torch.softmax(logits, dim=1).detach().numpy()\n",
    "    \n",
    "    return {\"probabilities\": probabilities}\n",
    "    \n",
    "# Apply predictions batch-wise\n",
    "dataset = dataset.map(\n",
    "    get_model_predictions,\n",
    "    batched=True,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b6f02-3775-4bda-b7c2-5c515d5bb619",
   "metadata": {},
   "source": [
    "## 3. Get label error candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f3ff64-c177-4bb8-8233-334bc33b6846",
   "metadata": {},
   "source": [
    "To identify label error candidates the cleanlab framework simply needs the probability matrix of our predictions (`n x m`, where `n` is the number of examples and `m` the number of labels), and the potentially noisy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e71b15bd-bde9-4a4f-93b0-c55b28cf6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the data as numpy arrays\n",
    "dataset.set_format(\"numpy\")\n",
    "\n",
    "# Get a boolean array of label error candidates\n",
    "label_error_candidates = get_noise_indices(\n",
    "    s=dataset[\"label\"],\n",
    "    psx=dataset[\"probabilities\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa940fc-c2ae-4f20-9b85-48b7cce189c8",
   "metadata": {},
   "source": [
    "This one line of code provides us with a boolean array of label error candidates that we can investigate further. \n",
    "Out of the **1725 sentence pairs** present in the test data set we obtain **129 candidates** (7.5%) for possible label errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7b615fa4-5d62-4a66-88a5-9a81b818707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1725\n",
      "Candidates: 129 (7.5%)\n"
     ]
    }
   ],
   "source": [
    "frac = label_error_candidates.sum()/len(dataset)\n",
    "print(\n",
    "    f\"Total: {len(dataset)}\\n\"\n",
    "    f\"Candidates: {label_error_candidates.sum()} ({100*frac:0.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f2f0e-6441-4a84-b4b5-503cc7cd7685",
   "metadata": {},
   "source": [
    "## 4. Uncover label errors in Rubrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ab494-0490-4efd-8423-25fecc4c5481",
   "metadata": {},
   "source": [
    "Now that we have a list of potential candidates, let us log them to *Rubrix* to uncover and correct the label errors.\n",
    "First we switch to a pandas DataFrame to filter out our candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cc03094d-58ca-408c-8467-4bbd6351e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = dataset.to_pandas()[label_error_candidates]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceef098-f462-4587-b15c-cdebd582decd",
   "metadata": {},
   "source": [
    "Then we will turn those candidates into [TextClassificationRecords](../reference/python_client_api.rst#rubrix.client.models.TextClassificationRecord) that we will log to *Rubrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5f7d5436-a829-4a74-983e-b2c827b2ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_record(row):\n",
    "    prediction = list(zip([\"Not equivalent\", \"Equivalent\"], row.probabilities))\n",
    "    annotation = \"Not equivalent\"\n",
    "    if row.label == 1:\n",
    "        annotation = \"Equivalent\"\n",
    "        \n",
    "    return rb.TextClassificationRecord(\n",
    "        inputs={\"sentence1\": row.sentence1, \"sentence2\": row.sentence2}, \n",
    "        prediction=prediction, \n",
    "        prediction_agent=\"textattack/roberta-base-MRPC\", \n",
    "        annotation=annotation, \n",
    "        annotation_agent=\"MRPC\"\n",
    "    )\n",
    "        \n",
    "records = candidates.apply(make_record, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208cc5a8-f349-435c-a6c1-297e30a8e52b",
   "metadata": {},
   "source": [
    "Having our records at hand we can now log them to *Rubrix* and save them in a dataset that we call `\"mrpc_label_error\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12750a89-33ce-4655-bb15-458b33cbd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.log(records, name=\"mrpc_label_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043088e-3e2a-4b99-9951-c744201843ed",
   "metadata": {},
   "source": [
    "Scanning through the records in the [*Explore Mode*](../reference/rubrix_webapp_reference.rst#explore-mode) of *Rubrix*, we were able to find at least **30 clear cases** of label errors. \n",
    "A couple of examples are shown below, in which the noisy labels are shown in the upper right corner of each example.\n",
    "The predictions of the model together with their probabilities are shown below each sentence pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd06927-7327-480a-9100-0cdb79458509",
   "metadata": {},
   "source": [
    "![Examples of label errors in the test set uncovered with Rubrix](./img/find_label_errors/test_sample_examples.png \"Examples of label errors in the test set uncovered with Rubrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f616ea-8dc9-4fb3-ba05-35484cd24b63",
   "metadata": {},
   "source": [
    "If your model is not terribly over-fitted, you can also try to run the candidate search over your training data to find very obvious label errors. \n",
    "If we repeat the steps above on the training split of the MRPC data set (3668 examples), we obtain **9 candidates** (this low number is expected) out of which **5 examples** were clear cases of label errors.\n",
    "A couple of examples are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde69fb5-136e-488f-bdc1-33aadf95cca3",
   "metadata": {},
   "source": [
    "![Examples of label errors in the training set uncovered with Rubrix](./img/find_label_errors/train_sample_examples.png \"Examples of label errors in the training set uncovered with Rubrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd648159-551c-4461-9677-4b5eec499630",
   "metadata": {},
   "source": [
    "## 5. Correct label errors\n",
    "\n",
    "With *Rubrix* it is very easy to correct those label errors.\n",
    "Just switch on the [*Annotation Mode*](../reference/rubrix_webapp_reference.rst#annotation-mode), correct the noisy labels and load the dataset back into your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9d8437d7-028e-4914-8700-030dd7aa02af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>prediction</th>\n",
       "      <th>annotation</th>\n",
       "      <th>prediction_agent</th>\n",
       "      <th>annotation_agent</th>\n",
       "      <th>multi_label</th>\n",
       "      <th>explanation</th>\n",
       "      <th>id</th>\n",
       "      <th>metadata</th>\n",
       "      <th>status</th>\n",
       "      <th>event_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'sentence1': 'Deaths in rollover crashes acco...</td>\n",
       "      <td>[(Equivalent, 0.9751904606819153), (Not equiva...</td>\n",
       "      <td>[Not equivalent]</td>\n",
       "      <td>textattack/roberta-base-MRPC</td>\n",
       "      <td>MRPC</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>bad3f616-46e3-43ca-8ba3-f2370d421fd2</td>\n",
       "      <td>{}</td>\n",
       "      <td>Validated</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'sentence1': 'Mr. Kozlowski contends that the...</td>\n",
       "      <td>[(Not equivalent, 0.9878258109092712), (Equiva...</td>\n",
       "      <td>[Equivalent]</td>\n",
       "      <td>textattack/roberta-base-MRPC</td>\n",
       "      <td>MRPC</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>50ca41c9-a147-411f-8682-1e3880a522f9</td>\n",
       "      <td>{}</td>\n",
       "      <td>Validated</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'sentence1': 'Larger rivals , including Tesco...</td>\n",
       "      <td>[(Equivalent, 0.986499547958374), (Not equival...</td>\n",
       "      <td>[Not equivalent]</td>\n",
       "      <td>textattack/roberta-base-MRPC</td>\n",
       "      <td>MRPC</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>6c06250f-7953-475a-934f-7eb35fc9dc4d</td>\n",
       "      <td>{}</td>\n",
       "      <td>Validated</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'sentence1': 'The Standard &amp; Poor 's 500 inde...</td>\n",
       "      <td>[(Not equivalent, 0.9457013010978699), (Equiva...</td>\n",
       "      <td>[Equivalent]</td>\n",
       "      <td>textattack/roberta-base-MRPC</td>\n",
       "      <td>MRPC</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>39f37fcc-ac22-4871-90f1-3766cf73f575</td>\n",
       "      <td>{}</td>\n",
       "      <td>Validated</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'sentence1': 'Defense lawyers had said a chan...</td>\n",
       "      <td>[(Equivalent, 0.9974484443664551), (Not equiva...</td>\n",
       "      <td>[Not equivalent]</td>\n",
       "      <td>textattack/roberta-base-MRPC</td>\n",
       "      <td>MRPC</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>080c6d5c-46de-4670-9e0a-98e0c7592b11</td>\n",
       "      <td>{}</td>\n",
       "      <td>Validated</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0  {'sentence1': 'Deaths in rollover crashes acco...   \n",
       "1  {'sentence1': 'Mr. Kozlowski contends that the...   \n",
       "2  {'sentence1': 'Larger rivals , including Tesco...   \n",
       "3  {'sentence1': 'The Standard & Poor 's 500 inde...   \n",
       "4  {'sentence1': 'Defense lawyers had said a chan...   \n",
       "\n",
       "                                          prediction        annotation  \\\n",
       "0  [(Equivalent, 0.9751904606819153), (Not equiva...  [Not equivalent]   \n",
       "1  [(Not equivalent, 0.9878258109092712), (Equiva...      [Equivalent]   \n",
       "2  [(Equivalent, 0.986499547958374), (Not equival...  [Not equivalent]   \n",
       "3  [(Not equivalent, 0.9457013010978699), (Equiva...      [Equivalent]   \n",
       "4  [(Equivalent, 0.9974484443664551), (Not equiva...  [Not equivalent]   \n",
       "\n",
       "               prediction_agent annotation_agent  multi_label explanation  \\\n",
       "0  textattack/roberta-base-MRPC             MRPC        False        None   \n",
       "1  textattack/roberta-base-MRPC             MRPC        False        None   \n",
       "2  textattack/roberta-base-MRPC             MRPC        False        None   \n",
       "3  textattack/roberta-base-MRPC             MRPC        False        None   \n",
       "4  textattack/roberta-base-MRPC             MRPC        False        None   \n",
       "\n",
       "                                     id metadata     status event_timestamp  \n",
       "0  bad3f616-46e3-43ca-8ba3-f2370d421fd2       {}  Validated            None  \n",
       "1  50ca41c9-a147-411f-8682-1e3880a522f9       {}  Validated            None  \n",
       "2  6c06250f-7953-475a-934f-7eb35fc9dc4d       {}  Validated            None  \n",
       "3  39f37fcc-ac22-4871-90f1-3766cf73f575       {}  Validated            None  \n",
       "4  080c6d5c-46de-4670-9e0a-98e0c7592b11       {}  Validated            None  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset into a pandas DataFrame\n",
    "dataset_with_corrected_labels = rb.load(\"mrpc_label_error\")\n",
    "\n",
    "dataset_with_corrected_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556f72e-3468-4131-b055-a21f7d2f5981",
   "metadata": {},
   "source": [
    "Now you can use the corrected data set to repeat your benchmarks and measure your model's \"real-word performance\" you care about in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba4005-aa2b-4e54-b937-760d8ceb0633",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial we saw how to leverage *cleanlab* and *Rubrix* to uncover label errors in your data set.\n",
    "In just a few steps you can quickly check if your test data set is seriously affected by label errors and if your benchmarks are really meaningful in practice.\n",
    "Maybe your less complex models turns out to beat your resource hungry super model, and the deployment process just got a little bit easier üòÄ.\n",
    "\n",
    "*Cleanlab* and *Rubrix* do not care about the model architecture or the framework you are working with.\n",
    "They just care about the underlying data and allow you to put more humans in the loop of your AI Lifecycle.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9926473e-a406-45a0-be00-1bcee539a7e2",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "### üìö [Rubrix documentation](https://docs.rubrix.ml) for more guides and tutorials.\n",
    "\n",
    "### üôã‚Äç‚ôÄÔ∏è Join the Rubrix community! A good place to start is the [discussion forum](https://github.com/recognai/rubrix/discussions).\n",
    "\n",
    "### ‚≠ê Rubrix [Github repo](https://github.com/recognai/rubrix) to stay updated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}