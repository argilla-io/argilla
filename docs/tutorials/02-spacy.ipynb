{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✨ Using Rubrix with `spaCy`\n",
    "\n",
    "This tutorial will walk you through the process of using [spaCy](https://spacy.io/) with Rubrix to track and monitor Name Entity Recognition (NER) predictions.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial we will:\n",
    "\n",
    "- Load the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub.\n",
    "\n",
    "- Use a transformer-based spaCy model for detecting entities in this dataset and log the detected entities into a Rubrix dataset. This dataset can be used for exploring the quality of predictions and for creating a new training set, by correcting, adding and validating entities.\n",
    "\n",
    "- Use a smaller spaCy model for detecting entities and log the detected entities into the same Rubrix dataset for comparing its predictions with the previous model.\n",
    "\n",
    "\n",
    "\n",
    "## Install tutorial dependencies\n",
    "\n",
    "In this tutorial, we'll use the `datasets` and `spaCy` libraries and the `en_core_web_trf` pretrained English model, a Roberta-based spaCy model . If you do not have them installed, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install datasets -qqq\n",
    "%pip install -U spacy -qqq\n",
    "%pip install protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Rubrix\n",
    "\n",
    "If you have not installed and launched Rubrix, check the [installation guide](https://github.com/recognai/rubrix#get-started). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dataset\n",
    "\n",
    "For this tutorial, we're going to use the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub. It contains all explicit time references in a dataset of 52,183 novels whose full text is available via Project Gutenberg. From extracts of novels, we are surely going to find some NER entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gutenberg_time\", split=\"train[0:20]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our dataset! Starting by the length of it and an sneak peek to one instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging spaCy NER entities into Rubrix\n",
    "\n",
    "### Using a Transformer-based pipeline\n",
    "\n",
    "Let's install and load our roberta-based pretrained pipeline and apply it to one of our dataset records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I crossed the ground she had traversed , noting every feature surrounding it , the curving wheel-track , the thin prickly sand-herbage , the wave - mounds , the sparse wet shells and pebbles , the gleaming flatness of the water , and the vast horizon-boundary of pale flat land level with shore , looking like a dead sister of the sea . By a careful examination of my watch and the sun 's altitude , I was able to calculate what would , in all likelihood , have been his height above yonder waves when her chair was turned toward the city , at a point I reached in the track . But of the matter then simultaneously occupying my mind , to recover which was the second supreme task I proposed to myself-of what . I also was thinking upon the stroke of five o'clock , I could recollect nothing . I could not even recollect whether I happened to be looking on sun and waves when she must have had them full and glorious in her face . With the heartiest consent I could give , and a blank cheque , my father returned to England to hire forthwith a commodious yacht , fitted and manned . Before going he discoursed of prudence in our expenditure ; though not for the sake of the mere money in hand , which was a trifle , barely more than the half of my future income ; but that the squire , should he by and by bethink him of inspecting our affairs , might perceive we were not spendthrifts ."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(dataset[0][\"tok_context\"])\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the nlp pipeline to our dataset records, collecting the tokens and NER entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "records = []   \n",
    "\n",
    "for record in dataset:\n",
    "    \n",
    "    # We only need the text of each instance\n",
    "    text = record[\"tok_context\"]\n",
    "    \n",
    "    # spaCy Doc creation\n",
    "    doc = nlp(text)    \n",
    "    \n",
    "    # Entity annotations\n",
    "    entities = [\n",
    "        (ent.label_, ent.start_char, ent.end_char)  \n",
    "        for ent in doc.ents\n",
    "    ] \n",
    "\n",
    "    # Pre-tokenized input text\n",
    "    tokens = [token.text  for token in doc]\n",
    "    \n",
    "\n",
    "    # Rubrix TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rb.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=entities,\n",
    "            prediction_agent=\"spacy.en_core_web_trf\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='gutenberg_spacy_ner', processed=20, failed=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"gutenberg_spacy_ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to the `gutenberg_spacy_ner` dataset in Rubrix you can explore the predictions of this model:\n",
    "\n",
    "- You can filter records containing specific entity types.\n",
    "- You can see the most frecuent \"mentions\" or surface forms for each entity. Mentions are the string values of specific entity types, such as for example \"1 month\" can be the mention of a duration entity. This is useful for error analysis, to quickly see potential issues and problematic entity types.\n",
    "- You can use the free-text search to find records containing specific words.\n",
    "- You could validate, include or reject specific entity annotations to build a new traning set.\n",
    "\n",
    "\n",
    "![spacy_ner explore](https://github.com/dvsrepo/imgs/raw/main/spacy_ner1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a smaller but more efficient pipeline\n",
    "\n",
    "Now let's compare with a smaller, but more efficient pre-trained model. Let's first download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(dataset[0][\"tok_context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []    # Creating and empty record list to save all the records\n",
    "\n",
    "for record in dataset:\n",
    "\n",
    "    text = record[\"tok_context\"]  # We only need the text of each instance\n",
    "    doc = nlp(text)    # spaCy Doc creation\n",
    "    \n",
    "    # Entity annotations\n",
    "    entities = [\n",
    "        (ent.label_, ent.start_char, ent.end_char)  \n",
    "        for ent in doc.ents\n",
    "    ] \n",
    "\n",
    "    # Pre-tokenized input text\n",
    "    tokens = [token.text  for token in doc]\n",
    "    \n",
    "\n",
    "    # Rubrix TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rb.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=entities,\n",
    "            prediction_agent=\"spacy.en_core_web_sm\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='gutenberg_spacy_ner', processed=20, failed=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"gutenberg_spacy_ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring and comparing `en_core_web_sm` and `en_core_web_trf` models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to your `gutenberg_spacy_ner` you can explore and compare the results of both models. \n",
    "\n",
    "A handy feature is the `predicted by` filter, which comes from the `prediction_agent` parameter of your `TextClassificationRecord`.\n",
    "\n",
    "![spacy_models_meta](img/spacy_ner2.png \"spaCy models predicted_by filter\")\n",
    "\n",
    "\n",
    "Some quick qualitative findings about these two models applied to this sample:\n",
    "\n",
    "- `en_core_web_trf` makes more conservative predictions, most of them accurate but misses a number of entities (higher precision, less recall for entities like `CARDINAL`).\n",
    "- `en_core_web_sm` has less precision for most of the entities, confusing for example `PERSON` with `ORG` entities, even with the same surface form within the same paragraph, but has better recall for entities like `CARDINAL`.\n",
    "- For `TIME` entities both model show almost the same distribution and are quite accurate. This could be further analysed by logging the time `annotations` in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this tutorial, we have learnt to log and explore differnt `spaCy` NER models with Rubrix. Using what we´ve learnt here you can:\n",
    "\n",
    "- Build custom dashboards using Kibana to monitor and visualize spaCy models.\n",
    "- Build training sets using pre-trained spaCy models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "### 📚 [Rubrix documentation](https://docs.rubrix.ml) for more guides and tutorials. \n",
    "\n",
    "### 🙋‍♀️ Join the Rubrix community! A good place to start is the [discussion forum](https://github.com/recognai/rubrix/discussions).\n",
    "\n",
    "### ⭐ Rubrix [Github repo](https://github.com/recognai/rubrix) to stay updated. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b709380ea7d1cb2eb4650c0f11ac7e002ec6a534602815725771481b4784238c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "0f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
