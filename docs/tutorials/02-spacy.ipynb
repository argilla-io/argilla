{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœ¨ Using Rubrix with `spaCy`\n",
    "\n",
    "In this tutorial, we will walk through the process of using Rubrix with `spaCy`, one of the most-widely used NLP libraries.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Our goal is to show you how to explore `spaCy` NER predictions with Rubrix**. \n",
    "\n",
    "## Install tutorial dependencies\n",
    "\n",
    "In this tutorial we will be using `datasets` and `spaCy` libraries and the `en_core_web_trf` pretrained English model, a Roberta-based spaCy model . If you do not have them installed, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install datasets -qqq\n",
    "%pip install -U spacy -qqq\n",
    "%pip install protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Rubrix\n",
    "\n",
    "If you have not installed and launched Rubrix, check the [installation guide](https://github.com/recognai/rubrix#get-started). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dataset\n",
    "\n",
    "For this tutorial, we are going to use the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub. It contains all explicit time references in a dataset of 52,183 novels whose full text is available via Project Gutenberg. From extracts of novels, we are surely going to find some NER entities. Well, technically, spaCy is going to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gutenberg_time\", split=\"train[0:20]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our dataset! Starting by the length of it and an sneak peek to one instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging spaCy NER entities into Rubrix\n",
    "\n",
    "### Using a Transformer-based pipeline\n",
    "\n",
    "Let's install and load our roberta-based pretrained pipeline and apply it to one of our dataset records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(dataset[0][\"tok_context\"])\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the nlp pipeline to our dataset records, collecting the tokens and NER entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "records = []    # Creating and empty record list to save all the records\n",
    "\n",
    "for record in dataset:\n",
    "\n",
    "    text = record[\"tok_context\"]  # We only need the text of each instance\n",
    "    doc = nlp(text)    # spaCy Doc creation\n",
    "    \n",
    "    # Entity annotations\n",
    "    entities = [\n",
    "        (ent.label_, ent.start_char, ent.end_char)  \n",
    "        for ent in doc.ents\n",
    "    ] \n",
    "\n",
    "    # Pre-tokenized input text\n",
    "    tokens = [token.text  for token in doc]\n",
    "    \n",
    "\n",
    "    # Rubrix TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rb.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=entities,\n",
    "            prediction_agent=\"spacy.en_core_web_trf\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.log(records=records, name=\"gutenberg_spacy_ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a smaller but more efficient pipeline\n",
    "\n",
    "Now let's compare with a smaller, but more efficient pre-trained model. Let's first download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(dataset[0][\"tok_context\"])\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []    # Creating and empty record list to save all the records\n",
    "\n",
    "for record in dataset:\n",
    "\n",
    "    text = record[\"tok_context\"]  # We only need the text of each instance\n",
    "    doc = nlp(text)    # spaCy Doc creation\n",
    "    \n",
    "    # Entity annotations\n",
    "    entities = [\n",
    "        (ent.label_, ent.start_char, ent.end_char)  \n",
    "        for ent in doc.ents\n",
    "    ] \n",
    "\n",
    "    # Pre-tokenized input text\n",
    "    tokens = [token.text  for token in doc]\n",
    "    \n",
    "\n",
    "    # Rubrix TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rb.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=entities,\n",
    "            prediction_agent=\"spacy.en_core_web_sm\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.log(records=records, name=\"gutenberg_spacy_ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring and comparing `en_core_web_sm` and `en_core_web_trf` models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to your `gutenberg_spacy_ner` you can explore and compare the results of both models. \n",
    "\n",
    "A handy feature is the `predicted by` filter, which comes from the `prediction_agent` parameter of your `TextClassificationRecord`.\n",
    "\n",
    "![spacy_models_meta](img/spacy_filter_meta.png \"spaCy models predicted_by filter\")\n",
    "\n",
    "\n",
    "Some quick qualitative findings about these two models applied to this sample:\n",
    "\n",
    "- `en_core_web_trf` makes more conservative predictions, most of them accurate but misses a number of entities (higher precision, less recall for entities like `CARDINAL`).\n",
    "- `en_core_web_sm` has less precision for most of the entities, confusing for example `PERSON` with `ORG` entities, even with the same surface form within the same paragraph, but has better recall for entities like `CARDINAL`.\n",
    "- For `TIME` entities both model show almost the same distribution and are quite accurate. This could be further analysed by logging the time `annotations` in the dataset.\n",
    "\n",
    "As an illustration of these findings, see an example of a records with `en_core_web_sm` (top) and `en_core_web_trf` (bottom) predicted entities.\n",
    "\n",
    "![spacy_sm_vs_trf](img/spacy_sm_vs_trf.png \"spaCy sm model vs. roberta-based model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this tutorial, we have learnt to log and explore `spaCy` NER models with Rubrix. \n",
    "\n",
    "## Next steps\n",
    "\n",
    "We invite you to check our other tutorials and join our community, a good place to start is our [discussion forum](https://github.com/recognai/rubrix/discussions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('rubrix0.1': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "0f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd"
   }
  },
  "interpreter": {
   "hash": "b709380ea7d1cb2eb4650c0f11ac7e002ec6a534602815725771481b4784238c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}