{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "respected-allowance",
   "metadata": {},
   "source": [
    "# ü§ó Using Rubrix to explore NLP data with Hugging Face datasets and transformers \n",
    "\n",
    "In this tutorial, we will walk through the process of using Rubrix to explore NLP datasets in combination with the amazing `datasets` and `transformer` libraries from Hugging Face.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Our goal is to show you how to store and explore NLP datasets using Rubrix** for use cases like training data management or model evaluation and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-guarantee",
   "metadata": {},
   "source": [
    "The tutorial is organized into three parts:\n",
    "\n",
    "1. **Storing and exploring text classification data**: We will use the ü§ó datasets library and Rubrix to store text classification datasets.\n",
    "2. **Storing and exploring token classification data**: We will use the ü§ó datasets library and Rubrix to store token classification data.\n",
    "2. **Exploring predictions**: We will use a pretrained ü§ó transformers model and store its predictions into Rubrix to explore and evaluate our pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-portfolio",
   "metadata": {},
   "source": [
    "## Install transformers and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets -qqq\n",
    "#!pip install tdqm  # for progress bars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-gravity",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup Rubrix\n",
    "\n",
    "[here we should point the user to the install and setup guide]\n",
    "\n",
    "By default, rubrix will make a local initialization (as shown in the setup guide). If you want to specify an API url and key, you can pass that information via two environment variables: **RUBRIX_API_KEY** and **RUBRIX_API_URL**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incomplete-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from rubrix import TextClassificationRecord, TextClassificationAnnotation, ClassPrediction, TokenClassificationRecord, TokenClassificationAnnotation\n",
    "\n",
    "# Removing this after rubrix.init() is possible\n",
    "import os\n",
    "os.environ[\"RUBRIX_API_URL\"] = \"https://rubrix-dev.biome.recogn.ai\"\n",
    "os.environ[\"RUBRIX_API_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJAcmVjb2duYWkiLCJleHAiOjE2MjEwMjE1NDd9.5r5yFsUnpukinzUBanG3pTOVlpy_aNd6OUuS0K3R2JM\"\n",
    "\n",
    "rb.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-physics",
   "metadata": {},
   "source": [
    "## 1. Storing and exploring text classification training data\n",
    "\n",
    "Rubrix allows you to log and track data for different NLP tasks (such as `Token Classification` or `Text Classification`). \n",
    "\n",
    "With Rubrix you can track both training data and predictions from models. In this part, we will focus only on training data. Typically, training data is data which has been curated, supervised or annotated by a human with the goal of training a machine learning model. Other terms for this same concept are: ground-truth data, \"gold-standard\" data, or even \"annotated\" data.\n",
    "\n",
    "In this part of the tutorial, you will learn how to use ü§ó datasets library for quick exploration of Text Classification and Token Classification training data. This is useful during model development, for getting a sense of the data, identifying potential issues, debugging, etc. Here we will use rather static \"research \"datasets but Rubrix really shines when you are collecting and using training data in the wild, or in other words in real data science projects.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-intranet",
   "metadata": {},
   "source": [
    "### Text classification with the `tweet_eval` dataset (Emoji classification)\n",
    "\n",
    "Text classification task is all about predicting in which categories a text fits. As if you're shown an image you could quickly tell if there's a dog or a cat in it, we build NLP models to distinguish between a Jane Austen's novel or a Charlotte Bronte's poem. It's all about feeding models with labeled examples and see how it start predicting over the very same labels.\n",
    "\n",
    "In this first case, we are going to play with `tweet_eval`, a dataset with a bunch of tweets from different authors and topics and the sentiment it transmits. This is, in fact, a very common NLP task called Sentiment Analysis, but with a cool tweak: we are representing these sentiments with emojis. Each tweet comes with a number between 0 and 19, which represents different emojis. You can see each one in a cell below or in the [tweet_eval site](https://huggingface.co/datasets/tweet_eval) at ü§ó Hub.\n",
    "\n",
    "First of all, we are going to load the dataset from ü§ó Hub and visualize its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "better-abortion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/tweet_eval/emoji/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", 'emoji', script_version=\"master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naval-category",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ù§',\n",
       " 'üòç',\n",
       " 'üòÇ',\n",
       " 'üíï',\n",
       " 'üî•',\n",
       " 'üòä',\n",
       " 'üòé',\n",
       " '‚ú®',\n",
       " 'üíô',\n",
       " 'üòò',\n",
       " 'üì∑',\n",
       " 'üá∫üá∏',\n",
       " '‚òÄ',\n",
       " 'üíú',\n",
       " 'üòâ',\n",
       " 'üíØ',\n",
       " 'üòÅ',\n",
       " 'üéÑ',\n",
       " 'üì∏',\n",
       " 'üòú']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset['train'].features['label'].names; labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-battlefield",
   "metadata": {},
   "source": [
    "Usually, datasets are divided into train, validation and test splits, and each one of them is used in a certain part of the training. For now, we can stick to the training split, which usually contains the majority of the instances of a dataset. Let's see what's inside!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "educational-mother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0     12  Sunday afternoon walking through Venice in the...\n",
      "1     19  Time for some BBQ and whiskey libations. Chomp...\n",
      "2      0  Love love love all these people Ô∏è Ô∏è Ô∏è #friends...\n",
      "3      0                                Ô∏è Ô∏è Ô∏è Ô∏è @ Toys\"R\"Us\n",
      "4      2  Man these are the funniest kids ever!! That fa...\n"
     ]
    }
   ],
   "source": [
    "with dataset['train'].formatted_as(\"pandas\"):\n",
    "    print(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-healthcare",
   "metadata": {},
   "source": [
    "Now, we are going to create our records from this dataset and log them into rubrix. Rubrix comes with `TextClassificationRecord` and `TokenClassificationRecord` classes, which can be created from a dictionary. These objects passes information to rubrix  about the input of the model, the predictions obtained and the annotations made, as well as a metadata field for other important details. \n",
    "\n",
    "In our case, we haven't predicted anything, so we are only going to include the labels of each instance as annotations, as we know they are the ground truth. We will also include each tweet into inputs, and specify in the metadata section that we are into the training split. Once `records` is populated, we can log it with `rubric.log()`, specifying the name of our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electrical-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for record in dataset['train']:\n",
    "    item = TextClassificationRecord(\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        annotation=TextClassificationAnnotation(\n",
    "            labels=[ClassPrediction(class_label=labels[record[\"label\"]])], \n",
    "            agent=\"https://huggingface.co/datasets/tweet_eval\"),\n",
    "        metadata={\"split\": \"train\"})\n",
    "    records.append(item) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ruled-paragraph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='tweet_eval_emojis', processed=45000, failed=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"tweet_eval_emojis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-montgomery",
   "metadata": {},
   "source": [
    "Thanks to our metadata section in the Text Classification Record, we can go all the way and log tweets from validation and test split too, so we can visualize all three of them in Rubrix UI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "medium-crawford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='tweet_eval_emojis', processed=5000, failed=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_validation = []\n",
    "for record in dataset['validation']:\n",
    "    item = TextClassificationRecord(\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        annotation=TextClassificationAnnotation(\n",
    "            labels=[ClassPrediction(class_label=labels[record[\"label\"]])], \n",
    "            agent=\"https://huggingface.co/datasets/tweet_eval\"),\n",
    "        metadata={\"split\": \"validation\"})\n",
    "    records_validation.append(item) \n",
    "\n",
    "rb.log(records=records_validation, name=\"tweet_eval_emojis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "raised-sally",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Connection error: API is not responding. The API answered with a 500 code: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-51b00830c206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrecords_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecords_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tweet_eval_emojis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/RecognAI/rubrix/src/rubrix/__init__.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(records, name, tags, metadata, chunk_size)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \"\"\"\n\u001b[1;32m     95\u001b[0m     return _client_instance().log(\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RecognAI/rubrix/src/rubrix/client/__init__.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(self, records, name, tags, metadata, chunk_size)\u001b[0m\n\u001b[1;32m    172\u001b[0m             )\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0m_check_response_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mprocessed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mfailed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RecognAI/rubrix/src/rubrix/client/__init__.py\u001b[0m in \u001b[0;36m_check_response_errors\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    248\u001b[0m         raise Exception(\n\u001b[1;32m    249\u001b[0m             \u001b[0;34m\"Connection error: API is not responding. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;34m\"The API answered with a {} code: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_status\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         )\n",
      "\u001b[0;31mException\u001b[0m: Connection error: API is not responding. The API answered with a 500 code: None"
     ]
    }
   ],
   "source": [
    "records_test = []\n",
    "for record in dataset['test']:\n",
    "    item = TextClassificationRecord(\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        annotation=TextClassificationAnnotation(\n",
    "            labels=[ClassPrediction(class_label=labels[record[\"label\"]])], \n",
    "            agent=\"https://huggingface.co/datasets/tweet_eval\"),\n",
    "        metadata={\"split\": \"test\"})\n",
    "    records_test.append(item) \n",
    "\n",
    "rb.log(records=records_test, name=\"tweet_eval_emojis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-vitamin",
   "metadata": {},
   "source": [
    "### Natural language inference with the `MRPC` dataset\n",
    "\n",
    "Natural Language Inference (NLI) is also a very common NLP task, but a little bit different to regular Text Classification. In NLI, the model receives a premise and a hypothesis, and it must figure out if the premise hypothesis is true or not given the premise. We have three categories: entailment (true), contradiction (false) or neutral (undetermined or unrelated). With the premise *\"We live in a flat planet called Earth\"*, the hypothesis *\"The Earth is flat\"* must be classified as entailment, as it is stated in the premise. NLI works with a sort of close-world assumption, in that everything not defined in the premise cannot be suppoused from the real world.\n",
    "\n",
    "Another key difference from Text Classification is that the input come in pairs of two sentences or texts, not only one. Text Classification treats its input as a cohesive and correlated unit, while NLI treats its input as a pair and tries to find correlation. \n",
    "\n",
    "To play around with NLI we are going to use ü§ó Hub [GLUE benchmark](https://huggingface.co/datasets/glue) over the MRPC task. GLUE is a well-known benchmark resource for NLP, and allow us to use its data directly over the Microsoft Research Paraphrase Corpus, a corpus of online news.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "simplified-stationery",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('glue', 'mrpc', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "structural-corruption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'label': 1,\n",
       " 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-chancellor",
   "metadata": {},
   "source": [
    "We can see the two input sentences instead of one. In order to simplify the workflow, let's just test if they are equivalent or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "horizontal-surgery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not_equivalent', 'equivalent']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset.features['label'].names ; labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-holmes",
   "metadata": {},
   "source": [
    "Populating our record list follows the same procedure as in Text Classification, adapting our input to the new scenario of pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wrapped-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "records= []\n",
    "for record in dataset:\n",
    "    item = TextClassificationRecord(\n",
    "        inputs={\"sentence1\": record[\"sentence1\"], \"sentence2\": record[\"sentence2\"]},\n",
    "        annotation=TextClassificationAnnotation(\n",
    "            labels=[ClassPrediction(class_label=labels[record[\"label\"]])], \n",
    "            agent=\"https://huggingface.co/datasets/glue\"),\n",
    "        metadata={\"split\": \"train\"})\n",
    "    records.append(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "virgin-honor",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Connection error: API is not responding. The API answered with a 500 code: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a1366889d70f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mrpc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/RecognAI/rubrix/src/rubrix/__init__.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(records, name, tags, metadata, chunk_size)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \"\"\"\n\u001b[1;32m     95\u001b[0m     return _client_instance().log(\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RecognAI/rubrix/src/rubrix/client/__init__.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(self, records, name, tags, metadata, chunk_size)\u001b[0m\n\u001b[1;32m    172\u001b[0m             )\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0m_check_response_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mprocessed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mfailed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RecognAI/rubrix/src/rubrix/client/__init__.py\u001b[0m in \u001b[0;36m_check_response_errors\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    248\u001b[0m         raise Exception(\n\u001b[1;32m    249\u001b[0m             \u001b[0;34m\"Connection error: API is not responding. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;34m\"The API answered with a {} code: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_status\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         )\n",
      "\u001b[0;31mException\u001b[0m: Connection error: API is not responding. The API answered with a 500 code: None"
     ]
    }
   ],
   "source": [
    "rb.log(records=records[0:10], name=\"mrpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-spanish",
   "metadata": {},
   "source": [
    "#### TODO: exploration \n",
    "[we need to define the best way to show the webapp using these use cases, show basic features, etc.: short videos? screenshots? demo instance?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-princeton",
   "metadata": {},
   "source": [
    "### Multilabel text classification with `go_emotions` dataset\n",
    "\n",
    "Another similar task to Text Classification, but yet a bit different, is Multilabel Text Classification. Just one key difference: more than one label may be predicted. While in a regular Text Classification task we may decide that the tweet *\"I can't wait to travel to Egypts and visit the pyramids\"* fits into the hastag **#Travel**, which is accurate, in Multilabel Text Classification we can classify it as more than one hastag, like **#Travel #History #Africa #Sightseeing #Desert**.\n",
    "\n",
    "In Text Classification, the category with the highest score (which our model predicted) is going to be the category predicted, but in this task we need to establish a threshold, a value between 0 and 1, from which we will classify the labels as predictions or not. If we set it to 0.5, only categories with more than a 0.5 probability value will be considered predictions. \n",
    "\n",
    "To get used to this task and see how we can log data to Rubrix, we are going to use ü§ó Hub [go_emotions dataset](https://huggingface.co/datasets/go_emotions), with comments from different reddit forums and an associated sentiment (this experiment would also be considered Sentiment Analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beautiful-champagne",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Reusing dataset go_emotions (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/ef1c18ea192c771555f1e0d638889dd5f1896255782c57c6a0b934d5f94f779e)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('go_emotions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-battle",
   "metadata": {},
   "source": [
    "Here's an example of an instance of the datasets, and the different labels, ordered. Each label will be represented in the dataset as a number, but we will translate to its name before logging to rubrix, to see things more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "plastic-louis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'eebbqej',\n",
       " 'labels': [27],\n",
       " 'text': \"My favourite food is anything I didn't have to cook myself.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "persistent-antique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset['train'].features['labels'].feature.names; labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-penny",
   "metadata": {},
   "source": [
    "Now, we need to add a confidence value to our annotation, from 0 to 1. As these are all ground truths, we consider they have the maximum probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "verbal-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "records= []\n",
    "for record in dataset['train']:\n",
    "    item = TextClassificationRecord(\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        annotation=TextClassificationAnnotation(\n",
    "            labels=[ClassPrediction(class_label=labels[cls], confidence= 1) for cls in record['labels']], \n",
    "            agent=\"https://huggingface.co/datasets/go_emotions\"),\n",
    "        metadata={\"split\": \"train\"})\n",
    "    records.append(item) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-hardwood",
   "metadata": {},
   "source": [
    "And logging is just as easy as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unexpected-rotation",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Connection error: API is not responding. The API answered with a 500 code: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-51ac7c0ad883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"go_emotions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/RecognAI/rubrix/src/rubrix/__init__.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(records, name, tags, metadata, chunk_size)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \"\"\"\n\u001b[1;32m     95\u001b[0m     return _client_instance().log(\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RecognAI/rubrix/src/rubrix/client/__init__.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(self, records, name, tags, metadata, chunk_size)\u001b[0m\n\u001b[1;32m    172\u001b[0m             )\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0m_check_response_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mprocessed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mfailed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RecognAI/rubrix/src/rubrix/client/__init__.py\u001b[0m in \u001b[0;36m_check_response_errors\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    248\u001b[0m         raise Exception(\n\u001b[1;32m    249\u001b[0m             \u001b[0;34m\"Connection error: API is not responding. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;34m\"The API answered with a {} code: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_status\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         )\n",
      "\u001b[0;31mException\u001b[0m: Connection error: API is not responding. The API answered with a 500 code: None"
     ]
    }
   ],
   "source": [
    "rb.log(records=records[0:10], name=\"go_emotions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-delivery",
   "metadata": {},
   "source": [
    "## 2. Storing and exploring token classification training data\n",
    "\n",
    "In this second part, we will cover Token Classification while still using ü§ó datasets library. These kind of NLP tasks aim to divide the input text into words, or syllabes, and assign certain values to them. Think about giving each word in a sentence its gramatical category, or highlight which parts of a medical report belong to a certain speciality. \n",
    "\n",
    "We are going to cover a few cases using ü§ó datasets, and see how `TokenClassificationRecord` allows us to log data in rubrix in a similar fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-pioneer",
   "metadata": {},
   "source": [
    "### Named-Entity Recognition with `wnut17` dataset\n",
    "\n",
    "Named-Entity Recognition (NER) seeks to locate and classify named entities metioned in unstructured text into pre-defined categories. And, what's powerful about NER is that this predefined categories can be whatever we want. Maybe gramatical categories, and be the best at syntax analysis in our English class, maybe person names, or organizations, or even medical codes.\n",
    "\n",
    "For this case, we are going to use ü§ó Hub [WNUT 17 dataset](https://huggingface.co/datasets/wnut_17), about rare entities on written text. Take for example the tweet ‚Äúso.. kktny in 30 mins?‚Äù - even human experts find entity kktny hard to detect and resolve. This task will evaluate the ability to detect and classify novel, emerging, singleton named entities in written text.\n",
    "\n",
    "As always, let's first dive into the data and see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "affected-satin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wnut_17 (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/983205ce50100b6e8fce4b3d402f36dce9b206f736e1a630c78fb25e1d23b9e8)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wnut_17\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "opposite-cloud",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'tokens': ['@paulwalk',\n",
       "  'It',\n",
       "  \"'s\",\n",
       "  'the',\n",
       "  'view',\n",
       "  'from',\n",
       "  'where',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'living',\n",
       "  'for',\n",
       "  'two',\n",
       "  'weeks',\n",
       "  '.',\n",
       "  'Empire',\n",
       "  'State',\n",
       "  'Building',\n",
       "  '=',\n",
       "  'ESB',\n",
       "  '.',\n",
       "  'Pretty',\n",
       "  'bad',\n",
       "  'storm',\n",
       "  'here',\n",
       "  'last',\n",
       "  'evening',\n",
       "  '.']}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-proportion",
   "metadata": {},
   "source": [
    "We can see a list of tags and the tokens they are refering to. We have the following rare entities in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cultural-channels",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empire: B-location\n",
      "State: I-location\n",
      "Building: I-location\n",
      "ESB: B-location\n"
     ]
    }
   ],
   "source": [
    "for entity, token in zip(dataset[0][\"ner_tags\"], dataset[0][\"tokens\"]):\n",
    "    if entity != 0:\n",
    "        print(f\"\"\"{token}: {dataset.features[\"ner_tags\"].feature.names[entity]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-continent",
   "metadata": {},
   "source": [
    "So, it make a lot of sense to translate these tags into NER tags, which are much more self-explanatory than an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "composed-institute",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/ignaciotalaveracepeda/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/983205ce50100b6e8fce4b3d402f36dce9b206f736e1a630c78fb25e1d23b9e8/cache-3ed11e136ed38c6e.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda instance: {\"ner_tags_translated\": [dataset.features[\"ner_tags\"].feature.names[tag] for tag in instance[\"ner_tags\"]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-vessel",
   "metadata": {},
   "source": [
    "What we did is a mapping function over ü§ó  dataset, which allow us to make changes in every instance of the dataset. The very same instance that we printed before is much more readable now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "political-theory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'ner_tags_translated': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-location',\n",
       "  'I-location',\n",
       "  'I-location',\n",
       "  'O',\n",
       "  'B-location',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " 'tokens': ['@paulwalk',\n",
       "  'It',\n",
       "  \"'s\",\n",
       "  'the',\n",
       "  'view',\n",
       "  'from',\n",
       "  'where',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'living',\n",
       "  'for',\n",
       "  'two',\n",
       "  'weeks',\n",
       "  '.',\n",
       "  'Empire',\n",
       "  'State',\n",
       "  'Building',\n",
       "  '=',\n",
       "  'ESB',\n",
       "  '.',\n",
       "  'Pretty',\n",
       "  'bad',\n",
       "  'storm',\n",
       "  'here',\n",
       "  'last',\n",
       "  'evening',\n",
       "  '.']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-bullet",
   "metadata": {},
   "source": [
    "Info about the meaning of the tags is available [here](https://huggingface.co/datasets/viewer/?dataset=wnut_17), but to sum up, *Empire* and *ESB* has been classified as **B-LOC**, or beggining of a location name, *State* and *Building* has been classified as **I-LOC** or intermediate/final of a location name.\n",
    "\n",
    "We need to transform a bit this information, providing an entity annotation, and for that use we will use `TokenClassificationAnnotation` objects. We can pass them entities, agents, scores and additional information. In our case, we will focus on population an entity list for each record in the dataset, with the following structure:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"start\": #position of the first character of the entity in the sentence\n",
    "    \"end\": #position of the last character of the entity in the sentence\n",
    "    \"start_token\": #position of the first token of the entity in the token list\n",
    "    \"end_token\": #position of the last token of the entity in the token list\n",
    "    \"label\": #token tag\n",
    "}\n",
    "```\n",
    "\n",
    "Let's create a function that transform our dataset records into entities. It's a bit weird, but don't worry! What's doing inside is getting the entities information as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "engaging-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entities(record):\n",
    "\n",
    "    entities = []\n",
    "    counter = 0\n",
    "    for i in range(len(record[\"ner_tags\"])):\n",
    "        \n",
    "        if record[\"ner_tags_translated\"][i][0] == 'B':\n",
    "            label = record[\"ner_tags_translated\"][i][2:]\n",
    "\n",
    "            inner_counter = counter\n",
    "        \n",
    "            for j in range(i, len(record[\"ner_tags\"])):\n",
    "                \n",
    "                if record[\"ner_tags_translated\"][j][0] != 'I' and j!=i:\n",
    "\n",
    "                    entities.append({\n",
    "                        \"start\": counter,\n",
    "                        \"end\": inner_counter-1,\n",
    "                        \"start_token\": i,\n",
    "                        \"end_token\": j,\n",
    "                        \"label\": label})\n",
    "                    break\n",
    "                \n",
    "                inner_counter += len(record[\"tokens\"][j]) + 1\n",
    "\n",
    "        counter += len(record[\"tokens\"][i]) + 1\n",
    "        \n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-manufacturer",
   "metadata": {},
   "source": [
    "Let's proceed and create a record list to log it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "weighted-heath",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for record in dataset:\n",
    "\n",
    "    entities = parse_entities(record)\n",
    "\n",
    "    item = TokenClassificationRecord(\n",
    "         annotation= TokenClassificationAnnotation(\n",
    "             agent= \"https://huggingface.co/datasets/wnut_17\",\n",
    "             entities= entities\n",
    "         ),\n",
    "        raw_text=\" \".join(record[\"tokens\"]),\n",
    "        tokens = record[\"tokens\"],\n",
    "        metadata= {\"split\": \"train\"})\n",
    "\n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "acoustic-comedy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassificationRecord(tokens=['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], raw_text=\"@paulwalk It 's the view from where I 'm living for two weeks . Empire State Building = ESB . Pretty bad storm here last evening .\", prediction=None, annotation=TokenClassificationAnnotation(agent='https://huggingface.co/datasets/wnut_17', entities=[{'start': 64, 'end': 85, 'start_token': 14, 'end_token': 17, 'label': 'location'}, {'start': 88, 'end': 91, 'start_token': 18, 'end_token': 19, 'label': 'location'}], score=None), id=None, metadata={'split': 'train'}, status=None, event_timestamp=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "helpful-advocacy",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='ner_wnut_17', processed=10, failed=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records[0:10], name=\"ner_wnut_17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-program",
   "metadata": {},
   "source": [
    "### Part of speech tagging with `conll2003` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-patrol",
   "metadata": {},
   "source": [
    "Another cool NLP task related with tokens is Part-of-Speech tagging (POS tagging). In it we will identify names, verbs, adverbs, adjectives...based on the context and the meaning of the words. It is a little bit trickier than having a huge dictionary where we can look up that *drink* is a verb and *dog* is a name. Many words change its gramatical type according to the context of the sentence, and here is where AI comes to save the day.\n",
    "\n",
    "With just our dictionary and a regular script, *dog* in `The sailor dogs the hatch.` would be classified as a name, because *dog* is a name, right? A trained NLP model would step up and say *No! That's is a very common example to ilustrate the ambiguity of words. It is a verb!*. Or maybe it would just say *verb*. That's up to you.\n",
    "\n",
    "In this [dataset](https://huggingface.co/datasets/conll2003) from ü§ó hub, we will see how differente sentence has POS and NER tags, and how we can log this POS tag information into Rubrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sonic-reflection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"conll2003\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "understood-charter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'id': '0',\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-postcard",
   "metadata": {},
   "source": [
    "Each POS and NER tag are represented by a number. In `dataset.features` we can see to which tag they refer (this [link](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) may serve you to look up the meaning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "choice-artwork",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(num_classes=47, names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], names_file=None, id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(num_classes=23, names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], names_file=None, id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-jefferson",
   "metadata": {},
   "source": [
    "The following function will help us create the entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "continent-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entities_POS(record):\n",
    "\n",
    "    entities = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(len(record[\"pos_tags\"])):\n",
    "        \n",
    "        entities.append({\n",
    "            \"start\": counter,\n",
    "            \"end\": counter + len(record[\"tokens\"][i]),\n",
    "            \"label\": dataset.features[\"pos_tags\"].feature.names[record[\"pos_tags\"][i]],\n",
    "        })\n",
    "\n",
    "        counter += len(record[\"tokens\"][i]) + 1\n",
    "        \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "amended-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for record in dataset:\n",
    "\n",
    "    entities = parse_entities_POS(record)\n",
    "\n",
    "    item = TokenClassificationRecord(\n",
    "         annotation= TokenClassificationAnnotation(\n",
    "             agent= \"https://huggingface.co/datasets/conll2003\",\n",
    "             entities= entities\n",
    "         ),\n",
    "        raw_text=\" \".join(record[\"tokens\"]),\n",
    "        tokens = record[\"tokens\"],\n",
    "        metadata= {\"split\": \"train\"})\n",
    "\n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "derived-billy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='conll2003', processed=10, failed=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records[0:10], name=\"conll2003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-camping",
   "metadata": {},
   "source": [
    "And so it is done! We have logged data from 5 different type of experiments, which now can be visualized in Rubrix UI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-halloween",
   "metadata": {},
   "source": [
    "## 3. Exploring predictions\n",
    "\n",
    "In this third part of the tutorial we are going to focus on loading a lot of predictions and annotations into Rubrix and visualize them from the UI. This process is something esential in any ML project, and Rubrix let us play with the data in many different ways: visualizing by predicted class, by annotated class, by split, selecting which ones were wrongly classified, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-hormone",
   "metadata": {},
   "source": [
    "### Agnews and zeroshot classification\n",
    "\n",
    "To explore some logged data on Rubrix UI, we are going to predict the topic of some news with a zero-shot classifier (that we don't need to train), and compare the predicted category with the ground truth. The dataset we are going to use in this part is [ag_news](https://huggingface.co/datasets/ag_news), with information of over 1 million articles written in English.\n",
    "\n",
    "First of all, as always, we are going to load the dataset from ü§ó Hub and visualize its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compound-caution",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset  \n",
    "\n",
    "dataset = load_dataset(\"ag_news\", split='test[:20%]') # 20% is over 1500 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "improving-printing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 2,\n",
       " 'text': \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "advisory-works",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=4, names=['World', 'Sports', 'Business', 'Sci/Tech'], names_file=None, id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-worship",
   "metadata": {},
   "source": [
    "This dataset has articles from four different classes, so we can define a category list, which may come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "upset-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-retention",
   "metadata": {},
   "source": [
    "Now, its time to load our zero-shot classificator model. We present to options:\n",
    "\n",
    "1. [DistilBart-MNLI](https://huggingface.co/valhalla/distilbart-mnli-12-1)\n",
    "2. [BERT-tiny](https://huggingface.co/prajjwal1/bert-tiny)\n",
    "\n",
    "With the first model, the obtained results are probably going to be better, but it is a larger model, which could take longer to use. We are going to stick with the first one, but feel free to change it, and even to compare them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "desperate-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"valhalla/distilbart-mnli-12-1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"valhalla/distilbart-mnli-12-1\")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "#model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "\n",
    "pl = pipeline('zero-shot-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-hawaiian",
   "metadata": {},
   "source": [
    "Let's try to make a quick prediction and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continent-guest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\",\n",
       " 'labels': ['Business', 'World', 'Sports', 'Sci/Tech'],\n",
       " 'scores': [0.765126645565033,\n",
       "  0.14847052097320557,\n",
       "  0.04636916145682335,\n",
       "  0.040033720433712006]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl(dataset[0]['text'], ['World', 'Sports', 'Business', 'Sci/Tech'], hypothesis_template='This example is {}.',multi_class=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-journal",
   "metadata": {},
   "source": [
    "Knowing how to make a prediction, we can now apply this to the whole selected dataset. Here, we also present you with two options:\n",
    "\n",
    "1. Traverse through all records in the dataset, predict each record and log it to Rubrix.\n",
    "2. Apply a map function to make the predictions and add that field to each record, and then log it as a whole to Rubrix.\n",
    "\n",
    "In the following categories, each approach is presented. You choose what you like the most, or even both (be careful with the time and the duplicated records, though!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-correspondence",
   "metadata": {},
   "source": [
    "### First approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "discrete-briefing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1520 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4f08a694fc31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Make the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'This example is {}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Create the prediction object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/transformers/pipelines/zero_shot_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sequences, candidate_labels, hypothesis_template, multi_class)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mnum_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mcandidate_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_and_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, inputs, return_tensors)\u001b[0m\n\u001b[1;32m    618\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         )\n\u001b[1;32m   1316\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# last hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m             )\n\u001b[1;32m   1053\u001b[0m         \u001b[0;31m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    736\u001b[0m                     )\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                     \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for record in tqdm(dataset):\n",
    "    \n",
    "    # Make the prediction\n",
    "    model_output = pl(record['text'], categories, hypothesis_template='This example is {}.')\n",
    "    \n",
    "    # Create the prediction object\n",
    "    prediction = TextClassificationAnnotation(\n",
    "        labels=[ClassPrediction(class_label=model_output['labels'][i], confidence=model_output['scores'][i]) for i in range(len(model_output['labels']))], \n",
    "        agent=\"https://huggingface.co/valhalla/distilbart-mnli-12-1\",\n",
    "        )\n",
    "    \n",
    "    # Create the annotation object\n",
    "    annotation = TextClassificationAnnotation(\n",
    "        labels=[ClassPrediction(class_label=categories[record[\"label\"]])], \n",
    "        agent=\"https://huggingface.co/datasets/ag_news\",\n",
    "        )\n",
    "    \n",
    "    # Create the item to log\n",
    "    item = TextClassificationRecord(\n",
    "        inputs={'text': record['text']},\n",
    "        prediction=prediction,\n",
    "        annotation=annotation,\n",
    "        metadata={'split':'train'})\n",
    "    \n",
    "    # Log to rubrix\n",
    "    rb.log(records=item, name=\"ag_news\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-notebook",
   "metadata": {},
   "source": [
    "### Second approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "global-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predictions(records):\n",
    "    \n",
    "    predictions = pl([record for record in records['text']], categories, hypothesis_template='This example is {}.')\n",
    "    \n",
    "    \n",
    "    if isinstance(predictions, list):\n",
    "        return {\"labels_predicted\": [pred[\"labels\"] for pred in predictions], \"probabilities_predicted\": [pred[\"scores\"] for pred in predictions]}\n",
    "    else:\n",
    "        return {\"labels_predicted\": predictions[\"labels\"], \"probabilities_predicted\": predictions[\"scores\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "solar-minute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51419b139b864d6e86545def56ce9a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=380.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_predicted = dataset.map(add_predictions, batched=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "returning-collection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 2,\n",
       " 'labels_predicted': ['Business', 'World', 'Sports', 'Sci/Tech'],\n",
       " 'probabilities_predicted': [0.7651262879371643,\n",
       "  0.1484706699848175,\n",
       "  0.0463692881166935,\n",
       "  0.040033772587776184],\n",
       " 'text': \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "compressed-newcastle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 32/1520 [00:40<31:42,  1.28s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f98da491caf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Log to rubrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ag_news\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/RecognAI/rubrix/src/rubrix/__init__.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(records, name, tags, metadata, chunk_size)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     return _client_instance().log(\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     )\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RecognAI/rubrix/src/rubrix/client/__init__.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(self, records, name, tags, metadata, chunk_size)\u001b[0m\n\u001b[1;32m    174\u001b[0m                     records=[\n\u001b[1;32m    175\u001b[0m                         \u001b[0mrecord_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                     ],\n\u001b[1;32m    178\u001b[0m                 ),\n",
      "\u001b[0;32m~/Documents/RecognAI/rubrix/src/rubrix/sdk/api/text_classification/bulk_records.py\u001b[0m in \u001b[0;36msync_detailed\u001b[0;34m(client, name, json_body)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     response = httpx.post(\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpx/_api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, content, data, files, json, params, headers, cookies, auth, proxies, allow_redirects, cert, verify, timeout, trust_env)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mtrust_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpx/_api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, params, content, data, files, json, headers, cookies, auth, proxies, timeout, allow_redirects, verify, cert, trust_env)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mcookies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         )\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    720\u001b[0m         )\n\u001b[1;32m    721\u001b[0m         return self.send(\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         )\n\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m         )\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, timeout, allow_redirects, history)\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m             )\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, timeout, allow_redirects, history)\u001b[0m\n\u001b[1;32m    821\u001b[0m                 )\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request, timeout)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m             )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, ext)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 response = connection.request(\n\u001b[0;32m--> 201\u001b[0;31m                     \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 )\n\u001b[1;32m    203\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNewConnectionRequired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, ext)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;34m\"connection.request method=%r url=%r headers=%r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         )\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimeoutDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSyncSocketStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, ext)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mreason_phrase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         ) = self._receive_response(timeout)\n\u001b[0m\u001b[1;32m     73\u001b[0m         response_stream = IteratorByteStream(\n\u001b[1;32m     74\u001b[0m             \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_response_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh11_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/site-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n, timeout)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimeoutDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1054\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rubrix/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for record in tqdm(dataset_predicted):\n",
    "    # Create the prediction object\n",
    "    prediction = TextClassificationAnnotation(\n",
    "        labels=[ClassPrediction(class_label=predicted_label, confidence=predicted_probability) \n",
    "                for predicted_label, predicted_probability in zip(record['labels_predicted'], record['probabilities_predicted'])], \n",
    "        agent=\"https://huggingface.co/valhalla/distilbart-mnli-12-1\",\n",
    "        )\n",
    "    \n",
    "    # Create the annotation object\n",
    "    annotation = TextClassificationAnnotation(\n",
    "        labels=[ClassPrediction(class_label=categories[record[\"label\"]])], \n",
    "        agent=\"https://huggingface.co/datasets/ag_news\",\n",
    "        )\n",
    "    \n",
    "    # Create the item to log\n",
    "    item = TextClassificationRecord(\n",
    "        inputs={'text': record['text']},\n",
    "        prediction=prediction,\n",
    "        annotation=annotation,\n",
    "        metadata={'split':'train'})\n",
    "    \n",
    "    # Log to rubrix\n",
    "    rb.log(records=item, name=\"ag_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-characterization",
   "metadata": {},
   "source": [
    "### Pretrained NER with conll2003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-cement",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we have learnt to:\n",
    "\n",
    "* bla bla\n",
    "* bla bla\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-sight",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Invite the reader to join the commu\n",
    "\n",
    "Point the reader to other materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-exhibit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}