{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Rubrix with Spacy v3\n",
    "\n",
    "In this tutorial, we will walk through the process of using Rubrix to log NER predictions from Spacy v3.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Our goal is to show you how explore with Rubrix predictions made with Spacy**, facing a NER task. We will use the new capabilities of Spacy v3, integrating Transformers to the pipeline.\n",
    "\n",
    "## Installations and imports\n",
    "\n",
    "We are going to need Huggingface Dataset (as we are going to use a dataset from Huggingface Hub), Spacy (and we will make sure it is upgraded to v3), and an English transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install datasets \n",
    "!pip install --upgrade spacy\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are importing rubrix ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And don't forget to import spacy and load the model we just downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we are going to use the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from Huggingface Hub. It contains all explicit time references in a dataset of 52,183 novels whose full text is available via Project Gutenberg. From extracts of novels, we are surely going to find some NER entities. Well, technically, spacy is going to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset gutenberg_time (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/gutenberg_time/gutenberg/0.0.0/f39225c596bc6125cf8ffcfb61ea510ae8dfa497a3ba392cd80d6893ebf715ee)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gutenberg_time\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preview\n",
    "\n",
    "Let's take a look at our dataset! Starting by the length of it and an sneak peek to one instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'guten_id': '4447',\n",
       " 'hour_reference': '5',\n",
       " 'is_ambiguous': True,\n",
       " 'time_phrase': \"five o'clock\",\n",
       " 'time_pos_end': 147,\n",
       " 'time_pos_start': 145,\n",
       " 'tok_context': \"I crossed the ground she had traversed , noting every feature surrounding it , the curving wheel-track , the thin prickly sand-herbage , the wave - mounds , the sparse wet shells and pebbles , the gleaming flatness of the water , and the vast horizon-boundary of pale flat land level with shore , looking like a dead sister of the sea . By a careful examination of my watch and the sun 's altitude , I was able to calculate what would , in all likelihood , have been his height above yonder waves when her chair was turned toward the city , at a point I reached in the track . But of the matter then simultaneously occupying my mind , to recover which was the second supreme task I proposed to myself-of what . I also was thinking upon the stroke of five o'clock , I could recollect nothing . I could not even recollect whether I happened to be looking on sun and waves when she must have had them full and glorious in her face . With the heartiest consent I could give , and a blank cheque , my father returned to England to hire forthwith a commodious yacht , fitted and manned . Before going he discoursed of prudence in our expenditure ; though not for the sake of the mere money in hand , which was a trifle , barely more than the half of my future income ; but that the squire , should he by and by bethink him of inspecting our affairs , might perceive we were not spendthrifts .\"}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['guten_id', 'hour_reference', 'time_phrase', 'is_ambiguous', 'time_pos_start', 'time_pos_end', 'tok_context'],\n",
       "    num_rows: 120694\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, maybe those are some big numbers. For the shake of the tutorial, let's reduce it a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['guten_id', 'hour_reference', 'time_phrase', 'is_ambiguous', 'time_pos_start', 'time_pos_end', 'tok_context'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "dataset = dataset.select(range(20))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how to create a spacy doc. An spacy doc is nothing more than an array of tokens, which spacy obtains from the Transformer model we just loaded. With the spacy doc, and using the power of the pretrained Transformer, we will find and mark our NER entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "I crossed the ground she had traversed , noting every feature surrounding it , the curving wheel-track , the thin prickly sand-herbage , the wave - mounds , the sparse wet shells and pebbles , the gleaming flatness of the water , and the vast horizon-boundary of pale flat land level with shore , looking like a dead sister of the sea . By a careful examination of my watch and the sun 's altitude , I was able to calculate what would , in all likelihood , have been his height above yonder waves when her chair was turned toward the city , at a point I reached in the track . But of the matter then simultaneously occupying my mind , to recover which was the second supreme task I proposed to myself-of what . I also was thinking upon the stroke of five o'clock , I could recollect nothing . I could not even recollect whether I happened to be looking on sun and waves when she must have had them full and glorious in her face . With the heartiest consent I could give , and a blank cheque , my father returned to England to hire forthwith a commodious yacht , fitted and manned . Before going he discoursed of prudence in our expenditure ; though not for the sake of the mere money in hand , which was a trifle , barely more than the half of my future income ; but that the squire , should he by and by bethink him of inspecting our affairs , might perceive we were not spendthrifts ."
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "doc = nlp(dataset[0][\"tok_context\"])\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy's doc are obtained from a string, so we will need to make this process iteratively through all of our dataset. But the good thing is that, once the tokens are obtained, we have all the info we need to log our data into rubrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "records = []    # Creating and empty record list to save all the records\n",
    "\n",
    "for record in dataset:\n",
    "\n",
    "    record = record[\"tok_context\"]  # We only need the text of each instance\n",
    "    entities = []   # Here we will store all the entities of each single record\n",
    "\n",
    "    doc= nlp(record)    # Spacy Doc creation\n",
    "    \n",
    "    # Storing all entity tuples in our list\n",
    "    for ent in doc.ents:\n",
    "        entities.append((str(ent.label_), ent.start_char, ent.end_char))\n",
    "\n",
    "    # Storing all tokens of the record\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        tokens.append(str(token))\n",
    "\n",
    "    # Rubrix TokenClassificationRecord append\n",
    "    records.append(rb.TokenClassificationRecord(\n",
    "        text=record,\n",
    "        tokens=tokens,\n",
    "        prediction=entities,\n",
    "        prediction_agent=\"spacy v3\",\n",
    "        metadata={\n",
    "            \"split\": \"train\",\n",
    "            },\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has made the prediction for us, and we can just log them right away! Remember that predictions, and annotations, must be tuples of ```(label, start_character, end_character)``` stored in a list.\n",
    "\n",
    "Let's log!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BulkResponse(dataset='gutenberg', processed=20, failed=0)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "rb.log(records=records, name=\"gutenberg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd00f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd",
   "display_name": "Python 3.8.8 64-bit ('rubrix': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "0f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}