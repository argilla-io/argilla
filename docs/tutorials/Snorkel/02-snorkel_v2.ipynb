{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "legitimate-jenny",
   "metadata": {
    "tags": []
   },
   "source": [
    "# üêô Using Rubrix and Snorkel for human-in-the-loop weak supervision\n",
    "\n",
    "In this tutorial, we will walk through the process of using Rubrix to improve weak supervision and data programming workflows with the amazing Snorkel library.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Our goal is to show you how you can incorporate Rubrix into data programming workflows** to programatically build training data with a human-in-the-loop approach. We will use the widely-known [Snorkel](https://www.snorkel.org/) library, but a similar approach can be used with other data augmentation libraries such as [Textattack](https://github.com/QData/TextAttack) or [nlpaug](https://github.com/makcedward/nlpaug).\n",
    "\n",
    "### What is weak supervision? and Snorkel?\n",
    "\n",
    "Weak supervision is a branch of machine learning based on getting lower quality labels more efficiently. We can achieve this by using Snorkel, a library for programmatically building and managing training datasets without manual labeling.\n",
    "\n",
    "### This tutorial\n",
    "\n",
    "In this tutorial, we will follow the [Spam classification tutorial](https://www.snorkel.org/use-cases/01-spam-tutorial) from Snorkel's documentation and show you how to extend weak supervision workflows with Rubrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-necessity",
   "metadata": {},
   "source": [
    "The tutorial is organized into:\n",
    "\n",
    "1. **Spam classification with Snorkel**: we provide a brief overview of the tutorial\n",
    "\n",
    "2. **Extending and finding labeling functions with Rubrix**: we analyze different strategies for extending the proposed labeling functions and for exploring new labeling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-nation",
   "metadata": {},
   "source": [
    "## Install Snorkel, Textblob and spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prostate-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snorkel textblob spacy -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "worldwide-digest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-roommate",
   "metadata": {},
   "source": [
    "# 2. Spam classification with Snorkel\n",
    "\n",
    "Rubrix allows you to log and track data for different NLP tasks (such as `Token Classification` or `Text Classification`). \n",
    "\n",
    "In this tutorial, we will use the [YouTube Spam Collection](http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/) dataset which a binary classification task for detecting spam comments in youtube videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-strike",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "We have 2 datasets, the training and the test. The first one does not include the label of the samples and it is set to -1. Something different happens with the test, where the label is set to 1 if it's considered SPAM and 0 for HAM.  \n",
    "\n",
    "Let's load it in Pandas and take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amber-volleyball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Alessandro leite</td>\n",
       "      <td>2014-11-05T22:21:36</td>\n",
       "      <td>pls http://www10.vakinha.com.br/VaquinhaE.aspx...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Salim Tayara</td>\n",
       "      <td>2014-11-02T14:33:30</td>\n",
       "      <td>if your like drones, plz subscribe to Kamal Ta...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Phuc Ly</td>\n",
       "      <td>2014-01-20T15:27:47</td>\n",
       "      <td>go here to check the views :3Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DropShotSk8r</td>\n",
       "      <td>2014-01-19T04:27:18</td>\n",
       "      <td>Came here to check the views, goodbye.Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>css403</td>\n",
       "      <td>2014-11-07T14:25:48</td>\n",
       "      <td>i am 2,126,492,636 viewer :DÔªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>443</td>\n",
       "      <td>Themayerlife</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check out my mummy chanel!</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>444</td>\n",
       "      <td>Fill Reseni</td>\n",
       "      <td>2015-05-27T17:10:53.724000</td>\n",
       "      <td>The rap: cool     Rihanna: STTUUPIDÔªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>445</td>\n",
       "      <td>Greg Fils Aim√©</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I hope everyone is in good spirits I&amp;#39;m a h...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>446</td>\n",
       "      <td>Lil M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lil m !!!!! Check hi out!!!!! Does live the wa...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>447</td>\n",
       "      <td>AvidorFilms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please check out my youtube channel! Just uplo...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1586 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0            author                        date  \\\n",
       "0              0  Alessandro leite         2014-11-05T22:21:36   \n",
       "1              1      Salim Tayara         2014-11-02T14:33:30   \n",
       "2              2           Phuc Ly         2014-01-20T15:27:47   \n",
       "3              3      DropShotSk8r         2014-01-19T04:27:18   \n",
       "4              4            css403         2014-11-07T14:25:48   \n",
       "...          ...               ...                         ...   \n",
       "1581         443      Themayerlife                         NaN   \n",
       "1582         444       Fill Reseni  2015-05-27T17:10:53.724000   \n",
       "1583         445    Greg Fils Aim√©                         NaN   \n",
       "1584         446             Lil M                         NaN   \n",
       "1585         447       AvidorFilms                         NaN   \n",
       "\n",
       "                                                   text  label  video  \n",
       "0     pls http://www10.vakinha.com.br/VaquinhaE.aspx...   -1.0      1  \n",
       "1     if your like drones, plz subscribe to Kamal Ta...   -1.0      1  \n",
       "2                        go here to check the views :3Ôªø   -1.0      1  \n",
       "3               Came here to check the views, goodbye.Ôªø   -1.0      1  \n",
       "4                         i am 2,126,492,636 viewer :DÔªø   -1.0      1  \n",
       "...                                                 ...    ...    ...  \n",
       "1581                         Check out my mummy chanel!   -1.0      4  \n",
       "1582               The rap: cool     Rihanna: STTUUPIDÔªø   -1.0      4  \n",
       "1583  I hope everyone is in good spirits I&#39;m a h...   -1.0      4  \n",
       "1584  Lil m !!!!! Check hi out!!!!! Does live the wa...   -1.0      4  \n",
       "1585  Please check out my youtube channel! Just uplo...   -1.0      4  \n",
       "\n",
       "[1586 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>‚Ä´ÿ≠ŸÑŸÖ ÿßŸÑÿ¥ÿ®ÿßÿ®‚Ä¨‚Äé</td>\n",
       "      <td>2015-05-25T23:42:49.533000</td>\n",
       "      <td>Check out this video on YouTube:Ôªø</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194</td>\n",
       "      <td>MOHAMED THASLEEM</td>\n",
       "      <td>2015-05-24T07:03:59.488000</td>\n",
       "      <td>super musicÔªø</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>277</td>\n",
       "      <td>AlabaGames</td>\n",
       "      <td>2015-05-22T00:31:43.922000</td>\n",
       "      <td>Subscribe my channel ¬†I RECORDING FIFA 15 GOAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>Manish Ray</td>\n",
       "      <td>2015-05-23T08:55:07.512000</td>\n",
       "      <td>This song is so beauty</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>163</td>\n",
       "      <td>Sudheer Yadav</td>\n",
       "      <td>2015-05-28T10:28:25.133000</td>\n",
       "      <td>SEE SOME MORE SONG OPEN GOOGLE AND TYPE Shakir...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>32</td>\n",
       "      <td>GamezZ MTA</td>\n",
       "      <td>2015-05-09T00:08:26.185000</td>\n",
       "      <td>Pleas subscribe my channelÔªø</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>176</td>\n",
       "      <td>Viv Varghese</td>\n",
       "      <td>2015-05-25T08:59:50.837000</td>\n",
       "      <td>The best FIFA world cup song for sure.Ôªø</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>314</td>\n",
       "      <td>yakikukamo FIRELOVER</td>\n",
       "      <td>2013-07-18T17:07:06.152000</td>\n",
       "      <td>hey you ! check out the channel of Alvar Lake !!</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>25</td>\n",
       "      <td>James Cook</td>\n",
       "      <td>2013-10-10T18:08:07.815000</td>\n",
       "      <td>Hello Guys...I Found a Way to Make Money Onlin...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>11</td>\n",
       "      <td>Trulee IsNotAmazing</td>\n",
       "      <td>2013-09-07T14:18:22.601000</td>\n",
       "      <td>Beautiful song beautiful girl it works</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                author                        date  \\\n",
       "0            27         ‚Ä´ÿ≠ŸÑŸÖ ÿßŸÑÿ¥ÿ®ÿßÿ®‚Ä¨‚Äé  2015-05-25T23:42:49.533000   \n",
       "1           194      MOHAMED THASLEEM  2015-05-24T07:03:59.488000   \n",
       "2           277            AlabaGames  2015-05-22T00:31:43.922000   \n",
       "3           132            Manish Ray  2015-05-23T08:55:07.512000   \n",
       "4           163         Sudheer Yadav  2015-05-28T10:28:25.133000   \n",
       "..          ...                   ...                         ...   \n",
       "245          32            GamezZ MTA  2015-05-09T00:08:26.185000   \n",
       "246         176          Viv Varghese  2015-05-25T08:59:50.837000   \n",
       "247         314  yakikukamo FIRELOVER  2013-07-18T17:07:06.152000   \n",
       "248          25            James Cook  2013-10-10T18:08:07.815000   \n",
       "249          11   Trulee IsNotAmazing  2013-09-07T14:18:22.601000   \n",
       "\n",
       "                                                  text  label  video  \n",
       "0                    Check out this video on YouTube:Ôªø      1      5  \n",
       "1                                         super musicÔªø      0      5  \n",
       "2    Subscribe my channel ¬†I RECORDING FIFA 15 GOAL...      1      5  \n",
       "3                               This song is so beauty      0      5  \n",
       "4    SEE SOME MORE SONG OPEN GOOGLE AND TYPE Shakir...      1      5  \n",
       "..                                                 ...    ...    ...  \n",
       "245                        Pleas subscribe my channelÔªø      1      5  \n",
       "246            The best FIFA world cup song for sure.Ôªø      0      5  \n",
       "247   hey you ! check out the channel of Alvar Lake !!      1      5  \n",
       "248  Hello Guys...I Found a Way to Make Money Onlin...      1      5  \n",
       "249             Beautiful song beautiful girl it works      0      5  \n",
       "\n",
       "[250 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('yt_comments_train.csv')\n",
    "df_test = pd.read_csv('yt_comments_test.csv')\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-telling",
   "metadata": {},
   "source": [
    "## Labeling functions\n",
    "\n",
    "Labeling functions (LFs) are heuristics that take a data point as input and assign a label to it or abstain (don‚Äôt assign any label).\n",
    "LFs are noisy (they might not have perfect accuracy) and don't have to label every data point.\n",
    "\n",
    "Some of the most common LFs rely on:\n",
    "\n",
    "- Keyword searches: looking for specific words in a sentence\n",
    "- Pattern matching: looking for specific syntactical patterns\n",
    "- Heuristics: slight improvements based on certain assumptions\n",
    "- Third-party models: using an pre-trained model (usually a model for a different task than the one at hand)\n",
    "- Distant supervision: using external knowledge base\n",
    "- Crowdworker labels: treating each crowdworker as a black-box function that assigns labels to subsets of the data\n",
    "\n",
    "More information can be found in the [Snorkel LFs tutorial](https://www.snorkel.org/use-cases/01-spam-tutorial#a-gentle-introduction-to-lfs)\n",
    "\n",
    "Let's see how we can use some of this methods to make LFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "early-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from snorkel.labeling import labeling_function, LabelingFunction\n",
    "from snorkel.labeling.lf.nlp import nlp_labeling_function\n",
    "from snorkel.preprocess import preprocessor\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "ABSTAIN = -1\n",
    "HAM = 0\n",
    "SPAM = 1\n",
    "\n",
    "# Keyword searches\n",
    "@labeling_function()\n",
    "def check(x):\n",
    "    return SPAM if \"check\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def check_out(x):\n",
    "    return SPAM if \"check out\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "# Heuristics\n",
    "@labeling_function()\n",
    "def short_comment(x):\n",
    "    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n",
    "    return HAM if len(x.text.split()) < 5 else ABSTAIN\n",
    "\n",
    "# List of keywords\n",
    "def keyword_lookup(x, keywords, label):\n",
    "    if any(word in x.text.lower() for word in keywords):\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "\n",
    "def make_keyword_lf(keywords, label=SPAM):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label),\n",
    "    )\n",
    "\n",
    "\"\"\"Spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "keyword_my = make_keyword_lf(keywords=[\"my\"])\n",
    "\n",
    "\"\"\"Spam comments ask users to subscribe to their channels.\"\"\"\n",
    "keyword_subscribe = make_keyword_lf(keywords=[\"subscribe\"])\n",
    "\n",
    "\"\"\"Spam comments post links to other channels.\"\"\"\n",
    "keyword_link = make_keyword_lf(keywords=[\"http\"])\n",
    "\n",
    "\"\"\"Spam comments make requests rather than commenting.\"\"\"\n",
    "keyword_please = make_keyword_lf(keywords=[\"please\", \"plz\"])\n",
    "\n",
    "\"\"\"Ham comments actually talk about the video's content.\"\"\"\n",
    "keyword_song = make_keyword_lf(keywords=[\"song\"], label=HAM)\n",
    "\n",
    "\n",
    "# Pattern matching with regex\n",
    "@labeling_function()\n",
    "def regex_check_out(x):\n",
    "    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN\n",
    "\n",
    "\n",
    "# Third party models (TextBlob and spaCy)\n",
    "# TextBlob\n",
    "@preprocessor(memoize=True)\n",
    "def textblob_sentiment(x):\n",
    "    scores = TextBlob(x.text)\n",
    "    x.polarity = scores.sentiment.polarity\n",
    "    x.subjectivity = scores.sentiment.subjectivity\n",
    "    return x\n",
    "\n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_subjectivity(x):\n",
    "    return HAM if x.subjectivity >= 0.5 else ABSTAIN\n",
    "\n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_polarity(x):\n",
    "    return HAM if x.polarity >= 0.9 else ABSTAIN\n",
    "\n",
    "# spaCy\n",
    "\n",
    "# There are two different methods to use spaCy:\n",
    "# Method 1:\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)\n",
    "\n",
    "@labeling_function(pre=[spacy])\n",
    "def has_person(x):\n",
    "    \"\"\"Ham comments mention specific people and are short.\"\"\"\n",
    "    if len(x.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "# Method 2:\n",
    "@nlp_labeling_function()\n",
    "def has_person_nlp(x):\n",
    "    \"\"\"Ham comments mention specific people.\"\"\"\n",
    "    if any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defined-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of labeling functions\n",
    "lfs = [\n",
    "    keyword_my,\n",
    "    keyword_subscribe,\n",
    "    keyword_link,\n",
    "    keyword_please,\n",
    "    keyword_song,\n",
    "    regex_check_out,\n",
    "    short_comment,\n",
    "    has_person_nlp,\n",
    "    textblob_polarity,\n",
    "    textblob_subjectivity,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-assist",
   "metadata": {},
   "source": [
    "We have mentioned multiple functions that could be used to label our data, but we never gave a solution on how to deal with the overlap and conflicts. In this section, we will deal with this problem.  \n",
    "A simple baseline for doing this is to take the majority vote on a per-data point basis: if more LFs voted SPAM than HAM, label it SPAM (and vice versa). We can test this with the [MajorityLabelVoter](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.model.baselines.MajorityLabelVoter.html#snorkel.labeling.model.baselines.MajorityLabelVoter) baseline model implemented in Snorkel.  \n",
    "The major inconvenience of this approach is that we defined rules that could be correlated, resulting in certain signals being overrepresented in a majority-vote-based model.  \n",
    "To handle this issue, we are going to make use of the LabelModel. You can read more about how it works in the [Snorkel tutorial](https://www.snorkel.org/use-cases/01-spam-tutorial#4-combining-labeling-function-outputs-with-the-label-model) and the [documentation](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.model.label_model.LabelModel.html#snorkel.labeling.model.label_model.LabelModel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "active-minutes",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javispp/miniconda3/envs/biome2/lib/python3.7/site-packages/tqdm/std.py:670: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:14<00:00, 111.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:02<00:00, 109.55it/s]\n",
      "/home/javispp/miniconda3/envs/biome2/lib/python3.7/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy:   81.2%\n",
      "Label Model Accuracy:     86.4%\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "# Apply LFs to datasets\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_test = applier.apply(df=df_test)\n",
    "\n",
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L=L_train) # y_train labels\n",
    "Y_test = df_test.label.values # y_test labels\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True) # cardinality = n¬∫ of classes\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)\n",
    "\n",
    "majority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-crime",
   "metadata": {},
   "source": [
    "# 2. Extending and finding labeling functions with Rubrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-psychology",
   "metadata": {},
   "source": [
    "## Setup Rubrix\n",
    "\n",
    "If you have not installed and launched Rubrix, check the [installation guide](https://github.com/recognai/rubrix#get-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "modified-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "\n",
    "rb.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-class",
   "metadata": {},
   "source": [
    "### Exploring the training set with Rubrix for initial inspiration\n",
    "\n",
    "Rubrix lets you log and track data for different NLP tasks (such as *Token Classification* or *Text Classification*). \n",
    "\n",
    "First of all, we have to create the dataset and load it into rubrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "three-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "records= []\n",
    "\n",
    "for index, record in df_train.iterrows():     \n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=index,\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        metadata = {\n",
    "            \"textlen\": str(len(record.text)), \n",
    "            \"author\": record.author,\n",
    "            \"video\": str(record.video)\n",
    "        }\n",
    "    )\n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "future-parcel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_spam_snorkel', processed=1586, failed=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_spam_snorkel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-highland",
   "metadata": {},
   "source": [
    "Once we have it into rubrix, we can explore the data and the metadata we just added by pressing the \"view metadata\" button on the bottom right corner of each sample. Take a look at the following picture:\n",
    "\n",
    "<img src=\"metadata_view.png\">\n",
    "\n",
    "Now, we can explore the metadata by going to the top left. Let's say we want to check the different authors or inspect all the data with a specific one. By going to the author section, we can easily do all of this.\n",
    "\n",
    "<img src=\"metadata_author.png\">\n",
    "\n",
    "After applying the changes we should only see the comments that belong to the selected author. We can also add multiple conditions besides the name of the author, but we will keep it simple this time.  \n",
    "Another option is to use the top right search box to find samples that contain a certain word or a phrase (\"put inside parantheses\"). We will search for \"check\", since it is more likely that those comments are SPAM. \n",
    "\n",
    "<img src=\"search_check.png\">\n",
    "\n",
    "As we can see, the LF used before to label the samples with the word check as SPAM makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-hamilton",
   "metadata": {},
   "source": [
    "An LF development cycle with Rubrix, could look like this:\n",
    "    \n",
    "1. Load dataset into Rubrix\n",
    "2. Explore data and write an initial version of an LF\n",
    "3. Spot check its performance by looking at its output on data points in the training set (or development set if available)\n",
    "4. Refine and debug to improve coverage or accuracy as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-finnish",
   "metadata": {},
   "source": [
    "### Exploring and improving heuristic LFs\n",
    "\n",
    "We have already seen how to use keywords to label our data, the next step would be to use heuristics to do the labeling. A simple approach to this could be setting a minimum length to the comment, considering it SPAM if its length is lower than a threshold.  \n",
    "To use the right threshold we are going to explore our data in Rubrix by using the metadata field, similar to what we did before with the author selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "center-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "records= []\n",
    "\n",
    "for index, record in df_train.iterrows():     \n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=index,\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        metadata = {\n",
    "            \"textlen\": str(len(record.text.split())), # N¬∫ of 'words' in the sample\n",
    "        }\n",
    "    )\n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "relevant-channels",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_spam_snorkel_heuristic', processed=1586, failed=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_spam_snorkel_heuristic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-stephen",
   "metadata": {},
   "source": [
    "For this example we will use a threshold of 20 'words'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "union-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for now the only LF we change\n",
    "@labeling_function()\n",
    "def short_comment_2(x):\n",
    "    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n",
    "    return HAM if len(x.text.split()) < 20 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-bookmark",
   "metadata": {},
   "source": [
    "### Writing Keyword LFs with Rubrix\n",
    "\n",
    "Now, we will use Rubrix to find improved LFs using lists of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "scientific-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "records= []\n",
    "\n",
    "for index, record in df_train.iterrows():     \n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=index,\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "    )\n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "attached-foundation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_spam_snorkel_lfs', processed=1586, failed=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_spam_snorkel_lfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intended-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Ham comments usually ask how something happened/is because they are interested.\"\"\"\n",
    "keyword_how = make_keyword_lf(keywords=[\"how\"], label=HAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-substance",
   "metadata": {},
   "source": [
    "### Exploring third-party models LFs with Rubrix\n",
    "\n",
    "\n",
    "#### Textblob\n",
    "\n",
    "Let's explore Textblob predictions on the training set with Rubrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "binding-windows",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: I would check this part to work with confidence intervals once we can set the multilabel attribute\n",
    "# to True. Otherwise, we have to load the same dataset for each label\n",
    "from textblob import TextBlob\n",
    "\n",
    "records= []\n",
    "for index, record in df_train.iterrows():   \n",
    "    scores = TextBlob(record[\"text\"])\n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=str(index),\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        multi_label= False,\n",
    "        prediction=[(\"subjectivity\", max(0.0, scores.sentiment.subjectivity))],\n",
    "        #prediction=[(\"polarity\", max(0.0, scores.sentiment.polarity))],\n",
    "        prediction_agent=\"TextBlob\",\n",
    "        metadata = {\n",
    "            \"textlen\": str(len(record.text)), \n",
    "            \"author\": record.author,\n",
    "            \"video\": str(record.video)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "plain-infrastructure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_spam_snorkel_textblob', processed=1586, failed=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_spam_snorkel_textblob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-accident",
   "metadata": {},
   "source": [
    "Checking the dataset, we can filter our data based on the confidence of our classifier. This can help us since the predictions of our TextBlob tend to be SPAM the lower the subjectivity is. We can take advantage of this by filtering the predictions using intervals of confidence. For this example we are going to set a threshold of 0.56 for the subjectivity.  \n",
    "\n",
    "<img src=\"confidence_interval.png\">\n",
    "\n",
    "The same way we did it for subjectivity, we can do it for polarity. In this case we will set the threshold to 0.9.\n",
    "This comes in handy when we want to add restrictions on top of another. This time we won't go much deeper, but we could also use metadata or create more than what we have, in combination with the confidence, to have a better understanding of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-istanbul",
   "metadata": {},
   "source": [
    "Once we have our conclusions, we can proceed with creating the labeling functions using the thresholds for the confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amazing-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def textblob_sentiment(x):\n",
    "    scores = TextBlob(x.text)\n",
    "    x.polarity = scores.sentiment.polarity\n",
    "    x.subjectivity = scores.sentiment.subjectivity\n",
    "    return x\n",
    "\n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_subjectivity(x):\n",
    "    return HAM if x.subjectivity >= 0.56 else ABSTAIN\n",
    "\n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_polarity(x):\n",
    "    return HAM if x.polarity >= 0.9 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-wright",
   "metadata": {},
   "source": [
    "#### spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-camel",
   "metadata": {},
   "source": [
    "For this example, we will use spaCy for a token classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faced-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-examination",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for index, record in df_train[40:].iterrows():\n",
    "    doc = nlp(record[\"text\"])\n",
    "    item = rb.TokenClassificationRecord(\n",
    "        text=record[\"text\"],\n",
    "        tokens=[t.text for t in doc],\n",
    "        prediction=[(e.label_, e.start_char, e.end_char) for e in doc.ents],\n",
    "        prediction_agent=\"spacy\"\n",
    "    )\n",
    "    print(item)\n",
    "    rb.log(item, name=\"yt_spam_snorkel_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-frank",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rb.log(records, name=\"yt_spam_snorkel_spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-stone",
   "metadata": {},
   "source": [
    "It's time to explore Rubrix and see if there are some entities that are more common in the SPAM or HAM comments.\n",
    "\n",
    "<img src=\"spacy_ner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-combine",
   "metadata": {},
   "source": [
    "Exploring a bit the data, we will create a rule to predict as HAM if the sample has a PERSON entity in its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "south-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nlp_labeling_function()\n",
    "def has_person_nlp(x):\n",
    "    \"\"\"Ham comments mention specific people and are short.\"\"\"\n",
    "    if any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-vulnerability",
   "metadata": {},
   "source": [
    "## 3. Using the Snorkel Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-subcommittee",
   "metadata": {},
   "source": [
    "Let's include the new LFs we created using Rubrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "neutral-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [\n",
    "    keyword_my,\n",
    "    keyword_subscribe,\n",
    "    keyword_link,\n",
    "    keyword_please,\n",
    "    keyword_song,\n",
    "    keyword_how,\n",
    "    regex_check_out,\n",
    "    short_comment,\n",
    "    short_comment_2,\n",
    "    has_person_nlp,\n",
    "    textblob_polarity,\n",
    "    textblob_subjectivity,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-mouse",
   "metadata": {},
   "source": [
    "And apply it to our training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_test = applier.apply(df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "useful-protection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>keyword_my</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.198613</td>\n",
       "      <td>0.197352</td>\n",
       "      <td>0.174653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_subscribe</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.127364</td>\n",
       "      <td>0.122951</td>\n",
       "      <td>0.106557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_http</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.119168</td>\n",
       "      <td>0.116646</td>\n",
       "      <td>0.110971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_please</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.112232</td>\n",
       "      <td>0.111602</td>\n",
       "      <td>0.095208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_song</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.141866</td>\n",
       "      <td>0.135561</td>\n",
       "      <td>0.043506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_how</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.036570</td>\n",
       "      <td>0.033417</td>\n",
       "      <td>0.012610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.233922</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.213745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>7</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.225725</td>\n",
       "      <td>0.225725</td>\n",
       "      <td>0.074401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment_2</th>\n",
       "      <td>8</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.810845</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.372636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_person_nlp</th>\n",
       "      <td>9</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.142497</td>\n",
       "      <td>0.138083</td>\n",
       "      <td>0.075662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_polarity</th>\n",
       "      <td>10</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.035939</td>\n",
       "      <td>0.035939</td>\n",
       "      <td>0.005675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <td>11</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.302648</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.135561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        j Polarity  Coverage  Overlaps  Conflicts\n",
       "keyword_my              0      [1]  0.198613  0.197352   0.174653\n",
       "keyword_subscribe       1      [1]  0.127364  0.122951   0.106557\n",
       "keyword_http            2      [1]  0.119168  0.116646   0.110971\n",
       "keyword_please          3      [1]  0.112232  0.111602   0.095208\n",
       "keyword_song            4      [0]  0.141866  0.135561   0.043506\n",
       "keyword_how             5      [0]  0.036570  0.033417   0.012610\n",
       "regex_check_out         6      [1]  0.233922  0.229508   0.213745\n",
       "short_comment           7      [0]  0.225725  0.225725   0.074401\n",
       "short_comment_2         8      [0]  0.810845  0.688525   0.372636\n",
       "has_person_nlp          9      [0]  0.142497  0.138083   0.075662\n",
       "textblob_polarity      10      [0]  0.035939  0.035939   0.005675\n",
       "textblob_subjectivity  11      [0]  0.302648  0.295082   0.135561"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fitted-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L=L_train) # y_train labels\n",
    "Y_test = df_test.label.values # y_test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "experimental-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True) # cardinality = n¬∫ of classes\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "standard-express",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy:   75.6%\n",
      "Label Model Accuracy:     89.6%\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True) # cardinality = n¬∫ of classes\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)\n",
    "\n",
    "majority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-opportunity",
   "metadata": {},
   "source": [
    "As we can see, the LabelModel outperforms the basic MajorityLabelVoter configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-buddy",
   "metadata": {},
   "source": [
    "#### Filtering unlabeled data\n",
    "\n",
    "The method we saw above, has a small inconvenience, our data can receive no prediction (predicted as ABSTAIN) from any of our LFs. Those cases with no label (labeled as ABSTAIN) will be removed from the training dataset using a [built-in utility](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.filter_unlabeled_dataframe.html#snorkel.labeling.filter_unlabeled_dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "banner-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_train,\n",
    "    y=label_model.predict_proba(L_train), # Probabilities of each data point for each class\n",
    "    L=L_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-longitude",
   "metadata": {},
   "source": [
    "Now that we have our data, we can explore the results in Rubrix and manually relabel those cases that have been wrongly classified or keep exploring the performance of our LFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-organic",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "i=0\n",
    "for index, record in df_train_filtered.iterrows():   \n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=str(index),\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        multilabel=True,\n",
    "        # our confidences/scores come from probs_train_filtered\n",
    "        # probs_train_filtered[i][j] is the probability the sample i belongs to class j\n",
    "        prediction=[(\"HAM\", probs_train_filtered[i][0]),   # 0 for HAM\n",
    "                    (\"SPAM\", probs_train_filtered[i][1])], # 1 for SPAM\n",
    "        prediction_agent=\"LabelModel\",\n",
    "        metadata = {\n",
    "            \"textlen\": str(len(record.text)), \n",
    "            \"author\": record.author,\n",
    "            \"video\": str(record.video)\n",
    "        }\n",
    "    )\n",
    "    records.append(item)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.log(records=records, name=\"yt_filtered_classified_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-sleeve",
   "metadata": {},
   "source": [
    "To relabel the data we switch into the annotation mode by pressing the bottom right button in the rubrix UI. \n",
    "After that, we can simply click on the label for each sample and it will be marked as a \"validated sample\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-opposition",
   "metadata": {},
   "source": [
    "## 4. Train a classifier and explore its predictions\n",
    "\n",
    "The last thing we can do with our data is training a classifier using some of the most popular libraries such as Scikit-learn, Tensorflow or Pytorch. For simplicity, we will use Scikit-learn, a well known library in ML.  \n",
    "We recommend checking [biome.txt](https://www.recogn.ai/biome-text/), a simple and powerful tool for NLP problems, developed by the same team behind Rubrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "brazilian-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 5)) # Bag Of Words (BoW) with n-grams\n",
    "X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\n",
    "X_test = vectorizer.transform(df_test.text.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-divorce",
   "metadata": {},
   "source": [
    "Since we need to tell the model the class for each sample, and we have probabilities, we can assign to each sample the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "governing-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-memphis",
   "metadata": {},
   "source": [
    "And then build the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "magnetic-reduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000.0, solver='liblinear')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sklearn_model = LogisticRegression(C=1e3, solver=\"liblinear\")\n",
    "sklearn_model.fit(X=X_train, y=preds_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "maritime-phrase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.2%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {sklearn_model.score(X=X_test, y=Y_test) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-conflict",
   "metadata": {},
   "source": [
    "Let's see how our new model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for index, record in df_train_filtered.iterrows(): \n",
    "    preds=sklearn_model.predict_proba(vectorizer.transform([record[\"text\"]]))\n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=str(index),\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        multilabel=True,\n",
    "        prediction=[(\"HAM\", preds[0]),   # 0 for HAM\n",
    "                    (\"SPAM\", preds[1])], # 1 for SPAM\n",
    "        prediction_agent=\"MyModel\",\n",
    "    )\n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.log(records=records, name=\"yt_my_model_performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-offering",
   "metadata": {},
   "source": [
    "Last but not least, we will show a simple model for text classification using biome.text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U biome-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-shame",
   "metadata": {},
   "source": [
    "But first, let's use the original name for the labels instead of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "difficult-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the label column to the class the sample belongs (HAM or SPAM)\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(lambda x: \"SPAM\" if x==1 else \"HAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the label column to the class the sample belongs (HAM or SPAM) according to our LFs\n",
    "df_train_filtered[\"label\"] = list(map(lambda x: \"SPAM\" if x==1 else \"HAM\", preds_train_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "controversial-innocent",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea5bf0ef9af4e318d770a7319c99c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loading instances', max=1567.0, style=ProgressStyle(descr‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0ba3dd80b947598563db9ec79b5c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loading instances', max=250.0, style=ProgressStyle(descri‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.data.vocabulary: Fitting token dictionary from dataset.\n",
      "2021-05-10 10:02:54,718 - allennlp.data.vocabulary - INFO - Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f113c44682a4bc9a2827bd1c834437d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='building vocab', layout=Layout(width='2‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.modules.token_embedders.embedding: Reading pretrained embeddings from file\n",
      "2021-05-10 10:02:54,925 - allennlp.modules.token_embedders.embedding - INFO - Reading pretrained embeddings from file\n",
      "INFO:allennlp.common.file_utils: cache of https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip is up-to-date\n",
      "2021-05-10 10:02:55,783 - allennlp.common.file_utils - INFO - cache of https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip is up-to-date\n",
      "INFO:allennlp.common.file_utils: cache of https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip is up-to-date\n",
      "2021-05-10 10:02:56,730 - allennlp.common.file_utils - INFO - cache of https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip is up-to-date\n",
      "INFO:allennlp.modules.token_embedders.embedding: Recognized a header line in the embedding file with number of tokens: 999994\n",
      "2021-05-10 10:02:56,744 - allennlp.modules.token_embedders.embedding - INFO - Recognized a header line in the embedding file with number of tokens: 999994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3058da5455744632b35d9b17a4f7670f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999994.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.modules.token_embedders.embedding: Initializing pre-trained embedding layer\n",
      "2021-05-10 10:03:13,256 - allennlp.modules.token_embedders.embedding - INFO - Initializing pre-trained embedding layer\n",
      "INFO:allennlp.modules.token_embedders.embedding: Pretrained embeddings were found for 2746 out of 4295 tokens\n",
      "2021-05-10 10:03:13,306 - allennlp.modules.token_embedders.embedding - INFO - Pretrained embeddings were found for 2746 out of 4295 tokens\n",
      "INFO:allennlp.modules.token_embedders.embedding: If you are fine-tuning and want to use a pretrained_file for embedding extension, please pass the mapping by --embedding-sources argument.\n",
      "2021-05-10 10:03:13,317 - allennlp.modules.token_embedders.embedding - INFO - If you are fine-tuning and want to use a pretrained_file for embedding extension, please pass the mapping by --embedding-sources argument.\n",
      "INFO:allennlp.common.params: random_seed = 13370\n",
      "2021-05-10 10:03:13,322 - allennlp.common.params - INFO - random_seed = 13370\n",
      "INFO:allennlp.common.params: numpy_seed = 1337\n",
      "2021-05-10 10:03:13,324 - allennlp.common.params - INFO - numpy_seed = 1337\n",
      "INFO:allennlp.common.params: pytorch_seed = 133\n",
      "2021-05-10 10:03:13,327 - allennlp.common.params - INFO - pytorch_seed = 133\n",
      "INFO:allennlp.common.checks: Pytorch version: 1.7.1\n",
      "2021-05-10 10:03:13,366 - allennlp.common.checks - INFO - Pytorch version: 1.7.1\n",
      "INFO:allennlp.common.params: type = gradient_descent\n",
      "2021-05-10 10:03:13,398 - allennlp.common.params - INFO - type = gradient_descent\n",
      "INFO:allennlp.common.params: local_rank = 0\n",
      "2021-05-10 10:03:13,401 - allennlp.common.params - INFO - local_rank = 0\n",
      "INFO:allennlp.common.params: patience = 2\n",
      "2021-05-10 10:03:13,403 - allennlp.common.params - INFO - patience = 2\n",
      "INFO:allennlp.common.params: validation_metric = -loss\n",
      "2021-05-10 10:03:13,408 - allennlp.common.params - INFO - validation_metric = -loss\n",
      "INFO:allennlp.common.params: num_epochs = 20\n",
      "2021-05-10 10:03:13,410 - allennlp.common.params - INFO - num_epochs = 20\n",
      "INFO:allennlp.common.params: cuda_device = None\n",
      "2021-05-10 10:03:13,412 - allennlp.common.params - INFO - cuda_device = None\n",
      "INFO:allennlp.common.params: grad_norm = None\n",
      "2021-05-10 10:03:13,414 - allennlp.common.params - INFO - grad_norm = None\n",
      "INFO:allennlp.common.params: grad_clipping = None\n",
      "2021-05-10 10:03:13,415 - allennlp.common.params - INFO - grad_clipping = None\n",
      "INFO:allennlp.common.params: distributed = False\n",
      "2021-05-10 10:03:13,417 - allennlp.common.params - INFO - distributed = False\n",
      "INFO:allennlp.common.params: world_size = 1\n",
      "2021-05-10 10:03:13,419 - allennlp.common.params - INFO - world_size = 1\n",
      "INFO:allennlp.common.params: num_gradient_accumulation_steps = 1\n",
      "2021-05-10 10:03:13,421 - allennlp.common.params - INFO - num_gradient_accumulation_steps = 1\n",
      "INFO:allennlp.common.params: use_amp = False\n",
      "2021-05-10 10:03:13,422 - allennlp.common.params - INFO - use_amp = False\n",
      "INFO:allennlp.common.params: no_grad = None\n",
      "2021-05-10 10:03:13,425 - allennlp.common.params - INFO - no_grad = None\n",
      "INFO:allennlp.common.params: learning_rate_scheduler = None\n",
      "2021-05-10 10:03:13,431 - allennlp.common.params - INFO - learning_rate_scheduler = None\n",
      "INFO:allennlp.common.params: momentum_scheduler = None\n",
      "2021-05-10 10:03:13,433 - allennlp.common.params - INFO - momentum_scheduler = None\n",
      "INFO:allennlp.common.params: moving_average = None\n",
      "2021-05-10 10:03:13,437 - allennlp.common.params - INFO - moving_average = None\n",
      "INFO:allennlp.common.params: batch_callbacks = None\n",
      "2021-05-10 10:03:13,439 - allennlp.common.params - INFO - batch_callbacks = None\n",
      "INFO:allennlp.common.params: end_callbacks = None\n",
      "2021-05-10 10:03:13,441 - allennlp.common.params - INFO - end_callbacks = None\n",
      "INFO:allennlp.common.params: trainer_callbacks = None\n",
      "2021-05-10 10:03:13,443 - allennlp.common.params - INFO - trainer_callbacks = None\n",
      "INFO:allennlp.common.params: optimizer.type = adam\n",
      "2021-05-10 10:03:13,445 - allennlp.common.params - INFO - optimizer.type = adam\n",
      "INFO:allennlp.common.params: optimizer.parameter_groups = None\n",
      "2021-05-10 10:03:13,447 - allennlp.common.params - INFO - optimizer.parameter_groups = None\n",
      "INFO:allennlp.common.params: optimizer.lr = 0.001\n",
      "2021-05-10 10:03:13,450 - allennlp.common.params - INFO - optimizer.lr = 0.001\n",
      "INFO:allennlp.common.params: optimizer.betas = (0.9, 0.999)\n",
      "2021-05-10 10:03:13,452 - allennlp.common.params - INFO - optimizer.betas = (0.9, 0.999)\n",
      "INFO:allennlp.common.params: optimizer.eps = 1e-08\n",
      "2021-05-10 10:03:13,464 - allennlp.common.params - INFO - optimizer.eps = 1e-08\n",
      "INFO:allennlp.common.params: optimizer.weight_decay = 0.0\n",
      "2021-05-10 10:03:13,466 - allennlp.common.params - INFO - optimizer.weight_decay = 0.0\n",
      "INFO:allennlp.common.params: optimizer.amsgrad = False\n",
      "2021-05-10 10:03:13,469 - allennlp.common.params - INFO - optimizer.amsgrad = False\n",
      "INFO:allennlp.training.optimizers: Number of trainable parameters: 1696758\n",
      "2021-05-10 10:03:13,471 - allennlp.training.optimizers - INFO - Number of trainable parameters: 1696758\n",
      "INFO:allennlp.common.util: The following parameters are Frozen (without gradient):\n",
      "2021-05-10 10:03:13,473 - allennlp.common.util - INFO - The following parameters are Frozen (without gradient):\n",
      "INFO:allennlp.common.util: The following parameters are Tunable (with gradient):\n",
      "2021-05-10 10:03:13,476 - allennlp.common.util - INFO - The following parameters are Tunable (with gradient):\n",
      "INFO:allennlp.common.util: _head.backbone.embedder.token_embedder_word.weight\n",
      "2021-05-10 10:03:13,478 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_word.weight\n",
      "INFO:allennlp.common.util: _head.backbone.embedder.token_embedder_char._embedding._module.weight\n",
      "2021-05-10 10:03:13,480 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_char._embedding._module.weight\n",
      "INFO:allennlp.common.util: _head.backbone.embedder.token_embedder_char._encoder._module._module.weight_ih_l0\n",
      "2021-05-10 10:03:13,482 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_char._encoder._module._module.weight_ih_l0\n",
      "INFO:allennlp.common.util: _head.backbone.embedder.token_embedder_char._encoder._module._module.weight_hh_l0\n",
      "2021-05-10 10:03:13,489 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_char._encoder._module._module.weight_hh_l0\n",
      "INFO:allennlp.common.util: _head.backbone.embedder.token_embedder_char._encoder._module._module.bias_ih_l0\n",
      "2021-05-10 10:03:13,492 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_char._encoder._module._module.bias_ih_l0\n",
      "INFO:allennlp.common.util: _head.backbone.embedder.token_embedder_char._encoder._module._module.bias_hh_l0\n",
      "2021-05-10 10:03:13,494 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_char._encoder._module._module.bias_hh_l0\n",
      "INFO:allennlp.common.util: _head.backbone.embedder.token_embedder_char._encoder._module._module.weight_ih_l0_reverse\n",
      "2021-05-10 10:03:13,496 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_char._encoder._module._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.common.util: _head.backbone.embedder.token_embedder_char._encoder._module._module.weight_hh_l0_reverse\n",
      "2021-05-10 10:03:13,499 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_char._encoder._module._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.common.util: _head.backbone.embedder.token_embedder_char._encoder._module._module.bias_ih_l0_reverse\n",
      "2021-05-10 10:03:13,501 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_char._encoder._module._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.common.util: _head.backbone.embedder.token_embedder_char._encoder._module._module.bias_hh_l0_reverse\n",
      "2021-05-10 10:03:13,502 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_char._encoder._module._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.common.util: _head.pooler._module.weight_ih_l0\n",
      "2021-05-10 10:03:13,505 - allennlp.common.util - INFO - _head.pooler._module.weight_ih_l0\n",
      "INFO:allennlp.common.util: _head.pooler._module.weight_hh_l0\n",
      "2021-05-10 10:03:13,507 - allennlp.common.util - INFO - _head.pooler._module.weight_hh_l0\n",
      "INFO:allennlp.common.util: _head.pooler._module.bias_ih_l0\n",
      "2021-05-10 10:03:13,508 - allennlp.common.util - INFO - _head.pooler._module.bias_ih_l0\n",
      "INFO:allennlp.common.util: _head.pooler._module.bias_hh_l0\n",
      "2021-05-10 10:03:13,510 - allennlp.common.util - INFO - _head.pooler._module.bias_hh_l0\n",
      "INFO:allennlp.common.util: _head.pooler._module.weight_ih_l0_reverse\n",
      "2021-05-10 10:03:13,512 - allennlp.common.util - INFO - _head.pooler._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.common.util: _head.pooler._module.weight_hh_l0_reverse\n",
      "2021-05-10 10:03:13,514 - allennlp.common.util - INFO - _head.pooler._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.common.util: _head.pooler._module.bias_ih_l0_reverse\n",
      "2021-05-10 10:03:13,518 - allennlp.common.util - INFO - _head.pooler._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.common.util: _head.pooler._module.bias_hh_l0_reverse\n",
      "2021-05-10 10:03:13,520 - allennlp.common.util - INFO - _head.pooler._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.common.util: _head.feedforward._linear_layers.0.weight\n",
      "2021-05-10 10:03:13,522 - allennlp.common.util - INFO - _head.feedforward._linear_layers.0.weight\n",
      "INFO:allennlp.common.util: _head.feedforward._linear_layers.0.bias\n",
      "2021-05-10 10:03:13,524 - allennlp.common.util - INFO - _head.feedforward._linear_layers.0.bias\n",
      "INFO:allennlp.common.util: _head._classification_layer.weight\n",
      "2021-05-10 10:03:13,526 - allennlp.common.util - INFO - _head._classification_layer.weight\n",
      "INFO:allennlp.common.util: _head._classification_layer.bias\n",
      "2021-05-10 10:03:13,531 - allennlp.common.util - INFO - _head._classification_layer.bias\n",
      "INFO:allennlp.common.params: checkpointer.type = default\n",
      "2021-05-10 10:03:13,533 - allennlp.common.params - INFO - checkpointer.type = default\n",
      "INFO:allennlp.common.params: checkpointer.keep_serialized_model_every_num_seconds = None\n",
      "2021-05-10 10:03:13,536 - allennlp.common.params - INFO - checkpointer.keep_serialized_model_every_num_seconds = None\n",
      "INFO:allennlp.common.params: checkpointer.num_serialized_models_to_keep = 1\n",
      "2021-05-10 10:03:13,538 - allennlp.common.params - INFO - checkpointer.num_serialized_models_to_keep = 1\n",
      "INFO:allennlp.common.params: checkpointer.model_save_interval = None\n",
      "2021-05-10 10:03:13,540 - allennlp.common.params - INFO - checkpointer.model_save_interval = None\n",
      "INFO:allennlp.common.params: tensorboard_writer.summary_interval = 100\n",
      "2021-05-10 10:03:13,543 - allennlp.common.params - INFO - tensorboard_writer.summary_interval = 100\n",
      "INFO:allennlp.common.params: tensorboard_writer.histogram_interval = None\n",
      "2021-05-10 10:03:13,544 - allennlp.common.params - INFO - tensorboard_writer.histogram_interval = None\n",
      "INFO:allennlp.common.params: tensorboard_writer.batch_size_interval = None\n",
      "2021-05-10 10:03:13,546 - allennlp.common.params - INFO - tensorboard_writer.batch_size_interval = None\n",
      "INFO:allennlp.common.params: tensorboard_writer.should_log_parameter_statistics = True\n",
      "2021-05-10 10:03:13,548 - allennlp.common.params - INFO - tensorboard_writer.should_log_parameter_statistics = True\n",
      "INFO:allennlp.common.params: tensorboard_writer.should_log_learning_rate = True\n",
      "2021-05-10 10:03:13,551 - allennlp.common.params - INFO - tensorboard_writer.should_log_learning_rate = True\n",
      "INFO:allennlp.common.params: tensorboard_writer.get_batch_num_total = None\n",
      "2021-05-10 10:03:13,553 - allennlp.common.params - INFO - tensorboard_writer.get_batch_num_total = None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjavispp\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.21<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ethereal-lion-155</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/javispp/biome\" target=\"_blank\">https://wandb.ai/javispp/biome</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/javispp/biome/runs/36ol5p92\" target=\"_blank\">https://wandb.ai/javispp/biome/runs/36ol5p92</a><br/>\n",
       "                Run data is saved locally in <code>.wandb/.wandb/run-20210510_100314-36ol5p92</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer: Beginning training.\n",
      "2021-05-10 10:03:16,986 - allennlp.training.trainer - INFO - Beginning training.\n",
      "INFO:allennlp.training.trainer: Epoch 0/19\n",
      "2021-05-10 10:03:16,991 - allennlp.training.trainer - INFO - Epoch 0/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.3G\n",
      "2021-05-10 10:03:16,995 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.3G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:03:17,000 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa94c1cd40b4e54b4641abeca90e987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:allennlp.training.util: Metrics with names beginning with \"_\" will not be logged to the tqdm progress bar.\n",
      "2021-05-10 10:03:17,474 - allennlp.training.util - WARNING - Metrics with names beginning with \"_\" will not be logged to the tqdm progress bar.\n",
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:03:46,997 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     0.738  |       N/A\n",
      "2021-05-10 10:03:47,001 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     0.738  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     0.800  |       N/A\n",
      "2021-05-10 10:03:47,004 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     0.800  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     0.802  |       N/A\n",
      "2021-05-10 10:03:47,007 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     0.802  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     0.754  |       N/A\n",
      "2021-05-10 10:03:47,013 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     0.754  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     0.683  |       N/A\n",
      "2021-05-10 10:03:47,016 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     0.683  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     0.851  |       N/A\n",
      "2021-05-10 10:03:47,019 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     0.851  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     0.773  |       N/A\n",
      "2021-05-10 10:03:47,027 - allennlp.training.tensorboard_writer - INFO - accuracy           |     0.773  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.459  |       N/A\n",
      "2021-05-10 10:03:47,036 - allennlp.training.tensorboard_writer - INFO - loss               |     0.459  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     0.769  |       N/A\n",
      "2021-05-10 10:03:47,040 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     0.769  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     0.778  |       N/A\n",
      "2021-05-10 10:03:47,043 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     0.778  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     0.767  |       N/A\n",
      "2021-05-10 10:03:47,047 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     0.767  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     0.773  |       N/A\n",
      "2021-05-10 10:03:47,050 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     0.773  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     0.773  |       N/A\n",
      "2021-05-10 10:03:47,054 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     0.773  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     0.773  |       N/A\n",
      "2021-05-10 10:03:47,059 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     0.773  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1312.680  |       N/A\n",
      "2021-05-10 10:03:47,065 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1312.680  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:03:47,091 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:30.131392\n",
      "2021-05-10 10:03:47,122 - allennlp.training.trainer - INFO - Epoch duration: 0:00:30.131392\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:09:32\n",
      "2021-05-10 10:03:47,126 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:09:32\n",
      "INFO:allennlp.training.trainer: Epoch 1/19\n",
      "2021-05-10 10:03:47,129 - allennlp.training.trainer - INFO - Epoch 1/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.7G\n",
      "2021-05-10 10:03:47,132 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.7G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:03:47,135 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2268e8a14f640dabb1c76a2825bf5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:04:17,192 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     0.927  |       N/A\n",
      "2021-05-10 10:04:17,194 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     0.927  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     0.934  |       N/A\n",
      "2021-05-10 10:04:17,198 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     0.934  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     0.915  |       N/A\n",
      "2021-05-10 10:04:17,202 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     0.915  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     0.946  |       N/A\n",
      "2021-05-10 10:04:17,205 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     0.946  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     0.940  |       N/A\n",
      "2021-05-10 10:04:17,209 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     0.940  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     0.923  |       N/A\n",
      "2021-05-10 10:04:17,212 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     0.923  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     0.931  |       N/A\n",
      "2021-05-10 10:04:17,218 - allennlp.training.tensorboard_writer - INFO - accuracy           |     0.931  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.191  |       N/A\n",
      "2021-05-10 10:04:17,224 - allennlp.training.tensorboard_writer - INFO - loss               |     0.191  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     0.931  |       N/A\n",
      "2021-05-10 10:04:17,229 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     0.931  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     0.930  |       N/A\n",
      "2021-05-10 10:04:17,232 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     0.930  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     0.932  |       N/A\n",
      "2021-05-10 10:04:17,236 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     0.932  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     0.931  |       N/A\n",
      "2021-05-10 10:04:17,243 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     0.931  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     0.931  |       N/A\n",
      "2021-05-10 10:04:17,248 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     0.931  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     0.931  |       N/A\n",
      "2021-05-10 10:04:17,251 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     0.931  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1786.684  |       N/A\n",
      "2021-05-10 10:04:17,257 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1786.684  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:04:17,289 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:30.233240\n",
      "2021-05-10 10:04:17,363 - allennlp.training.trainer - INFO - Epoch duration: 0:00:30.233240\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:09:03\n",
      "2021-05-10 10:04:17,368 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:09:03\n",
      "INFO:allennlp.training.trainer: Epoch 2/19\n",
      "2021-05-10 10:04:17,383 - allennlp.training.trainer - INFO - Epoch 2/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:04:17,390 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:04:17,396 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5161520f8564e959e356065528c4e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:04:47,900 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     0.973  |       N/A\n",
      "2021-05-10 10:04:47,903 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     0.973  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     0.976  |       N/A\n",
      "2021-05-10 10:04:47,907 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     0.976  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     0.968  |       N/A\n",
      "2021-05-10 10:04:47,911 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     0.968  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     0.981  |       N/A\n",
      "2021-05-10 10:04:47,915 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     0.981  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     0.978  |       N/A\n",
      "2021-05-10 10:04:47,918 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     0.978  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     0.971  |       N/A\n",
      "2021-05-10 10:04:47,922 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     0.971  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     0.974  |       N/A\n",
      "2021-05-10 10:04:47,926 - allennlp.training.tensorboard_writer - INFO - accuracy           |     0.974  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.078  |       N/A\n",
      "2021-05-10 10:04:47,932 - allennlp.training.tensorboard_writer - INFO - loss               |     0.078  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     0.974  |       N/A\n",
      "2021-05-10 10:04:47,935 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     0.974  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     0.974  |       N/A\n",
      "2021-05-10 10:04:47,940 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     0.974  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     0.975  |       N/A\n",
      "2021-05-10 10:04:47,944 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     0.975  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     0.974  |       N/A\n",
      "2021-05-10 10:04:47,948 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     0.974  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     0.974  |       N/A\n",
      "2021-05-10 10:04:47,951 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     0.974  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     0.974  |       N/A\n",
      "2021-05-10 10:04:47,955 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     0.974  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:04:47,959 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:04:47,979 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:30.618229\n",
      "2021-05-10 10:04:48,002 - allennlp.training.trainer - INFO - Epoch duration: 0:00:30.618229\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:08:35\n",
      "2021-05-10 10:04:48,004 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:08:35\n",
      "INFO:allennlp.training.trainer: Epoch 3/19\n",
      "2021-05-10 10:04:48,007 - allennlp.training.trainer - INFO - Epoch 3/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:04:48,010 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:04:48,016 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b1187710d2422fb6788377fd80a95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:05:17,228 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     0.991  |       N/A\n",
      "2021-05-10 10:05:17,230 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     0.991  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     0.992  |       N/A\n",
      "2021-05-10 10:05:17,234 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     0.992  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     0.992  |       N/A\n",
      "2021-05-10 10:05:17,238 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     0.992  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     0.992  |       N/A\n",
      "2021-05-10 10:05:17,241 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     0.992  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     0.990  |       N/A\n",
      "2021-05-10 10:05:17,245 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     0.990  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     0.993  |       N/A\n",
      "2021-05-10 10:05:17,248 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     0.993  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     0.992  |       N/A\n",
      "2021-05-10 10:05:17,251 - allennlp.training.tensorboard_writer - INFO - accuracy           |     0.992  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.031  |       N/A\n",
      "2021-05-10 10:05:17,255 - allennlp.training.tensorboard_writer - INFO - loss               |     0.031  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     0.992  |       N/A\n",
      "2021-05-10 10:05:17,258 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     0.992  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     0.992  |       N/A\n",
      "2021-05-10 10:05:17,264 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     0.992  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     0.992  |       N/A\n",
      "2021-05-10 10:05:17,270 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     0.992  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     0.992  |       N/A\n",
      "2021-05-10 10:05:17,274 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     0.992  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     0.992  |       N/A\n",
      "2021-05-10 10:05:17,279 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     0.992  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     0.992  |       N/A\n",
      "2021-05-10 10:05:17,282 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     0.992  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:05:17,286 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:05:17,312 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:29.339425\n",
      "2021-05-10 10:05:17,346 - allennlp.training.trainer - INFO - Epoch duration: 0:00:29.339425\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:08:01\n",
      "2021-05-10 10:05:17,350 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:08:01\n",
      "INFO:allennlp.training.trainer: Epoch 4/19\n",
      "2021-05-10 10:05:17,354 - allennlp.training.trainer - INFO - Epoch 4/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:05:17,357 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:05:17,360 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180d4d0ac2cb44ffbb9e953a50f663ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:05:46,294 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     0.999  |       N/A\n",
      "2021-05-10 10:05:46,296 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     0.999  |       N/A\n",
      "2021-05-10 10:05:46,300 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     0.999  |       N/A\n",
      "2021-05-10 10:05:46,303 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:05:46,307 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:05:46,311 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     0.999  |       N/A\n",
      "2021-05-10 10:05:46,315 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     0.999  |       N/A\n",
      "2021-05-10 10:05:46,319 - allennlp.training.tensorboard_writer - INFO - accuracy           |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.007  |       N/A\n",
      "2021-05-10 10:05:46,327 - allennlp.training.tensorboard_writer - INFO - loss               |     0.007  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     0.999  |       N/A\n",
      "2021-05-10 10:05:46,332 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     0.999  |       N/A\n",
      "2021-05-10 10:05:46,335 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     0.999  |       N/A\n",
      "2021-05-10 10:05:46,339 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     0.999  |       N/A\n",
      "2021-05-10 10:05:46,344 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     0.999  |       N/A\n",
      "2021-05-10 10:05:46,348 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     0.999  |       N/A\n",
      "2021-05-10 10:05:46,353 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:05:46,359 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:05:46,385 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:29.062707\n",
      "2021-05-10 10:05:46,416 - allennlp.training.trainer - INFO - Epoch duration: 0:00:29.062707\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:07:28\n",
      "2021-05-10 10:05:46,419 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:07:28\n",
      "INFO:allennlp.training.trainer: Epoch 5/19\n",
      "2021-05-10 10:05:46,428 - allennlp.training.trainer - INFO - Epoch 5/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:05:46,431 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:05:46,434 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49e342b97b64c179620d9b083b30e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:06:17,450 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     0.999  |       N/A\n",
      "2021-05-10 10:06:17,452 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     0.999  |       N/A\n",
      "2021-05-10 10:06:17,456 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     0.999  |       N/A\n",
      "2021-05-10 10:06:17,459 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:06:17,462 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:06:17,465 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     0.999  |       N/A\n",
      "2021-05-10 10:06:17,469 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     0.999  |       N/A\n",
      "2021-05-10 10:06:17,474 - allennlp.training.tensorboard_writer - INFO - accuracy           |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.002  |       N/A\n",
      "2021-05-10 10:06:17,477 - allennlp.training.tensorboard_writer - INFO - loss               |     0.002  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     0.999  |       N/A\n",
      "2021-05-10 10:06:17,481 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     0.999  |       N/A\n",
      "2021-05-10 10:06:17,484 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     0.999  |       N/A\n",
      "2021-05-10 10:06:17,488 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     0.999  |       N/A\n",
      "2021-05-10 10:06:17,492 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     0.999  |       N/A\n",
      "2021-05-10 10:06:17,495 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     0.999  |       N/A\n",
      "2021-05-10 10:06:17,498 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     0.999  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:06:17,502 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:06:17,523 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:31.126133\n",
      "2021-05-10 10:06:17,554 - allennlp.training.trainer - INFO - Epoch duration: 0:00:31.126133\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:07:01\n",
      "2021-05-10 10:06:17,557 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:07:01\n",
      "INFO:allennlp.training.trainer: Epoch 6/19\n",
      "2021-05-10 10:06:17,562 - allennlp.training.trainer - INFO - Epoch 6/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:06:17,565 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:06:17,569 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4c76801a694311aa3142b9e6cb2727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:06:48,070 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,073 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,075 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,079 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,083 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,086 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,089 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,092 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:06:48,096 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,099 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,103 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,106 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,110 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,114 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:06:48,117 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:06:48,121 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:06:48,143 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:30.603468\n",
      "2021-05-10 10:06:48,166 - allennlp.training.trainer - INFO - Epoch duration: 0:00:30.603468\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:06:32\n",
      "2021-05-10 10:06:48,167 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:06:32\n",
      "INFO:allennlp.training.trainer: Epoch 7/19\n",
      "2021-05-10 10:06:48,171 - allennlp.training.trainer - INFO - Epoch 7/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:06:48,174 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:06:48,177 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a298a296d44e4a5cbf43408001bc79ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:07:19,117 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,120 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,124 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,128 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,131 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,134 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,139 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,143 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:07:19,147 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,152 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,156 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,164 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,168 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,173 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:07:19,176 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:07:19,179 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:07:19,200 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:31.054160\n",
      "2021-05-10 10:07:19,225 - allennlp.training.trainer - INFO - Epoch duration: 0:00:31.054160\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:06:03\n",
      "2021-05-10 10:07:19,227 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:06:03\n",
      "INFO:allennlp.training.trainer: Epoch 8/19\n",
      "2021-05-10 10:07:19,229 - allennlp.training.trainer - INFO - Epoch 8/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:07:19,233 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:07:19,236 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1514ce1a6bd4189b49d1fc467c9c021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:07:48,719 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,721 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,724 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,729 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,732 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,736 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,739 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,743 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:07:48,746 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,751 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,759 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,764 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,773 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,776 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:07:48,780 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:07:48,782 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:07:48,809 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:29.608123\n",
      "2021-05-10 10:07:48,837 - allennlp.training.trainer - INFO - Epoch duration: 0:00:29.608123\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:05:32\n",
      "2021-05-10 10:07:48,840 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:05:32\n",
      "INFO:allennlp.training.trainer: Epoch 9/19\n",
      "2021-05-10 10:07:48,844 - allennlp.training.trainer - INFO - Epoch 9/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:07:48,851 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:07:48,855 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dcbe5ab688490dbaf832cade7243b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:08:20,635 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,637 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,640 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,644 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,647 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,651 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,654 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,656 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:08:20,660 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,664 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,667 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,670 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,673 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,676 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:08:20,680 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:08:20,683 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:08:20,704 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:31.889384\n",
      "2021-05-10 10:08:20,733 - allennlp.training.trainer - INFO - Epoch duration: 0:00:31.889384\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:05:03\n",
      "2021-05-10 10:08:20,735 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:05:03\n",
      "INFO:allennlp.training.trainer: Epoch 10/19\n",
      "2021-05-10 10:08:20,739 - allennlp.training.trainer - INFO - Epoch 10/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:08:20,742 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:08:20,745 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b516bda8ebc8426bab58770d6eef4cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:08:52,167 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,170 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,173 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,176 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,179 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,183 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,186 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,190 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:08:52,193 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,196 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,199 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,202 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,206 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,210 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:08:52,214 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:08:52,217 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:08:52,237 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:31.522085\n",
      "2021-05-10 10:08:52,261 - allennlp.training.trainer - INFO - Epoch duration: 0:00:31.522085\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:04:34\n",
      "2021-05-10 10:08:52,263 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:04:34\n",
      "INFO:allennlp.training.trainer: Epoch 11/19\n",
      "2021-05-10 10:08:52,266 - allennlp.training.trainer - INFO - Epoch 11/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:08:52,269 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:08:52,273 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789610198caf4822b3e8699992a727f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:09:20,964 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:09:20,967 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:09:20,970 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:09:20,973 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:09:20,976 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:09:20,979 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:09:20,982 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:09:20,985 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:09:20,988 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:09:20,992 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:09:20,995 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:09:20,998 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:09:21,004 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:09:21,007 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:09:21,011 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:09:21,014 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:09:21,039 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:28.804435\n",
      "2021-05-10 10:09:21,070 - allennlp.training.trainer - INFO - Epoch duration: 0:00:28.804435\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:04:02\n",
      "2021-05-10 10:09:21,073 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:04:02\n",
      "INFO:allennlp.training.trainer: Epoch 12/19\n",
      "2021-05-10 10:09:21,075 - allennlp.training.trainer - INFO - Epoch 12/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:09:21,078 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:09:21,081 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e09ee5d7bab45a985442d3fc4fc4aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:09:50,216 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,218 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,221 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,225 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,229 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,233 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,237 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,240 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:09:50,244 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,248 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,251 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,254 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,258 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,261 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:09:50,265 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:09:50,268 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:09:50,289 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:29.239764\n",
      "2021-05-10 10:09:50,315 - allennlp.training.trainer - INFO - Epoch duration: 0:00:29.239764\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:03:31\n",
      "2021-05-10 10:09:50,317 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:03:31\n",
      "INFO:allennlp.training.trainer: Epoch 13/19\n",
      "2021-05-10 10:09:50,320 - allennlp.training.trainer - INFO - Epoch 13/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:09:50,323 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:09:50,326 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed912e87c95446586cfebdf3b200d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:10:20,122 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,124 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,128 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,132 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,137 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,140 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,144 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,148 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:10:20,152 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,155 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,159 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,164 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,168 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,172 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:10:20,175 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:10:20,180 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:10:20,203 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:29.911201\n",
      "2021-05-10 10:10:20,231 - allennlp.training.trainer - INFO - Epoch duration: 0:00:29.911201\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:03:01\n",
      "2021-05-10 10:10:20,234 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:03:01\n",
      "INFO:allennlp.training.trainer: Epoch 14/19\n",
      "2021-05-10 10:10:20,236 - allennlp.training.trainer - INFO - Epoch 14/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:10:20,239 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:10:20,242 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb79fac36c67456e80ce340876121de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:10:50,277 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,279 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,282 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,286 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,290 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,293 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,298 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,301 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:10:50,308 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,312 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,316 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,321 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,324 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,329 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:10:50,332 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:10:50,336 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:10:50,364 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:30.156915\n",
      "2021-05-10 10:10:50,393 - allennlp.training.trainer - INFO - Epoch duration: 0:00:30.156915\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:02:31\n",
      "2021-05-10 10:10:50,396 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:02:31\n",
      "INFO:allennlp.training.trainer: Epoch 15/19\n",
      "2021-05-10 10:10:50,399 - allennlp.training.trainer - INFO - Epoch 15/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:10:50,403 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:10:50,406 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40945fa7cedb4b529d063648b7cc227d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:11:21,198 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,201 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,203 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,207 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,210 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,214 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,218 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,221 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:11:21,224 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,228 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,231 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,235 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,238 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,241 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:11:21,244 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:11:21,248 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:11:21,270 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:30.895345\n",
      "2021-05-10 10:11:21,295 - allennlp.training.trainer - INFO - Epoch duration: 0:00:30.895345\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:02:01\n",
      "2021-05-10 10:11:21,297 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:02:01\n",
      "INFO:allennlp.training.trainer: Epoch 16/19\n",
      "2021-05-10 10:11:21,300 - allennlp.training.trainer - INFO - Epoch 16/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:11:21,303 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:11:21,306 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dec7ac784df49a5881de31a08b28285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:11:51,404 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,407 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,413 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,416 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,420 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,425 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,430 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,434 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:11:51,437 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,441 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,444 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,447 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,452 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,461 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:11:51,465 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:11:51,468 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:11:51,496 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:30.228908\n",
      "2021-05-10 10:11:51,529 - allennlp.training.trainer - INFO - Epoch duration: 0:00:30.228908\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:01:30\n",
      "2021-05-10 10:11:51,533 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:01:30\n",
      "INFO:allennlp.training.trainer: Epoch 17/19\n",
      "2021-05-10 10:11:51,536 - allennlp.training.trainer - INFO - Epoch 17/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:11:51,540 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:11:51,544 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a70db99434a418baea487f532a3b2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:12:20,862 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,864 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,868 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,872 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,875 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,880 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,883 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,887 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:12:20,897 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,901 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,904 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,909 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,912 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,915 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:12:20,919 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:12:20,924 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:12:20,949 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:29.444772\n",
      "2021-05-10 10:12:20,981 - allennlp.training.trainer - INFO - Epoch duration: 0:00:29.444772\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:01:00\n",
      "2021-05-10 10:12:20,983 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:01:00\n",
      "INFO:allennlp.training.trainer: Epoch 18/19\n",
      "2021-05-10 10:12:20,986 - allennlp.training.trainer - INFO - Epoch 18/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:12:20,991 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:12:20,994 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475fbd4951f0487da759f48652b26e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:12:50,208 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,210 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,213 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,217 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,221 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,224 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,228 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,231 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:12:50,234 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,237 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,241 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,244 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,248 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,251 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:12:50,255 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:12:50,259 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:12:50,281 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:29.317864\n",
      "2021-05-10 10:12:50,304 - allennlp.training.trainer - INFO - Epoch duration: 0:00:29.317864\n",
      "INFO:allennlp.training.trainer: Estimated training time remaining: 0:00:30\n",
      "2021-05-10 10:12:50,306 - allennlp.training.trainer - INFO - Estimated training time remaining: 0:00:30\n",
      "INFO:allennlp.training.trainer: Epoch 19/19\n",
      "2021-05-10 10:12:50,309 - allennlp.training.trainer - INFO - Epoch 19/19\n",
      "INFO:allennlp.training.trainer: Worker 0 memory usage: 1.8G\n",
      "2021-05-10 10:12:50,312 - allennlp.training.trainer - INFO - Worker 0 memory usage: 1.8G\n",
      "INFO:allennlp.training.trainer: Training\n",
      "2021-05-10 10:12:50,315 - allennlp.training.trainer - INFO - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c99a7948774cd0a697ca9c72a2a256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.training.tensorboard_writer:                        Training |  Validation\n",
      "2021-05-10 10:13:19,380 - allennlp.training.tensorboard_writer - INFO -                        Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,382 - allennlp.training.tensorboard_writer - INFO - _fscore/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _fscore/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,386 - allennlp.training.tensorboard_writer - INFO - _fscore/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/HAM     |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,390 - allennlp.training.tensorboard_writer - INFO - _precision/HAM     |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _precision/SPAM    |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,394 - allennlp.training.tensorboard_writer - INFO - _precision/SPAM    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/HAM        |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,397 - allennlp.training.tensorboard_writer - INFO - _recall/HAM        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: _recall/SPAM       |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,401 - allennlp.training.tensorboard_writer - INFO - _recall/SPAM       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: accuracy           |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,404 - allennlp.training.tensorboard_writer - INFO - accuracy           |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: loss               |     0.000  |       N/A\n",
      "2021-05-10 10:13:19,411 - allennlp.training.tensorboard_writer - INFO - loss               |     0.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,415 - allennlp.training.tensorboard_writer - INFO - macro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,419 - allennlp.training.tensorboard_writer - INFO - macro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: macro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,423 - allennlp.training.tensorboard_writer - INFO - macro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/fscore       |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,427 - allennlp.training.tensorboard_writer - INFO - micro/fscore       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/precision    |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,433 - allennlp.training.tensorboard_writer - INFO - micro/precision    |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: micro/recall       |     1.000  |       N/A\n",
      "2021-05-10 10:13:19,438 - allennlp.training.tensorboard_writer - INFO - micro/recall       |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer: worker_0_memory_MB |  1850.324  |       N/A\n",
      "2021-05-10 10:13:19,442 - allennlp.training.tensorboard_writer - INFO - worker_0_memory_MB |  1850.324  |       N/A\n",
      "INFO:allennlp.training.checkpointer: Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "2021-05-10 10:13:19,464 - allennlp.training.checkpointer - INFO - Best validation performance so far. Copying weights to 'path_to_store_training_run_output/best.th'.\n",
      "INFO:allennlp.training.trainer: Epoch duration: 0:00:29.188675\n",
      "2021-05-10 10:13:19,498 - allennlp.training.trainer - INFO - Epoch duration: 0:00:29.188675\n",
      "INFO:allennlp.training.checkpointer: loading best weights\n",
      "2021-05-10 10:13:19,500 - allennlp.training.checkpointer - INFO - loading best weights\n",
      "INFO:biome.text._helpers: The model will be evaluated using the best epoch weights.\n",
      "2021-05-10 10:13:19,534 - biome.text._helpers - INFO - The model will be evaluated using the best epoch weights.\n",
      "INFO:allennlp.training.util: Iterating over dataset\n",
      "2021-05-10 10:13:19,541 - allennlp.training.util - INFO - Iterating over dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70690658fb2846e78f25ed244d0c3d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:allennlp.models.archival: archiving weights and vocabulary to path_to_store_training_run_output/model.tar.gz\n",
      "2021-05-10 10:13:20,127 - allennlp.models.archival - INFO - archiving weights and vocabulary to path_to_store_training_run_output/model.tar.gz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 288<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba6d07c01914dec924bea9f88d55b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>.wandb/.wandb/run-20210510_100314-36ol5p92/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>.wandb/.wandb/run-20210510_100314-36ol5p92/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>best_epoch</td><td>19</td></tr><tr><td>peak_worker_0_memory_MB</td><td>1850.32422</td></tr><tr><td>training_duration</td><td>0:10:02.455251</td></tr><tr><td>training_start_epoch</td><td>0</td></tr><tr><td>training_epochs</td><td>19</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>training_accuracy</td><td>1.0</td></tr><tr><td>training_micro/precision</td><td>1.0</td></tr><tr><td>training_micro/recall</td><td>1.0</td></tr><tr><td>training_micro/fscore</td><td>1.0</td></tr><tr><td>training_macro/precision</td><td>1.0</td></tr><tr><td>training_macro/recall</td><td>1.0</td></tr><tr><td>training_macro/fscore</td><td>1.0</td></tr><tr><td>training__precision/HAM</td><td>1.0</td></tr><tr><td>training__precision/SPAM</td><td>1.0</td></tr><tr><td>training__recall/HAM</td><td>1.0</td></tr><tr><td>training__recall/SPAM</td><td>1.0</td></tr><tr><td>training__fscore/HAM</td><td>1.0</td></tr><tr><td>training__fscore/SPAM</td><td>1.0</td></tr><tr><td>training_loss</td><td>1e-05</td></tr><tr><td>training_worker_0_memory_MB</td><td>1850.32422</td></tr><tr><td>_runtime</td><td>605</td></tr><tr><td>_timestamp</td><td>1620634399</td></tr><tr><td>_step</td><td>19</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>best_epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>peak_worker_0_memory_MB</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training_start_epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>training_epochs</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>training_accuracy</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training_micro/precision</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training_micro/recall</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training_micro/fscore</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training_macro/precision</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training_macro/recall</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training_macro/fscore</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training__precision/HAM</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training__precision/SPAM</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training__recall/HAM</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training__recall/SPAM</td><td>‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training__fscore/HAM</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training__fscore/SPAM</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>training_loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>training_worker_0_memory_MB</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>_runtime</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>_timestamp</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>_step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">ethereal-lion-155</strong>: <a href=\"https://wandb.ai/javispp/biome/runs/36ol5p92\" target=\"_blank\">https://wandb.ai/javispp/biome/runs/36ol5p92</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from biome.text import Pipeline, Dataset\n",
    "\n",
    "ds_train = Dataset.from_pandas(df_train_filtered)\n",
    "ds_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "pipeline = Pipeline.from_config({\n",
    "    \"name\": \"my-first-classifier\",\n",
    "    \"features\": {\n",
    "        \"word\": {\n",
    "            \"embedding_dim\": 300,\n",
    "            \"weights_file\":\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\",\n",
    "            \"lowercase_tokens\": True,\n",
    "        },\n",
    "        \"char\": {\n",
    "            \"embedding_dim\": 32,\n",
    "            \"lowercase_characters\": True,\n",
    "            \"encoder\": {\n",
    "                \"type\": \"gru\",\n",
    "                \"num_layers\": 1,\n",
    "                \"hidden_size\": 32,\n",
    "                \"bidirectional\": True,\n",
    "            },\n",
    "            \"dropout\": 0.0,\n",
    "        },\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"type\": \"TextClassification\",\n",
    "        \"labels\": list(set(df_test[\"label\"])),\n",
    "        \"pooler\": {\n",
    "            \"type\": \"gru\",\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_size\": 128,\n",
    "            \"bidirectional\": True,\n",
    "        },\n",
    "        \"feedforward\": {\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_dims\": [32],\n",
    "            \"activations\": [\"relu\"],\n",
    "            \"dropout\": [0.0],\n",
    "        },\n",
    "    },       \n",
    "})\n",
    "\n",
    "training_results = pipeline.train(\n",
    "    output=\"path_to_store_training_run_output\",\n",
    "    training=ds_train,\n",
    "    test=ds_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-picnic",
   "metadata": {},
   "source": [
    "Once the process has finished, we can go to the \"path_to_store_training_run_output\" folder and open our metrics.json to see all the information. We won't show everything, but make sure to check all the information available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "miniature-seeker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple pipeline test accuracy: 0.912\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./path_to_store_training_run_output/metrics.json\") as json_file:\n",
    "    metrics = json.load(json_file)\n",
    "    \n",
    "print(\"Simple pipeline test accuracy:\", metrics[\"test_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "driving-belief",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['SPAM', 'HAM'],\n",
       " 'probabilities': [0.9999998807907104, 1.6763804921993142e-07]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(\"Don't forget to check out biome.text for more information :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-charge",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-humor",
   "metadata": {},
   "source": [
    "In this tutorial, we accomplished the following:\n",
    "\n",
    "- We introduced the concept of Labeling Functions (LFs) and demonstrated some of the forms they can take.\n",
    "- We used the Snorkel LabelModel to automatically learn how to combine the outputs of our LFs into strong probabilistic labels.\n",
    "- We showed how we can incorporate Rubrix into our workflow to get faster and more accurate results.\n",
    "- We built and tested a simple pipeline using biome.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-tonight",
   "metadata": {},
   "source": [
    "## What's next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-emergency",
   "metadata": {},
   "source": [
    "We invite you to check our other tutorials and join our community, a good place to start is our [discussion forum](https://github.com/recognai/rubrix/discussions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
