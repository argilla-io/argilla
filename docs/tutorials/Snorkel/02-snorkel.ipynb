{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "metropolitan-adelaide",
   "metadata": {
    "tags": []
   },
   "source": [
    "# üêô Using Rubrix and Snorkel for human-in-the-loop weak supervision\n",
    "\n",
    "In this tutorial, we will walk through the process of using Rubrix to improve weak supervision and data programming workflows with the amazing Snorkel library.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Our goal is to show you how you can incorporate Rubrix into data programming workflows** to programatically build training data with a human-in-the-loop approach. We will use the widely-known [Snorkel](https://www.snorkel.org/) library, but a similar approach can be used with other data augmentation libraries such as [Textattack](https://github.com/QData/TextAttack) or [nlpaug](https://github.com/makcedward/nlpaug).\n",
    "\n",
    "### What is weak supervision? and Snorkel?\n",
    "\n",
    "Weak supervision is a branch of machine learning based on getting lower quality labels more efficiently. We can achieve this by using Snorkel, a library for programmatically building and managing training datasets without manual labeling.\n",
    "\n",
    "### This tutorial\n",
    "\n",
    "In this tutorial, we will follow the [Spam classification tutorial](https://www.snorkel.org/use-cases/01-spam-tutorial) from Snorkel's documentation and add specific parts where you can leverage Rubrix for an improved workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-magic",
   "metadata": {},
   "source": [
    "The tutorial is organized into:\n",
    "\n",
    "1. **Loading and exploring data**: \n",
    "\n",
    "2. **Finding and writing good labelling functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-avenue",
   "metadata": {},
   "source": [
    "## Install Snorkel, Textblob and spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "copyrighted-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install snorkel textblob -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "laden-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-index",
   "metadata": {},
   "source": [
    "## Setup Rubrix\n",
    "\n",
    "[here we should point the user to the install and setup guide]\n",
    "\n",
    "And then show how to start with local or remote install.\n",
    "\n",
    "By default, rubrix will make a local initialization (as shown in the setup guide). If you want to specify an API url and key, you can pass that information via two environment variables: `RUBRIX_API_KEY` and `RUBRIX_API_URL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "material-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-halloween",
   "metadata": {},
   "source": [
    "## 1. Loading data\n",
    "\n",
    "Rubrix allows you to log and track data for different NLP tasks (such as *Token Classification* or *Text Classification*). \n",
    "\n",
    "In this tutorial, we will use the [YouTube Spam Collection](http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/) dataset which a binary classification task for detecting spam comments in youtube videos.\n",
    "\n",
    "Let's load it in Pandas and take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cutting-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# we should avoid this in the final tutorial (use S3, or a util function)\n",
    "# i've just created the splits locally for reproducing the Snorkel results\n",
    "df_train = pd.read_csv('yt_comments_train.csv')\n",
    "df_test = pd.read_csv('yt_comments_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certain-convention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Alessandro leite</td>\n",
       "      <td>2014-11-05T22:21:36</td>\n",
       "      <td>pls http://www10.vakinha.com.br/VaquinhaE.aspx...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Salim Tayara</td>\n",
       "      <td>2014-11-02T14:33:30</td>\n",
       "      <td>if your like drones, plz subscribe to Kamal Ta...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Phuc Ly</td>\n",
       "      <td>2014-01-20T15:27:47</td>\n",
       "      <td>go here to check the views :3Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DropShotSk8r</td>\n",
       "      <td>2014-01-19T04:27:18</td>\n",
       "      <td>Came here to check the views, goodbye.Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>css403</td>\n",
       "      <td>2014-11-07T14:25:48</td>\n",
       "      <td>i am 2,126,492,636 viewer :DÔªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            author                 date  \\\n",
       "0           0  Alessandro leite  2014-11-05T22:21:36   \n",
       "1           1      Salim Tayara  2014-11-02T14:33:30   \n",
       "2           2           Phuc Ly  2014-01-20T15:27:47   \n",
       "3           3      DropShotSk8r  2014-01-19T04:27:18   \n",
       "4           4            css403  2014-11-07T14:25:48   \n",
       "\n",
       "                                                text  label  video  \n",
       "0  pls http://www10.vakinha.com.br/VaquinhaE.aspx...   -1.0      1  \n",
       "1  if your like drones, plz subscribe to Kamal Ta...   -1.0      1  \n",
       "2                     go here to check the views :3Ôªø   -1.0      1  \n",
       "3            Came here to check the views, goodbye.Ôªø   -1.0      1  \n",
       "4                      i am 2,126,492,636 viewer :DÔªø   -1.0      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quiet-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity, we define constants to represent the class labels for spam, ham, and abstaining.\n",
    "ABSTAIN = -1\n",
    "HAM = 0\n",
    "SPAM = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-pension",
   "metadata": {},
   "source": [
    "## 2. Finding and writing good labelling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-biography",
   "metadata": {},
   "source": [
    "### What are labeling functions?\n",
    "\n",
    "Labeling functions (LFs) are heuristics that take a data point as input and assign a label to it or abstain (don‚Äôt assign any label).\n",
    "LFs are noisy (they might not have perfect accuracy) and don't have to label every data point.\n",
    "\n",
    "Some of the most common LFs rely on:\n",
    "\n",
    "- Keyword searches: looking for specific words in a sentence\n",
    "- Pattern matching: looking for specific syntactical patterns\n",
    "- Third-party models: using an pre-trained model (usually a model for a different task than the one at hand)\n",
    "- Distant supervision: using external knowledge base\n",
    "- Crowdworker labels: treating each crowdworker as a black-box function that assigns labels to subsets of the data\n",
    "\n",
    "More information can be found in the [Snorkel LFs tutorial](https://www.snorkel.org/use-cases/01-spam-tutorial#a-gentle-introduction-to-lfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-spoke",
   "metadata": {},
   "source": [
    "### Typical LF development workflow\n",
    "\n",
    "As mentioned in the Snorkel tutorial, a LF development cycle looks like this:\n",
    "    \n",
    "1. Look at examples to generate ideas for LFs\n",
    "2. Write an initial version of an LF\n",
    "3. Spot check its performance by looking at its output on data points in the training set (or development set if available)\n",
    "4. Refine and debug to improve coverage or accuracy as necessary\n",
    "\n",
    "Let's go through these steps and use Rubrix along the way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-locking",
   "metadata": {},
   "source": [
    "#### a) Exploring the training set with Rubrix for initial inspiration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-proceeding",
   "metadata": {},
   "source": [
    "First of all, we have to create the dataset and load it into rubrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "signed-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "records= []\n",
    "\n",
    "for index, record in df_train.iterrows():     \n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=index,\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        metadata = {\n",
    "            \"textlen\": str(len(record.text)), \n",
    "            \"author\": record.author,\n",
    "            \"video\": str(record.video)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "entire-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tried to log data without previous initialization. An initialization by default has been performed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_spam_snorkel', processed=1586, failed=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_spam_snorkel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-selection",
   "metadata": {},
   "source": [
    "Once we have it into rubrix, we can explore the data and the metadata we just added by pressing the \"view metadata\" button on the bottom right corner of each sample. Take a look at the following picture:\n",
    "\n",
    "<img src=\"metadata_view.png\">\n",
    "\n",
    "Now, we can explore the metadata by going to the top left. Let's say we want to check the different authors or inspect all the data with a specific one. By going to the author section, we can easily do all of this.\n",
    "\n",
    "<img src=\"metadata_author.png\">\n",
    "\n",
    "After applying the changes we should only see the comments that belong to the selected author. We can also add multiple conditions besides the name of the author, but we will keep it simple this time.  \n",
    "Another option is to use the top right search box to find samples that contain a certain word or a phrase (\"put inside parantheses\"). We will search for \"check\", since it is more likely that those comments are SPAM. \n",
    "\n",
    "<img src=\"search_check.png\">\n",
    "\n",
    "By looking at the comments, it seems like most of them belong to the SPAM class. It's time we create a function that assigns the SPAM label to those comments.  \n",
    "We will also add a rule that assigns the SPAM label to comments with the expression \"check out\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-comment",
   "metadata": {},
   "source": [
    "#### b) Writing our initial LFs and analyzing their outputs with Rubrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suspected-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def check(x):\n",
    "    return SPAM if \"check\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def check_out(x):\n",
    "    return SPAM if \"check out\" in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "continuous-islam",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:00<00:00, 56539.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "# List of labeling functions\n",
    "lfs = [check_out, check]\n",
    "\n",
    "# Apply labeling functions to the dataset (df_train)\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-operation",
   "metadata": {},
   "source": [
    "To see how the functions affect our dataset, we can show some information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "naughty-struggle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check_out</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.214376</td>\n",
       "      <td>0.214376</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.257881</td>\n",
       "      <td>0.214376</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           j Polarity  Coverage  Overlaps  Conflicts\n",
       "check_out  0      [1]  0.214376  0.214376        0.0\n",
       "check      1      [1]  0.257881  0.214376        0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We could eventually remove this and further analyses to keep it short and direct. \n",
    "# Leaving only the final summary with all lfs\n",
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-surgeon",
   "metadata": {},
   "source": [
    "[[Snorkel source]](https://www.snorkel.org/use-cases/01-spam-tutorial#c-evaluate-performance-on-training-set) Let‚Äôs see 10 data points where the \"check out\" rule abstained, but \"check\" rule labeled. We can use the `get_label_buckets()` to group data points by their predicted label and/or true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "south-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "buckets = get_label_buckets(L_train[:, 0], L_train[:, 1]) # 0 corresponds to checkout lf and 1 to check lf\n",
    "sample_abstain_spam = df_train.iloc[buckets[(ABSTAIN, SPAM)]]#.sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unlikely-restaurant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Phuc Ly</td>\n",
       "      <td>2014-01-20T15:27:47</td>\n",
       "      <td>go here to check the views :3Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DropShotSk8r</td>\n",
       "      <td>2014-01-19T04:27:18</td>\n",
       "      <td>Came here to check the views, goodbye.Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>zhichao wang</td>\n",
       "      <td>2013-11-29T02:13:56</td>\n",
       "      <td>i think about 100 millions of the views come f...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>BeBe Burkey</td>\n",
       "      <td>2013-11-28T16:30:13</td>\n",
       "      <td>and u should.d check my channel and tell me wh...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>BIGMOFO Tonkatruck</td>\n",
       "      <td>2014-11-12T06:26:42</td>\n",
       "      <td>just came to check the view countÔªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>8</td>\n",
       "      <td>Dominic Randall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi everyone. We are a duo and we are starting ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>352</td>\n",
       "      <td>MrJtill0317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>‚îè‚îÅ‚îÅ‚îÅ‚îì‚îè‚îì‚ïã‚îè‚îì‚îè‚îÅ‚îÅ‚îÅ‚îì‚îè‚îÅ‚îÅ‚îÅ‚îì‚îè‚îì‚ïã‚ïã‚îè‚îì  ‚îÉ‚îè‚îÅ‚îì‚îÉ‚îÉ‚îÉ‚ïã‚îÉ‚îÉ‚îÉ‚îè‚îÅ‚îì‚îÉ‚îó‚îì‚îè...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>372</td>\n",
       "      <td>George Raps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñ∫‚ñ∫My name is George and let me tell u EMIN...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>416</td>\n",
       "      <td>Chelsea Cameron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*****PLEASE READ*****  Hey everyone! I&amp;#39;m a...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>446</td>\n",
       "      <td>Lil M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lil m !!!!! Check hi out!!!!! Does live the wa...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0              author                 date  \\\n",
       "2              2             Phuc Ly  2014-01-20T15:27:47   \n",
       "3              3        DropShotSk8r  2014-01-19T04:27:18   \n",
       "16            16        zhichao wang  2013-11-29T02:13:56   \n",
       "21            21         BeBe Burkey  2013-11-28T16:30:13   \n",
       "38            38  BIGMOFO Tonkatruck  2014-11-12T06:26:42   \n",
       "...          ...                 ...                  ...   \n",
       "1146           8     Dominic Randall                  NaN   \n",
       "1490         352         MrJtill0317                  NaN   \n",
       "1510         372         George Raps                  NaN   \n",
       "1554         416     Chelsea Cameron                  NaN   \n",
       "1584         446               Lil M                  NaN   \n",
       "\n",
       "                                                   text  label  video  \n",
       "2                        go here to check the views :3Ôªø   -1.0      1  \n",
       "3               Came here to check the views, goodbye.Ôªø   -1.0      1  \n",
       "16    i think about 100 millions of the views come f...   -1.0      1  \n",
       "21    and u should.d check my channel and tell me wh...   -1.0      1  \n",
       "38                   just came to check the view countÔªø   -1.0      1  \n",
       "...                                                 ...    ...    ...  \n",
       "1146  Hi everyone. We are a duo and we are starting ...   -1.0      4  \n",
       "1490  ‚îè‚îÅ‚îÅ‚îÅ‚îì‚îè‚îì‚ïã‚îè‚îì‚îè‚îÅ‚îÅ‚îÅ‚îì‚îè‚îÅ‚îÅ‚îÅ‚îì‚îè‚îì‚ïã‚ïã‚îè‚îì  ‚îÉ‚îè‚îÅ‚îì‚îÉ‚îÉ‚îÉ‚ïã‚îÉ‚îÉ‚îÉ‚îè‚îÅ‚îì‚îÉ‚îó‚îì‚îè...   -1.0      4  \n",
       "1510  ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñ∫‚ñ∫My name is George and let me tell u EMIN...   -1.0      4  \n",
       "1554  *****PLEASE READ*****  Hey everyone! I&#39;m a...   -1.0      4  \n",
       "1584  Lil m !!!!! Check hi out!!!!! Does live the wa...   -1.0      4  \n",
       "\n",
       "[69 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_abstain_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-toyota",
   "metadata": {},
   "source": [
    "Let's use Rubrix to see how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "natural-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "records= []\n",
    "\n",
    "for index, record in sample_abstain_spam.iterrows():     \n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=str(index),\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        metadata = {\n",
    "            \"textlen\": str(len(record.text)), \n",
    "            \"author\": record.author,\n",
    "            \"video\": str(record.video)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "blind-vietnamese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_spam_snorkel_sample_abstain_spam', processed=69, failed=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_spam_snorkel_sample_abstain_spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-mechanics",
   "metadata": {},
   "source": [
    "Most of these are SPAM, but a good number are false positives. One way to keep precision high (while not sacrificing much in terms of coverage), would be to use a regular expression to include examples where the word \"check\" is followed by something else and then the word \"out\", for example \"check this out\".\n",
    "We could explore the dataset with Rubrix and try to find more precise patterns, or discover completely new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "valuable-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function that checks if the pattern \"check [something_else] out\" exists\n",
    "@labeling_function()\n",
    "def regex_check_out(x):\n",
    "    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suspected-fleet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:00<00:00, 44651.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# List of labeling functions\n",
    "lfs = [check_out, regex_check_out]\n",
    "\n",
    "# Apply labeling functions to the dataset (df_train)\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "polyphonic-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = get_label_buckets(L_train[:, 0], L_train[:, 1]) # 0 corresponds to check_out lf and 1 to regex_check_out lf\n",
    "yt_sample_abstain_spam_regex = df_train.iloc[buckets[(ABSTAIN, SPAM)]].sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "understanding-inquiry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>8</td>\n",
       "      <td>Dominic Randall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi everyone. We are a duo and we are starting ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>178</td>\n",
       "      <td>Imprezzi Vidzz</td>\n",
       "      <td>2014-11-04T03:12:23</td>\n",
       "      <td>My videos are half way decent, check them out ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>185</td>\n",
       "      <td>David Sean</td>\n",
       "      <td>2014-09-15T10:53:46</td>\n",
       "      <td>Hey guys! I've made a amazing Smiley T-Shirt.O...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>115</td>\n",
       "      <td>Mah Productions</td>\n",
       "      <td>2014-07-26T17:07:51.274000</td>\n",
       "      <td>Check the shit out on my channel&lt;br /&gt;&lt;br /&gt;&lt;b...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>320</td>\n",
       "      <td>Nadeen Efein</td>\n",
       "      <td>2014-10-01T21:15:49</td>\n",
       "      <td>hi beaties! i made a new channel please go che...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>280</td>\n",
       "      <td>Michael Jurek</td>\n",
       "      <td>2014-11-02T13:37:06</td>\n",
       "      <td>I did a cover if u want to check it out THANK ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>213</td>\n",
       "      <td>Technibility</td>\n",
       "      <td>2014-09-12T21:08:47</td>\n",
       "      <td>Great video by a great artist in Katy Perry! A...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>179</td>\n",
       "      <td>Nerdy Peach</td>\n",
       "      <td>2014-10-29T22:44:41</td>\n",
       "      <td>Hey! I'm NERDY PEACH and I'm a new youtuber an...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>Kyle Jaber</td>\n",
       "      <td>2014-01-19T00:21:29</td>\n",
       "      <td>Check me out! I'm kyle. I rap so yeah Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>309</td>\n",
       "      <td>Hazetrix (EHazardStudio)</td>\n",
       "      <td>2015-04-08T00:09:33.033000</td>\n",
       "      <td>hey guys im 17 years old remixer and producer ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                    author                        date  \\\n",
       "1146           8           Dominic Randall                         NaN   \n",
       "178          178            Imprezzi Vidzz         2014-11-04T03:12:23   \n",
       "535          185                David Sean         2014-09-15T10:53:46   \n",
       "815          115           Mah Productions  2014-07-26T17:07:51.274000   \n",
       "670          320              Nadeen Efein         2014-10-01T21:15:49   \n",
       "630          280             Michael Jurek         2014-11-02T13:37:06   \n",
       "563          213              Technibility         2014-09-12T21:08:47   \n",
       "529          179               Nerdy Peach         2014-10-29T22:44:41   \n",
       "270          270                Kyle Jaber         2014-01-19T00:21:29   \n",
       "1009         309  Hazetrix (EHazardStudio)  2015-04-08T00:09:33.033000   \n",
       "\n",
       "                                                   text  label  video  \n",
       "1146  Hi everyone. We are a duo and we are starting ...   -1.0      4  \n",
       "178   My videos are half way decent, check them out ...   -1.0      1  \n",
       "535   Hey guys! I've made a amazing Smiley T-Shirt.O...   -1.0      2  \n",
       "815   Check the shit out on my channel<br /><br /><b...   -1.0      3  \n",
       "670   hi beaties! i made a new channel please go che...   -1.0      2  \n",
       "630   I did a cover if u want to check it out THANK ...   -1.0      2  \n",
       "563   Great video by a great artist in Katy Perry! A...   -1.0      2  \n",
       "529   Hey! I'm NERDY PEACH and I'm a new youtuber an...   -1.0      2  \n",
       "270             Check me out! I'm kyle. I rap so yeah Ôªø   -1.0      1  \n",
       "1009  hey guys im 17 years old remixer and producer ...   -1.0      3  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_sample_abstain_spam_regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-chassis",
   "metadata": {},
   "source": [
    "Load dataset into Rubrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "chief-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "records= []\n",
    "\n",
    "for index, record in yt_sample_abstain_spam_regex.iterrows():     \n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=str(index),\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        metadata = {\n",
    "            \"textlen\": str(len(record.text)), \n",
    "            \"author\": record.author,\n",
    "            \"video\": str(record.video)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fifteen-greene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_spam_snorkel_sample_abstain_spam_regex', processed=10, failed=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_spam_snorkel_sample_abstain_spam_regex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-smooth",
   "metadata": {},
   "source": [
    "Seems like the regex rule catches quite some spam that is overlooked by the simple \"check out\" rule. Let's discard the two initial rules in favor of the regex rule for the rest of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-garden",
   "metadata": {},
   "source": [
    "#### c) Writing Keyword LFs with Rubrix\n",
    "\n",
    "In the same way we created labeling functions for rules before, we can create label functions that check a list of keywords using the `LabelingFunction` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adolescent-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelingFunction\n",
    "\n",
    "\n",
    "def keyword_lookup(x, keywords, label):\n",
    "    if any(word in x.text.lower() for word in keywords):\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "def make_keyword_lf(keywords, label=SPAM):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-subdivision",
   "metadata": {},
   "source": [
    "This are from the original Snorkel tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "geographic-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "keyword_my = make_keyword_lf(keywords=[\"my\"])\n",
    "\n",
    "\"\"\"Spam comments ask users to subscribe to their channels.\"\"\"\n",
    "keyword_subscribe = make_keyword_lf(keywords=[\"subscribe\"])\n",
    "\n",
    "\"\"\"Spam comments post links to other channels.\"\"\"\n",
    "keyword_link = make_keyword_lf(keywords=[\"http\"])\n",
    "\n",
    "\"\"\"Spam comments make requests rather than commenting.\"\"\"\n",
    "keyword_please = make_keyword_lf(keywords=[\"please\", \"plz\"])\n",
    "\n",
    "\"\"\"Ham comments actually talk about the video's content.\"\"\"\n",
    "keyword_song = make_keyword_lf(keywords=[\"song\"], label=HAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-gothic",
   "metadata": {},
   "source": [
    "But we could always try to find better lists of words by searching the word we want in our Rubrix dataset and decide whether or not the word should be considered key to filter the SPAM/HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "prerequisite-immunology",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:00<00:00, 19187.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# List of labeling functions\n",
    "lfs = [keyword_my, keyword_subscribe, keyword_link, keyword_please, keyword_song, regex_check_out]\n",
    "\n",
    "# Apply labeling functions to the dataset (df_train)\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "invalid-renaissance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>keyword_my</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.198613</td>\n",
       "      <td>0.165826</td>\n",
       "      <td>0.030265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_subscribe</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.127364</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.008827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_http</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.119168</td>\n",
       "      <td>0.042875</td>\n",
       "      <td>0.005675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_please</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.112232</td>\n",
       "      <td>0.102774</td>\n",
       "      <td>0.013241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_song</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.141866</td>\n",
       "      <td>0.043506</td>\n",
       "      <td>0.043506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.233922</td>\n",
       "      <td>0.101513</td>\n",
       "      <td>0.020177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   j Polarity  Coverage  Overlaps  Conflicts\n",
       "keyword_my         0      [1]  0.198613  0.165826   0.030265\n",
       "keyword_subscribe  1      [1]  0.127364  0.081967   0.008827\n",
       "keyword_http       2      [1]  0.119168  0.042875   0.005675\n",
       "keyword_please     3      [1]  0.112232  0.102774   0.013241\n",
       "keyword_song       4      [0]  0.141866  0.043506   0.043506\n",
       "regex_check_out    5      [1]  0.233922  0.101513   0.020177"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-messaging",
   "metadata": {},
   "source": [
    "#### d) Writing Heuristic LFs with Rubrix\n",
    "\n",
    "We have already seen how to use keywords to label our data, the next step would be to use heuristics to do the labeling. A simple approach to this could be setting a minimum length to the comment, considering it SPAM if its length is lower than a threshold.  \n",
    "To use the right threshold we are going to explore our data in Rubrix by using the metadata field, similar to what we did before with the author selection. For this example we will use a threshold of 20 'words'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "lonely-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def short_comment(x):\n",
    "    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n",
    "    return HAM if len(x.text.split()) < 20 else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "representative-covering",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:00<00:00, 43625.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# List of labeling functions\n",
    "lfs = [short_comment]\n",
    "\n",
    "# Apply labeling functions to the dataset (df_train)\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "excellent-intellectual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.810845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               j Polarity  Coverage  Overlaps  Conflicts\n",
       "short_comment  0      [0]  0.810845       0.0        0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-force",
   "metadata": {},
   "source": [
    "#### e) Writing and exploring third-party models LFs\n",
    "\n",
    "Let's explore Textblob predictions on the training set with Rubrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "still-gates",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "records= []\n",
    "for index, record in df_train.iterrows():   \n",
    "    scores = TextBlob(record[\"text\"])\n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=str(index),\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        multi_label= False,\n",
    "        prediction=[(\"subjectivity\", max(0.0, scores.sentiment.subjectivity))],\n",
    "        prediction_agent=\"TextBlob\",\n",
    "        metadata = {\n",
    "            \"textlen\": str(len(record.text)), \n",
    "            \"author\": record.author,\n",
    "            \"video\": str(record.video)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "corporate-horizontal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_spam_snorkel_textblob', processed=1586, failed=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_spam_snorkel_textblob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-holiday",
   "metadata": {},
   "source": [
    "Checking the dataset, we can filter our data based on the confidence of our classifier. This can help us since the predictions of our TextBlob tend to be SPAM the lower the subjectivity is. We can take advantage of this by filtering the predictions using intervals of confidence. For this example we are going to set a threshold of 0.56 for the subjectivity.  \n",
    "\n",
    "<img src=\"confidence_interval.png\">\n",
    "\n",
    "The same way we did it for subjectivity, we can do it for polarity. In this case we will set the threshold to 0.9.\n",
    "This comes in handy when we want to add restrictions on top of another. This time we won't go much deeper, but we could also use metadata or create more than what we have, in combination with the confidence, to have a better understanding of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-wallace",
   "metadata": {},
   "source": [
    "Once we have our conclusions, we can proceed with creating the labeling functions using the thresholds for the confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "wrapped-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def textblob_sentiment(x):\n",
    "    scores = TextBlob(x.text)\n",
    "    x.polarity = scores.sentiment.polarity\n",
    "    x.subjectivity = scores.sentiment.subjectivity\n",
    "    return x\n",
    "\n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_subjectivity(x):\n",
    "    return HAM if x.subjectivity >= 0.5 else ABSTAIN\n",
    "\n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_polarity(x):\n",
    "    return HAM if x.polarity >= 0.9 else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sustainable-cycling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:00<00:00, 1760.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>textblob_polarity</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.035939</td>\n",
       "      <td>0.014502</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.357503</td>\n",
       "      <td>0.014502</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       j Polarity  Coverage  Overlaps  Conflicts\n",
       "textblob_polarity      0      [0]  0.035939  0.014502        0.0\n",
       "textblob_subjectivity  1      [0]  0.357503  0.014502        0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfs = [textblob_polarity, textblob_subjectivity]\n",
    "\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)\n",
    "\n",
    "LFAnalysis(L_train, lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-bidder",
   "metadata": {},
   "source": [
    "#### SpaCy LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "essential-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "# The SpacyPreprocessor parses the text in text_field and\n",
    "# stores the new enriched representation in doc_field\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-printer",
   "metadata": {},
   "source": [
    "We can also use information from a [spaCy](https://spacy.io/) model in our labeling functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "drawn-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function(pre=[spacy])\n",
    "def has_person(x):\n",
    "    \"\"\"Ham comments mention specific people and are short.\"\"\"\n",
    "    if len(x.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-honey",
   "metadata": {},
   "source": [
    "However, spaCy is such a common preprocessor for NLP applications and Snorkel also provides a prebuilt labeling_function-like decorator that uses spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "satellite-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.lf.nlp import nlp_labeling_function\n",
    "\n",
    "\n",
    "@nlp_labeling_function()\n",
    "def has_person_nlp(x):\n",
    "    # Ham comments usually mention specific people\n",
    "    if any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "black-wesley",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:07<00:00, 222.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# List of labeling functions\n",
    "lfs = [has_person_nlp, regex_check_out]\n",
    "\n",
    "# Apply labeling functions to the dataset (df_train)\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "uniform-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = get_label_buckets(L_train[:, 0], L_train[:, 1]) # 0 corresponds to person lf and 1to regex_check_out lf\n",
    "yt_sample_ham_abstain_person = df_train.iloc[buckets[(HAM, SPAM)]].sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adjustable-perry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>212</td>\n",
       "      <td>101Tele</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yo I know nobody will probably even read this....</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>155</td>\n",
       "      <td>TheJohnRage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alright ladies, if you like this song, then ch...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>439</td>\n",
       "      <td>Andrew Guasch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Listen...Check out Andrew Guasch - Crazy, Sick...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>246</td>\n",
       "      <td>media.uploader</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check out my channel to see Rihanna short mix ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>228</td>\n",
       "      <td>DanteBTV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check Out The New Hot Video By Dante B Called ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>374</td>\n",
       "      <td>Jacob Johnson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You guys should check out this EXTRAORDINARY w...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>321</td>\n",
       "      <td>killtheclockhd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>check out our bands page on youtube killtheclo...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>234</td>\n",
       "      <td>marion guy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Okay trust me I&amp;#39;m doing a favor. You NEED ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>339</td>\n",
       "      <td>Carren Mangali</td>\n",
       "      <td>2015-03-21T07:01:04.171000</td>\n",
       "      <td>Check out this playlist on YouTube:üç¥üç¥üèÑüèÑüèÑüç¥üèÑüèÑüèÑüèÑüèä...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0          author                        date  \\\n",
       "1350         212         101Tele                         NaN   \n",
       "92            92       Julius NM         2013-11-07T06:20:48   \n",
       "1293         155     TheJohnRage                         NaN   \n",
       "1577         439   Andrew Guasch                         NaN   \n",
       "1384         246  media.uploader                         NaN   \n",
       "1366         228        DanteBTV                         NaN   \n",
       "1512         374   Jacob Johnson                         NaN   \n",
       "1459         321  killtheclockhd                         NaN   \n",
       "1372         234      marion guy                         NaN   \n",
       "1039         339  Carren Mangali  2015-03-21T07:01:04.171000   \n",
       "\n",
       "                                                   text  label  video  \n",
       "1350  yo I know nobody will probably even read this....   -1.0      4  \n",
       "92    Huh, anyway check out this you[tube] channel: ...   -1.0      1  \n",
       "1293  Alright ladies, if you like this song, then ch...   -1.0      4  \n",
       "1577  Listen...Check out Andrew Guasch - Crazy, Sick...   -1.0      4  \n",
       "1384  Check out my channel to see Rihanna short mix ...   -1.0      4  \n",
       "1366  Check Out The New Hot Video By Dante B Called ...   -1.0      4  \n",
       "1512  You guys should check out this EXTRAORDINARY w...   -1.0      4  \n",
       "1459  check out our bands page on youtube killtheclo...   -1.0      4  \n",
       "1372  Okay trust me I&#39;m doing a favor. You NEED ...   -1.0      4  \n",
       "1039  Check out this playlist on YouTube:üç¥üç¥üèÑüèÑüèÑüç¥üèÑüèÑüèÑüèÑüèä...   -1.0      3  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_sample_ham_abstain_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "surprising-analyst",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>has_person_nlp</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.123581</td>\n",
       "      <td>0.034678</td>\n",
       "      <td>0.034678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.233922</td>\n",
       "      <td>0.034678</td>\n",
       "      <td>0.034678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 j Polarity  Coverage  Overlaps  Conflicts\n",
       "has_person_nlp   0      [0]  0.123581  0.034678   0.034678\n",
       "regex_check_out  1      [1]  0.233922  0.034678   0.034678"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L_train, lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-reproduction",
   "metadata": {},
   "source": [
    "As we can see, since one is predicting HAM and the other one SPAM, the overlap is the same as the conflicts, but the amount is pretty small. The coverage might not be impressive but could be considered significant if the has_person_nlp heuristic works well. \n",
    "\n",
    "Our dataset is ready now to be used in Rubrix and see how it looks. We can also combine it with our TextBlob model (or any other) and make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "greek-gibson",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "records= []\n",
    "\n",
    "for index, record in yt_sample_ham_abstain_person.iterrows():   \n",
    "    scores = TextBlob(record[\"text\"])\n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=str(index),\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        multi_label= True,\n",
    "        prediction=[(\"subjectivity\", max(0.0, scores.sentiment.subjectivity)),\n",
    "                    (\"polarity\", max(0.0, scores.sentiment.polarity))],\n",
    "        prediction_agent=\"TextBlob\",\n",
    "        metadata = {\n",
    "            \"textlen\": str(len(record.text)), \n",
    "            \"author\": record.author,\n",
    "            \"video\": str(record.video)\n",
    "        }\n",
    "    )\n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "premier-stereo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_spam_snorkel_spacy', processed=10, failed=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_spam_snorkel_spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-jersey",
   "metadata": {},
   "source": [
    "## 4. Using Snorkel Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-chorus",
   "metadata": {},
   "source": [
    "We have mentioned multiple functions that could be used to label our data, but we never gave a solution on how to deal with the overlap and conflicts. In this section, we will deal with this problem.  \n",
    "But first, let's see all the functions we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ranging-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [\n",
    "    keyword_my,\n",
    "    keyword_subscribe,\n",
    "    keyword_link,\n",
    "    keyword_please,\n",
    "    keyword_song,\n",
    "    regex_check_out,\n",
    "    short_comment,\n",
    "    has_person_nlp,\n",
    "    textblob_polarity,\n",
    "    textblob_subjectivity,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-johnston",
   "metadata": {},
   "source": [
    "And apply it to our training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "defined-spanish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:00<00:00, 10145.38it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:01<00:00, 176.34it/s]\n"
     ]
    }
   ],
   "source": [
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_test = applier.apply(df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "through-amsterdam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>keyword_my</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.198613</td>\n",
       "      <td>0.197982</td>\n",
       "      <td>0.175914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_subscribe</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.127364</td>\n",
       "      <td>0.124212</td>\n",
       "      <td>0.108449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_http</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.119168</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.108449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_please</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.112232</td>\n",
       "      <td>0.112232</td>\n",
       "      <td>0.094578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_song</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.141866</td>\n",
       "      <td>0.134931</td>\n",
       "      <td>0.043506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.233922</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>0.216267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>6</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.810845</td>\n",
       "      <td>0.612863</td>\n",
       "      <td>0.372636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_person_nlp</th>\n",
       "      <td>7</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.123581</td>\n",
       "      <td>0.121059</td>\n",
       "      <td>0.071879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_polarity</th>\n",
       "      <td>8</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.035939</td>\n",
       "      <td>0.035939</td>\n",
       "      <td>0.005675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <td>9</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.357503</td>\n",
       "      <td>0.339218</td>\n",
       "      <td>0.160151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       j Polarity  Coverage  Overlaps  Conflicts\n",
       "keyword_my             0      [1]  0.198613  0.197982   0.175914\n",
       "keyword_subscribe      1      [1]  0.127364  0.124212   0.108449\n",
       "keyword_http           2      [1]  0.119168  0.115385   0.108449\n",
       "keyword_please         3      [1]  0.112232  0.112232   0.094578\n",
       "keyword_song           4      [0]  0.141866  0.134931   0.043506\n",
       "regex_check_out        5      [1]  0.233922  0.231400   0.216267\n",
       "short_comment          6      [0]  0.810845  0.612863   0.372636\n",
       "has_person_nlp         7      [0]  0.123581  0.121059   0.071879\n",
       "textblob_polarity      8      [0]  0.035939  0.035939   0.005675\n",
       "textblob_subjectivity  9      [0]  0.357503  0.339218   0.160151"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-australia",
   "metadata": {},
   "source": [
    "A simple baseline for doing this is to take the majority vote on a per-data point basis: if more LFs voted SPAM than HAM, label it SPAM (and vice versa). We can test this with the [MajorityLabelVoter](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.model.baselines.MajorityLabelVoter.html#snorkel.labeling.model.baselines.MajorityLabelVoter) baseline model implemented in Snorkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "emerging-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L=L_train) # y_train labels\n",
    "Y_test = df_test.label.values # y_test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-reverse",
   "metadata": {},
   "source": [
    "The major inconvenience of this approach is that we defined rules that could be correlated, resulting in certain signals being overrepresented in a majority-vote-based model.  \n",
    "To handle this issue, we are going to make use of the LabelModel. You can read more about how it works in the [Snorkel tutorial](https://www.snorkel.org/use-cases/01-spam-tutorial#4-combining-labeling-function-outputs-with-the-label-model) and the [documentation](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.model.label_model.LabelModel.html#snorkel.labeling.model.label_model.LabelModel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "instructional-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True) # cardinality = n¬∫ of classes\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "stuck-battle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy:   76.0%\n",
      "Label Model Accuracy:     88.8%\n"
     ]
    }
   ],
   "source": [
    "majority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-samoa",
   "metadata": {},
   "source": [
    "As we can see, the LabelModel outperforms the basic MajorityLabelVoter configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-proof",
   "metadata": {},
   "source": [
    "#### Filtering unlabeled data\n",
    "\n",
    "The method we saw above, has a small inconvenience, our data can receive no prediction (predicted as ABSTAIN) from any of our LFs. Those cases with no label (labeled as ABSTAIN) will be removed from the training dataset using a [built-in utility](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.filter_unlabeled_dataframe.html#snorkel.labeling.filter_unlabeled_dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "emerging-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_train,\n",
    "    y=label_model.predict_proba(L_train), # Probabilities of each data point for each class\n",
    "    L=L_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-iceland",
   "metadata": {},
   "source": [
    "Now that we have our data, we can explore the results in Rubrix and manually relabel those cases that have been wrongly classified or keep exploring the performance of our LFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "sticky-documentary",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "i=0\n",
    "for index, record in df_train_filtered.iterrows():   \n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=str(index),\n",
    "        inputs={\"text\": record[\"text\"]},\n",
    "        multilabel=True,\n",
    "        # our confidences/scores come from probs_train_filtered\n",
    "        # probs_train_filtered[i][j] is the probability the sample i belongs to class j\n",
    "        prediction=[(\"HAM\", probs_train_filtered[i][0]),   # 0 for HAM\n",
    "                    (\"SPAM\", probs_train_filtered[i][1])], # 1 for SPAM\n",
    "        prediction_agent=\"LabelModel\",\n",
    "        metadata = {\n",
    "            \"textlen\": str(len(record.text)), \n",
    "            \"author\": record.author,\n",
    "            \"video\": str(record.video)\n",
    "        }\n",
    "    )\n",
    "    records.append(item)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "serious-stream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_filtered_classified_sample', processed=1568, failed=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_filtered_classified_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-sterling",
   "metadata": {},
   "source": [
    "To relabel the data we switch into the annotation mode by pressing the bottom right button in the rubrix UI. \n",
    "After that, we can simply click on the label for each sample and it will be marked as a \"validated sample\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-genesis",
   "metadata": {},
   "source": [
    "## 5. Train a classifier and explore its predictions\n",
    "\n",
    "The last thing we can do with our data is training a classifier using some of the most popular libraries such as Scikit-learn, Tensorflow or Pytorch. For simplicity, we will use Scikit-learn, a well known library in ML.  \n",
    "We recommend checking [biome.txt](https://www.recogn.ai/biome-text/), a simple and powerful tool for NLP problems, developed by the same team behind Rubrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "manufactured-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 5)) # Bag Of Words (BoW) with n-grams\n",
    "X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\n",
    "X_test = vectorizer.transform(df_test.text.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-ceramic",
   "metadata": {},
   "source": [
    "Since we need to tell the model the class for each sample, and we have probabilities, we can assign to each sample the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "spectacular-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-edinburgh",
   "metadata": {},
   "source": [
    "And then build the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "exempt-amateur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000.0, solver='liblinear')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sklearn_model = LogisticRegression(C=1e3, solver=\"liblinear\")\n",
    "sklearn_model.fit(X=X_train, y=preds_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eight-superior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 89.6%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {sklearn_model.score(X=X_test, y=Y_test) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-hobby",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-damages",
   "metadata": {},
   "source": [
    "In this tutorial, we accomplished the following:\n",
    "\n",
    "- We introduced the concept of Labeling Functions (LFs) and demonstrated some of the forms they can take.\n",
    "- We used the Snorkel LabelModel to automatically learn how to combine the outputs of our LFs into strong probabilistic labels.\n",
    "- We showed that a classifier trained on a weakly supervised dataset can outperform an approach based on the LFs alone as it learns to generalize beyond the noisy heuristics we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-notion",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-child",
   "metadata": {},
   "source": [
    "If you enjoyed this tutorial, check out the [Snorkel Tutorials](https://www.snorkel.org/use-cases/) page for other tutorials that you may find interesting, including demonstrations of how to use Snorkel\n",
    "\n",
    "- [As part of a hybrid crowdsourcing pipeline](https://www.snorkel.org/use-cases/crowdsourcing-tutorial)\n",
    "- [For visual relationship detection over images](https://www.snorkel.org/use-cases/visual-relation-tutorial)\n",
    "- [For information extraction over text](https://www.snorkel.org/use-cases/spouse-demo)\n",
    "- [For data augmentation](https://www.snorkel.org/use-cases/02-spam-data-augmentation-tutorial)\n",
    "\n",
    "and more! You can also visit the [Snorkel website](https://www.snorkel.org/) or [Snorkel API](https://snorkel.readthedocs.io/en/v0.9.7/) documentation for more info!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
