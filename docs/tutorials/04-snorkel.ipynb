{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "micro-child",
   "metadata": {
    "tags": []
   },
   "source": [
    "# üê† Human-in-the-loop weak supervision with `snorkel`\n",
    "\n",
    "This tutorial will walk you through the process of using Rubrix to improve weak supervision and data programming workflows with the amazing Snorkel library.\n",
    "\n",
    "![Snorkel explore](https://github.com/recognai/rubrix-materials/raw/main/tutorials/snorkel/snorkel1.gif \"Snorkel exploration with Rubrix\")\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Our goal is to show you how you can incorporate Rubrix into data programming workflows** to programatically build training data with a human-in-the-loop approach. We will use the widely-known [Snorkel](https://www.snorkel.org/) library, but a similar approach can be used with other data augmentation libraries such as [Textattack](https://github.com/QData/TextAttack) or [nlpaug](https://github.com/makcedward/nlpaug).\n",
    "\n",
    "### What is weak supervision? and Snorkel?\n",
    "\n",
    "Weak supervision is a branch of machine learning based on getting lower quality labels more efficiently. We can achieve this by using Snorkel, a library for programmatically building and managing training datasets without manual labeling.\n",
    "\n",
    "### This tutorial\n",
    "\n",
    "In this tutorial, we'll follow the [Spam classification tutorial](https://www.snorkel.org/use-cases/01-spam-tutorial) from Snorkel's documentation and show you how to extend weak supervision workflows with Rubrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-physiology",
   "metadata": {},
   "source": [
    "The tutorial is organized into:\n",
    "\n",
    "1. **Spam classification with Snorkel**: we provide a brief overview of the tutorial\n",
    "\n",
    "2. **Extending and finding labeling functions with Rubrix**: we analyze different strategies for extending the proposed labeling functions and for exploring new labeling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-advancement",
   "metadata": {},
   "source": [
    "## Install Snorkel, Textblob and spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snorkel textblob spacy -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56c572-1daa-46b0-9006-0c1a5eb5080e",
   "metadata": {},
   "source": [
    "## Setup Rubrix\n",
    "\n",
    "**If you are new to Rubrix, visit and ‚≠ê star Rubrix for more materials like and detailed docs**: [Github repo](https://github.com/recognai/rubrix)\n",
    "\n",
    "If you have not installed and launched Rubrix, check the [Setup and Installation guide](https://docs.rubrix.ml/en/latest/getting_started/setup%26installation.html).\n",
    "\n",
    "\n",
    "Once installed, you only need to import Rubrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efba3564-faeb-4eb0-b813-fd05b56cb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-logistics",
   "metadata": {},
   "source": [
    "## 1. Spam classification with Snorkel\n",
    "\n",
    "Rubrix allows you to log and track data for different NLP tasks (such as `Token Classification` or `Text Classification`). \n",
    "\n",
    "In this tutorial, we will use the [YouTube Spam Collection](http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/) dataset which a binary classification task for detecting spam comments in youtube videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-possession",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "\n",
    "We have a training set and and a test set. The first one does not include the label of the samples and it is set to -1. The test set contains ground-truth labels from the original dataset, where the label is set to 1 if it's considered SPAM and 0 for HAM.  \n",
    "\n",
    "In this tutorial we'll be using Snorkel's data programming methods for programatically building a training set with the help of Rubrix for analizing and reviewing data. We'll then train a model with this train set and evaluate it against the test set.\n",
    "\n",
    "Let's load it in Pandas and take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "settled-designation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Alessandro leite</td>\n",
       "      <td>2014-11-05T22:21:36</td>\n",
       "      <td>pls http://www10.vakinha.com.br/VaquinhaE.aspx...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Salim Tayara</td>\n",
       "      <td>2014-11-02T14:33:30</td>\n",
       "      <td>if your like drones, plz subscribe to Kamal Ta...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Phuc Ly</td>\n",
       "      <td>2014-01-20T15:27:47</td>\n",
       "      <td>go here to check the views :3Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DropShotSk8r</td>\n",
       "      <td>2014-01-19T04:27:18</td>\n",
       "      <td>Came here to check the views, goodbye.Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>css403</td>\n",
       "      <td>2014-11-07T14:25:48</td>\n",
       "      <td>i am 2,126,492,636 viewer :DÔªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>443</td>\n",
       "      <td>Themayerlife</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check out my mummy chanel!</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>444</td>\n",
       "      <td>Fill Reseni</td>\n",
       "      <td>2015-05-27T17:10:53.724000</td>\n",
       "      <td>The rap: cool     Rihanna: STTUUPIDÔªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>445</td>\n",
       "      <td>Greg Fils Aim√©</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I hope everyone is in good spirits I&amp;#39;m a h...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>446</td>\n",
       "      <td>Lil M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lil m !!!!! Check hi out!!!!! Does live the wa...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>447</td>\n",
       "      <td>AvidorFilms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please check out my youtube channel! Just uplo...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1586 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0            author                        date  \\\n",
       "0              0  Alessandro leite         2014-11-05T22:21:36   \n",
       "1              1      Salim Tayara         2014-11-02T14:33:30   \n",
       "2              2           Phuc Ly         2014-01-20T15:27:47   \n",
       "3              3      DropShotSk8r         2014-01-19T04:27:18   \n",
       "4              4            css403         2014-11-07T14:25:48   \n",
       "...          ...               ...                         ...   \n",
       "1581         443      Themayerlife                         NaN   \n",
       "1582         444       Fill Reseni  2015-05-27T17:10:53.724000   \n",
       "1583         445    Greg Fils Aim√©                         NaN   \n",
       "1584         446             Lil M                         NaN   \n",
       "1585         447       AvidorFilms                         NaN   \n",
       "\n",
       "                                                   text  label  video  \n",
       "0     pls http://www10.vakinha.com.br/VaquinhaE.aspx...   -1.0      1  \n",
       "1     if your like drones, plz subscribe to Kamal Ta...   -1.0      1  \n",
       "2                        go here to check the views :3Ôªø   -1.0      1  \n",
       "3               Came here to check the views, goodbye.Ôªø   -1.0      1  \n",
       "4                         i am 2,126,492,636 viewer :DÔªø   -1.0      1  \n",
       "...                                                 ...    ...    ...  \n",
       "1581                         Check out my mummy chanel!   -1.0      4  \n",
       "1582               The rap: cool     Rihanna: STTUUPIDÔªø   -1.0      4  \n",
       "1583  I hope everyone is in good spirits I&#39;m a h...   -1.0      4  \n",
       "1584  Lil m !!!!! Check hi out!!!!! Does live the wa...   -1.0      4  \n",
       "1585  Please check out my youtube channel! Just uplo...   -1.0      4  \n",
       "\n",
       "[1586 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>‚Ä´ÿ≠ŸÑŸÖ ÿßŸÑÿ¥ÿ®ÿßÿ®‚Ä¨‚Äé</td>\n",
       "      <td>2015-05-25T23:42:49.533000</td>\n",
       "      <td>Check out this video on YouTube:Ôªø</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194</td>\n",
       "      <td>MOHAMED THASLEEM</td>\n",
       "      <td>2015-05-24T07:03:59.488000</td>\n",
       "      <td>super musicÔªø</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>277</td>\n",
       "      <td>AlabaGames</td>\n",
       "      <td>2015-05-22T00:31:43.922000</td>\n",
       "      <td>Subscribe my channel ¬†I RECORDING FIFA 15 GOAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>Manish Ray</td>\n",
       "      <td>2015-05-23T08:55:07.512000</td>\n",
       "      <td>This song is so beauty</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>163</td>\n",
       "      <td>Sudheer Yadav</td>\n",
       "      <td>2015-05-28T10:28:25.133000</td>\n",
       "      <td>SEE SOME MORE SONG OPEN GOOGLE AND TYPE Shakir...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>32</td>\n",
       "      <td>GamezZ MTA</td>\n",
       "      <td>2015-05-09T00:08:26.185000</td>\n",
       "      <td>Pleas subscribe my channelÔªø</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>176</td>\n",
       "      <td>Viv Varghese</td>\n",
       "      <td>2015-05-25T08:59:50.837000</td>\n",
       "      <td>The best FIFA world cup song for sure.Ôªø</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>314</td>\n",
       "      <td>yakikukamo FIRELOVER</td>\n",
       "      <td>2013-07-18T17:07:06.152000</td>\n",
       "      <td>hey you ! check out the channel of Alvar Lake !!</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>25</td>\n",
       "      <td>James Cook</td>\n",
       "      <td>2013-10-10T18:08:07.815000</td>\n",
       "      <td>Hello Guys...I Found a Way to Make Money Onlin...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>11</td>\n",
       "      <td>Trulee IsNotAmazing</td>\n",
       "      <td>2013-09-07T14:18:22.601000</td>\n",
       "      <td>Beautiful song beautiful girl it works</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                author                        date  \\\n",
       "0            27         ‚Ä´ÿ≠ŸÑŸÖ ÿßŸÑÿ¥ÿ®ÿßÿ®‚Ä¨‚Äé  2015-05-25T23:42:49.533000   \n",
       "1           194      MOHAMED THASLEEM  2015-05-24T07:03:59.488000   \n",
       "2           277            AlabaGames  2015-05-22T00:31:43.922000   \n",
       "3           132            Manish Ray  2015-05-23T08:55:07.512000   \n",
       "4           163         Sudheer Yadav  2015-05-28T10:28:25.133000   \n",
       "..          ...                   ...                         ...   \n",
       "245          32            GamezZ MTA  2015-05-09T00:08:26.185000   \n",
       "246         176          Viv Varghese  2015-05-25T08:59:50.837000   \n",
       "247         314  yakikukamo FIRELOVER  2013-07-18T17:07:06.152000   \n",
       "248          25            James Cook  2013-10-10T18:08:07.815000   \n",
       "249          11   Trulee IsNotAmazing  2013-09-07T14:18:22.601000   \n",
       "\n",
       "                                                  text  label  video  \n",
       "0                    Check out this video on YouTube:Ôªø      1      5  \n",
       "1                                         super musicÔªø      0      5  \n",
       "2    Subscribe my channel ¬†I RECORDING FIFA 15 GOAL...      1      5  \n",
       "3                               This song is so beauty      0      5  \n",
       "4    SEE SOME MORE SONG OPEN GOOGLE AND TYPE Shakir...      1      5  \n",
       "..                                                 ...    ...    ...  \n",
       "245                        Pleas subscribe my channelÔªø      1      5  \n",
       "246            The best FIFA world cup song for sure.Ôªø      0      5  \n",
       "247   hey you ! check out the channel of Alvar Lake !!      1      5  \n",
       "248  Hello Guys...I Found a Way to Make Money Onlin...      1      5  \n",
       "249             Beautiful song beautiful girl it works      0      5  \n",
       "\n",
       "[250 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('data/yt_comments_train.csv')\n",
    "df_test = pd.read_csv('data/yt_comments_test.csv')\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-buffalo",
   "metadata": {},
   "source": [
    "### Labeling functions\n",
    "\n",
    "Labeling functions (LFs) are Python function which encode heuristics (such as keywords or pattern matching), distant supervision methods (using external knowledge) or even \"low-quality\" crowd-worker label datasets. The goal is to create a probabilistic model which is able to combine the output of a set of noisy labels assigned by this LFs. Snorkel provides several strategies for defining and combining LFs, for more information check [Snorkel LFs tutorial](https://www.snorkel.org/use-cases/01-spam-tutorial#a-gentle-introduction-to-lfs).\n",
    "\n",
    "In this tutorial, we will first define the LFs from the Snorkel tutorial and then show you how you can use Rubrix to enhance this type of weak-supervision workflows.\n",
    "\n",
    "Let's take a look at the original LFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "academic-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from snorkel.labeling import labeling_function, LabelingFunction\n",
    "from snorkel.labeling.lf.nlp import nlp_labeling_function\n",
    "from snorkel.preprocess import preprocessor\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "ABSTAIN = -1\n",
    "HAM = 0\n",
    "SPAM = 1\n",
    "\n",
    "# Keyword searches\n",
    "@labeling_function()\n",
    "def check(x):\n",
    "    return SPAM if \"check\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def check_out(x):\n",
    "    return SPAM if \"check out\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "# Heuristics\n",
    "@labeling_function()\n",
    "def short_comment(x):\n",
    "    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n",
    "    return HAM if len(x.text.split()) < 5 else ABSTAIN\n",
    "\n",
    "# List of keywords\n",
    "def keyword_lookup(x, keywords, label):\n",
    "    if any(word in x.text.lower() for word in keywords):\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "\n",
    "def make_keyword_lf(keywords, label=SPAM):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label),\n",
    "    )\n",
    "\n",
    "\"\"\"Spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "keyword_my = make_keyword_lf(keywords=[\"my\"])\n",
    "\n",
    "\"\"\"Spam comments ask users to subscribe to their channels.\"\"\"\n",
    "keyword_subscribe = make_keyword_lf(keywords=[\"subscribe\"])\n",
    "\n",
    "\"\"\"Spam comments post links to other channels.\"\"\"\n",
    "keyword_link = make_keyword_lf(keywords=[\"http\"])\n",
    "\n",
    "\"\"\"Spam comments make requests rather than commenting.\"\"\"\n",
    "keyword_please = make_keyword_lf(keywords=[\"please\", \"plz\"])\n",
    "\n",
    "\"\"\"Ham comments actually talk about the video's content.\"\"\"\n",
    "keyword_song = make_keyword_lf(keywords=[\"song\"], label=HAM)\n",
    "\n",
    "\n",
    "# Pattern matching with regex\n",
    "@labeling_function()\n",
    "def regex_check_out(x):\n",
    "    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN\n",
    "\n",
    "\n",
    "# Third party models (TextBlob and spaCy)\n",
    "# TextBlob\n",
    "@preprocessor(memoize=True)\n",
    "def textblob_sentiment(x):\n",
    "    scores = TextBlob(x.text)\n",
    "    x.polarity = scores.sentiment.polarity\n",
    "    x.subjectivity = scores.sentiment.subjectivity\n",
    "    return x\n",
    "\n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_subjectivity(x):\n",
    "    return HAM if x.subjectivity >= 0.5 else ABSTAIN\n",
    "\n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_polarity(x):\n",
    "    return HAM if x.polarity >= 0.9 else ABSTAIN\n",
    "\n",
    "# spaCy\n",
    "\n",
    "# There are two different methods to use spaCy:\n",
    "# Method 1:\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)\n",
    "\n",
    "@labeling_function(pre=[spacy])\n",
    "def has_person(x):\n",
    "    \"\"\"Ham comments mention specific people and are short.\"\"\"\n",
    "    if len(x.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "# Method 2:\n",
    "@nlp_labeling_function()\n",
    "def has_person_nlp(x):\n",
    "    \"\"\"Ham comments mention specific people.\"\"\"\n",
    "    if any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "residential-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of labeling functions proposed at \n",
    "original_labelling_functions = [\n",
    "    keyword_my,\n",
    "    keyword_subscribe,\n",
    "    keyword_link,\n",
    "    keyword_please,\n",
    "    keyword_song,\n",
    "    regex_check_out,\n",
    "    short_comment,\n",
    "    has_person_nlp,\n",
    "    textblob_polarity,\n",
    "    textblob_subjectivity,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-bennett",
   "metadata": {},
   "source": [
    "We have mentioned multiple functions that could be used to label our data, but we never gave a solution on how to deal with the overlap and conflicts. \n",
    "\n",
    "To handle this issue, Snorkel provide the `LabelModel`. You can read more about how it works in the [Snorkel tutorial](https://www.snorkel.org/use-cases/01-spam-tutorial#4-combining-labeling-function-outputs-with-the-label-model) and the [documentation](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.model.label_model.LabelModel.html#snorkel.labeling.model.label_model.LabelModel).\n",
    "\n",
    "Let's just use a `LabelModel` to test the proposed LFs and let's wrap it into a function so we can reuse it to evaluate new LFs along the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sized-regular",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:14<00:00, 112.31it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:02<00:00, 98.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Model Accuracy:     85.6%\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "def test_label_model(lfs):\n",
    "\n",
    "    # Apply LFs to datasets\n",
    "    applier = PandasLFApplier(lfs=lfs)\n",
    "    L_train = applier.apply(df=df_train)\n",
    "    L_test = applier.apply(df=df_test)\n",
    "    Y_test = df_test.label.values # y_test labels\n",
    "\n",
    "    label_model = LabelModel(cardinality=2, verbose=True) # cardinality = n¬∫ of classes\n",
    "    label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)\n",
    "\n",
    "    label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "        \"accuracy\"\n",
    "    ]\n",
    "    print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")\n",
    "    return label_model\n",
    "\n",
    "label_model = test_label_model(original_labelling_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-building",
   "metadata": {},
   "source": [
    "## 2. Extending and finding labeling functions with Rubrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1162d218-34da-43d5-a459-5be048092302",
   "metadata": {},
   "source": [
    "In this section, we'll review some of the LFs from the original tutorial and see how to use Rubrix in combination with Snorkel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-thriller",
   "metadata": {},
   "source": [
    "### Exploring the training set with Rubrix for initial inspiration\n",
    "\n",
    "Rubrix lets you track data for different NLP tasks (such as *Token Classification* or *Text Classification*). \n",
    "\n",
    "Let's log our unlabelled training set into Rubrix for initial inspiration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "agreed-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "records= []\n",
    "\n",
    "for index, record in df_train.iterrows():     \n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=index,\n",
    "        inputs=record[\"text\"],\n",
    "        metadata = {\n",
    "            \"author\": record.author,\n",
    "            \"video\": str(record.video)\n",
    "        }\n",
    "    )\n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "welcome-gross",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_spam_snorkel', processed=1586, failed=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_spam_snorkel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233fc745-48bf-49e3-8b00-a03d0cb805dd",
   "metadata": {},
   "source": [
    "After a few seconds, we have a fully searchable version of our unlabelled training set, which can be used for quickly defining new LFs or improve existing ones. We can of course view our data on a text editor, using Pandas or printing rows on a Jupyter Notebook, but Rubrix focuses on making this easy and powerful with features like searching using the [Elasticsearch's query string DSL](https://docs.rubrix.ml/en/latest/reference/rubrix_webapp_reference.html#search-input), or the ability to log arbitrary inputs and metadata items.\n",
    "\n",
    "![Snorkel explore](https://github.com/recognai/rubrix-materials/raw/main/tutorials/snorkel/snorkel1.gif \"Snorkel exploration with Rubrix\")\n",
    "\n",
    "\n",
    "\n",
    "First thing we can see on our Rubrix Dataset are the most frequent keywords on our text field. With just a quick look, we can see the coverage of two of the proposed keyword-based LFs (using the word \"check\" and \"subscribe\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-spanish",
   "metadata": {},
   "source": [
    "\n",
    "Another thing we can do is to explore by metadata. Let's say we want to check the distribution by authors, as maybe some authors are posting SPAM several times with different wordings. Here we can see one of the top posting authors, who's also a top spammer, but seems to be using very similar messages:\n",
    "\n",
    "\n",
    "\n",
    "Exploring some other top spammers, we see some of them use the word \"money\", let's check some examples using this keyword:\n",
    "\n",
    "![Snorkel explore](https://github.com/recognai/rubrix-materials/raw/main/tutorials/snorkel/snorkel2.png \"Rubrix metadata\")\n",
    "\n",
    "Yes, it seems using \"money\" has some correlation with SPAM and a overlaps with \"check\" but still covers other data points (as we can see in the Keywords component).\n",
    "\n",
    "\n",
    "Let's add this new LF to see its effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1e0ebe5-8f04-4020-b6c0-9f35fe4c90ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def money(x):\n",
    "    return SPAM if \"money\" in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3ee1e03-ac3f-409f-bcd7-474b5b958fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:00<00:00, 3540.46it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:00<00:00, 4887.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Model Accuracy:     86.8%\n"
     ]
    }
   ],
   "source": [
    "label_model = test_label_model(original_labelling_functions + [money])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca8c71-2731-4510-b310-a2bb2032f044",
   "metadata": {},
   "source": [
    "Yes! With just some quick exploration we've improved the accuracy of the Label Model by `1.2`%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-enhancement",
   "metadata": {},
   "source": [
    "### Exploring and improving heuristic LFs\n",
    "\n",
    "We've already seen how to use keywords to label our data, the next step would be to use heuristics to do the labeling. \n",
    "\n",
    "A simple approach proposed in the original Snorkel tutorial is checking the length of the comments' text, considering it SPAM if its length is lower than a threshold. \n",
    "\n",
    "To find a suitable threshold we can use Rubrix to visually explore the messages, similar to what we did before with the author selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "antique-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "records= []\n",
    "\n",
    "for index, record in df_train.iterrows():     \n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=index,\n",
    "        inputs=record[\"text\"],\n",
    "        metadata = {\n",
    "            \"textlen\": str(len(record.text.split())), # N¬∫ of 'words' in the sample\n",
    "        }\n",
    "    )\n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "expensive-symbol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_spam_snorkel_heuristic', processed=1586, failed=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_spam_snorkel_heuristic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-ghana",
   "metadata": {},
   "source": [
    "In the original tutorial, a threshold of 5 words is used, by exploring in Rubrix, we see we can go above that threshold. Let's try with 20 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "still-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def short_comment_2(x):\n",
    "    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n",
    "    return HAM if len(x.text.split()) < 20 else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e4f8d5e-72a6-4ba2-942f-33dc4fb001d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelingFunction short_comment, Preprocessors: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's replace the original short comment function\n",
    "original_labelling_functions[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7edbd271-a61f-4102-93d9-500abb8c63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labelling_functions[6] = short_comment_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6271dd3b-3bc7-436a-832b-8267c643faee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:00<00:00, 5388.84it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:00<00:00, 5542.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Model Accuracy:     90.8%\n"
     ]
    }
   ],
   "source": [
    "label_model = test_label_model(original_labelling_functions + [money])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed1a59-eb4a-47a1-bdf9-65216acc137c",
   "metadata": {},
   "source": [
    "Yes! With some additional exploration we've improved the accuracy of the Label Model by `5.2`%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb34f9c0-13dd-4509-9c0c-37bfbcefe981",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_lfs = original_labelling_functions + [money]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-biology",
   "metadata": {},
   "source": [
    "### Exploring third-party models LFs with Rubrix\n",
    "\n",
    "Another class of Snorkel LFs are those third-party models, which can be combined with the Label Model.\n",
    "\n",
    "Rubrix can be used for exploring how these models work with unlabelled data in order to define more precise LFs.\n",
    "\n",
    "Let's see this with the original Textblob's based labelling functions.\n",
    "\n",
    "\n",
    "#### Textblob\n",
    "\n",
    "Let's explore Textblob predictions on the training set with Rubrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baking-wallet",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "records= []\n",
    "for index, record in df_train.iterrows():   \n",
    "    scores = TextBlob(record[\"text\"])\n",
    "    item = rb.TextClassificationRecord(\n",
    "        id=str(index),\n",
    "        inputs=record[\"text\"],\n",
    "        multi_label= False,\n",
    "        prediction=[(\"subjectivity\", max(0.0, scores.sentiment.subjectivity))],\n",
    "        prediction_agent=\"TextBlob\",\n",
    "        metadata = {\n",
    "            \"author\": record.author,\n",
    "            \"video\": str(record.video)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "derived-springfield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_spam_snorkel_textblob', processed=1586, failed=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_spam_snorkel_textblob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-discount",
   "metadata": {},
   "source": [
    "Checking the dataset, we can filter our data based on the prediction score of our classifier. This can help us since the predictions of our TextBlob tend to be SPAM the lower the subjectivity is. We can take advantage of this and filter the predictions by their score:\n",
    "\n",
    "\n",
    "![Snorkel explore](img/snorkel_4.png \"Rubrix metadata\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d9935d-2eab-44af-ab59-991c078cd627",
   "metadata": {},
   "source": [
    "## 3. Checking and curating programatically created data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c7d934-1310-4208-8344-61dcbe0a9eb2",
   "metadata": {},
   "source": [
    "In this section, we're going to analyse the training set we're able to generate using our data programming model (the Label Model).\n",
    "\n",
    "First thing, we need to do is to remove the unlabeled data. Remember we're only labeling a subset using our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "applier = PandasLFApplier(lfs=current_lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_test = applier.apply(df=df_test)\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_train,\n",
    "    y=label_model.predict_proba(L_train), # Probabilities of each data point for each class\n",
    "    L=L_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-spice",
   "metadata": {},
   "source": [
    "Now that we have our data, we can explore the results in Rubrix and manually relabel those cases that have been wrongly classified or keep exploring the performance of our LFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "floating-alignment",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for i, (index, record) in enumerate(df_train_filtered.iterrows()):   \n",
    "    item = rb.TextClassificationRecord(\n",
    "        inputs=record[\"text\"],\n",
    "        # our scores come from probs_train_filtered\n",
    "        # probs_train_filtered[i][j] is the probability the sample i belongs to class j\n",
    "        prediction=[(\"HAM\", probs_train_filtered[i][0]),   # 0 for HAM\n",
    "                    (\"SPAM\", probs_train_filtered[i][1])], # 1 for SPAM\n",
    "        prediction_agent=\"LabelModel\",\n",
    "    )\n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "choice-circle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_filtered_classified_sample_2', processed=1568, failed=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_filtered_classified_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-modem",
   "metadata": {},
   "source": [
    "With this Rubrix Dataset, we can explore the predictions of our label model. We could add the label model output as `annotations` to create a training set and share it subject matter experts for review e.g., for relabelling problematic data points. \n",
    "\n",
    "To do this, simply adding the max. probability class as `annotation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97099315-d567-42be-a058-1a44769596aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for i, (index, record) in enumerate(df_train_filtered.iterrows()):\n",
    "    gold_label = \"SPAM\" if probs_train_filtered[i][1] > probs_train_filtered[i][0] else \"HAM\"\n",
    "    item = rb.TextClassificationRecord(\n",
    "        inputs=record[\"text\"],\n",
    "        # our scores come from probs_train_filtered\n",
    "        # probs_train_filtered[i][j] is the probability the sample i belongs to class j\n",
    "        prediction=[(\"HAM\", probs_train_filtered[i][0]),   # 0 for HAM\n",
    "                    (\"SPAM\", probs_train_filtered[i][1])], # 1 for SPAM\n",
    "        prediction_agent=\"LabelModel\",\n",
    "        annotation=[gold_label]\n",
    "    )\n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69de2fed-70ed-4379-8fd1-32bfcac4669b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_filtered_classified_sample_with_annotation', processed=1568, failed=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_filtered_classified_sample_with_annotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21e3e8-5589-4b3a-9ec5-f25ba2d5d9a7",
   "metadata": {},
   "source": [
    "Using the [Annotation mode](https://docs.rubrix.ml/en/latest/reference/rubrix_webapp_reference.html#annotation-mode), you and other users could review the labels proposed by the Snorkel model and refine the training set, with a similar exploration pattern as we used for defining LFs.\n",
    "\n",
    "\n",
    "![Snorkel explore](https://github.com/recognai/rubrix-materials/raw/main/tutorials/snorkel/snorkel3.png \"Rubrix annotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-liver",
   "metadata": {},
   "source": [
    "## 4. Training and evaluating a classifier\n",
    "\n",
    "The next thing we can do with our data is training a classifier using some of the most popular libraries such as Scikit-learn, Tensorflow or Pytorch. For simplicity, we will use scikit-learn, a widely-used library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "falling-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 5)) # Bag Of Words (BoW) with n-grams\n",
    "X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\n",
    "X_test = vectorizer.transform(df_test.text.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-netscape",
   "metadata": {},
   "source": [
    "Since we need to tell the model the class for each sample, and we have probabilities, we can assign to each sample the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "painful-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-bouquet",
   "metadata": {},
   "source": [
    "And then build the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Y_test = df_test.label.values\n",
    "\n",
    "sklearn_model = LogisticRegression(C=1e3, solver=\"liblinear\")\n",
    "sklearn_model.fit(X=X_train, y=preds_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "personal-fountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.6%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {sklearn_model.score(X=X_test, y=Y_test) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-acquisition",
   "metadata": {},
   "source": [
    "Let's explore how our new model performs on the test data, in this case the annotation comes from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "hybrid-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for index, record in df_test.iterrows(): \n",
    "    preds = sklearn_model.predict_proba(vectorizer.transform([record[\"text\"]]))\n",
    "    preds = preds[0]\n",
    "    item = rb.TextClassificationRecord(\n",
    "        inputs=record[\"text\"],\n",
    "        prediction=[(\"HAM\", preds[0]),   # 0 for HAM\n",
    "                    (\"SPAM\", preds[1])], # 1 for SPAM\n",
    "        prediction_agent=\"MyModel\",\n",
    "        annotation=[\"SPAM\" if record.label == 1 else \"HAM\"]\n",
    "    )\n",
    "    records.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "physical-latter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='yt_my_model_test', processed=250, failed=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.log(records=records, name=\"yt_my_model_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50eaf6a-3cd4-4cbb-af90-122b8acc9e81",
   "metadata": {},
   "source": [
    "This exploration is useful for error analysis and debugging, for example we can check all incorrectly classified examples using the Prediction filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14525d-05a6-4d32-beff-8a936eb8e277",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this tutorial, we have learnt to use Snorkel in combination with Rubrix for data programming workflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae86a798-d0f5-4446-84b1-d653bf75ee8b",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "### üìö [Rubrix documentation](https://docs.rubrix.ml) for more guides and tutorials.\n",
    "\n",
    "### üôã‚Äç‚ôÄÔ∏è Join the Rubrix community! A good place to start is the [discussion forum](https://github.com/recognai/rubrix/discussions).\n",
    "\n",
    "### ‚≠ê Rubrix [Github repo](https://github.com/recognai/rubrix) to stay updated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}