{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e548eacd",
   "metadata": {},
   "source": [
    "# FlyingSquid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a75ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "dev = dataset['validation']\n",
    "\n",
    "records = [\n",
    "    rb.TextClassificationRecord(\n",
    "        inputs=text\n",
    "    )\n",
    "    for text in train['sentence']\n",
    "]\n",
    "\n",
    "records += [\n",
    "    rb.TextClassificationRecord(\n",
    "        inputs=text,\n",
    "        annotation=dev.features['label'].names[label]\n",
    "    )\n",
    "    for text, label in zip(dev['sentence'], dev['label'])\n",
    "]\n",
    "\n",
    "\n",
    "rb.log(records, name=\"weak_supervision_sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eafae505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.8/site-packages/jose/backends/cryptography_backend.py:18: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes, int_to_bytes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b2bdc907df4917bd2216c48b252699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing rules:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c7bd4a7a084d2a8d7620dd7ea209e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying rules:   0%|          | 0/68221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/rubrix/lib/python3.8/site-packages/pgmpy/models/MarkovModel.py:8: FutureWarning: MarkovModel has been renamed to MarkovNetwork. Please use MarkovNetwork class, MarkovModel will be removed in future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'efficacy': 0.4733926941735914,\n",
       " 'fscore_cautious': 0.33672608742524845,\n",
       " 'coverage': 0.2190366972477064,\n",
       " 'negative': {'precision': 0.9038461538461539,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.6438356164383562,\n",
       "  'support': 94},\n",
       " 'positive': {'precision': 0.6618705035971223,\n",
       "  'recall': 0.9484536082474226,\n",
       "  'f1-score': 0.7796610169491525,\n",
       "  'support': 97},\n",
       " 'accuracy': 0.7277486910994765,\n",
       " 'macro avg': {'precision': 0.7828583287216381,\n",
       "  'recall': 0.7242268041237113,\n",
       "  'f1-score': 0.7117483166937544,\n",
       "  'support': 191},\n",
       " 'weighted avg': {'precision': 0.7809579963898394,\n",
       "  'recall': 0.7277486910994765,\n",
       "  'f1-score': 0.7128150083208026,\n",
       "  'support': 191}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rubrix.labeling.text_classification import Rule, WeakLabels\n",
    "import rubrix as rb\n",
    "\n",
    "positive_keywords = [\n",
    "    \"funny\", \"comedy\", \"love\",\n",
    "    \"fun\", \"entertaining\", \"romantic\",\n",
    "    \"compelling\", \"worth\", \"sweet\",\n",
    "    \"fascinating\", \"laughs\", \"comic\",\n",
    "    \"enjoyable\", \"clever\", \"perfect\",\n",
    "    \"beautiful\", \"amusing\", \"powerful\",\n",
    "    \"charming\", \"engaging\", \n",
    "]\n",
    "\n",
    "negative_keywords = [\n",
    "    \"bad\", \"dull\", \"worst\", \"worse\",\n",
    "    \"spiritless\", \"silly\", \"monotonous\", \n",
    "    \"terrible\", \"banal\", \"unimaginative\", \n",
    "    \"inane\", \"shallow\", \"offensive\", \n",
    "    \"redundant\", \"lazy\", \"loose\", \n",
    "    \"poorly\", \"awful\", \"pathetic\", \n",
    "    \"lousy\", \"inept\"\n",
    "]\n",
    "\n",
    "rules = [ Rule(query=keyword, label=\"positive\") for keyword in positive_keywords ]\n",
    "rules += [ Rule(query=keyword, label=\"negative\") for keyword in negative_keywords ]\n",
    "\n",
    "from rubrix.labeling.text_classification import load_rules\n",
    "\n",
    "# optionally add the rules defined in the web app UI\n",
    "rules += load_rules(dataset=\"weak_supervision_sst2\")\n",
    "\n",
    "# apply the rules to a dataset to obtain the weak labels\n",
    "weak_labels = WeakLabels(\n",
    "    rules=rules,\n",
    "    dataset=\"weak_supervision_sst2\"\n",
    ")\n",
    "\n",
    "from rubrix.labeling.text_classification import FlyingSquid\n",
    "\n",
    "# we pass our WeakLabels instance to our FlyingSquid label model\n",
    "flyingsquid_model = FlyingSquid(weak_labels)\n",
    "\n",
    "# we fit the model\n",
    "flyingsquid_model.fit()\n",
    "\n",
    "flyingsquid_model.score(tie_break_policy=\"abstain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51488a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision recall f1-score support\n",
      "efficacy             0.47   0.47     0.47    0.47\n",
      "fscore_cautious      0.34   0.34     0.34    0.34\n",
      "coverage             0.22   0.22     0.22    0.22\n",
      "negative             0.90   0.50     0.64      94\n",
      "positive             0.66   0.95     0.78      97\n",
      "accuracy                             0.73     191\n",
      "macro avg            0.78   0.72     0.71     191\n",
      "weighted avg         0.78   0.73     0.71     191\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "report = flyingsquid_model.score(tie_break_policy=\"abstain\", output_str=False)\n",
    "report.update({\"accuracy\": {\"precision\": None, \"recall\": None, \"f1-score\": report[\"accuracy\"], \"support\": report['macro avg']['support']}})\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df = df.astype(float).applymap(lambda x: '{:,.2f}'.format(x))\n",
    "df['support'] = df['support'].astype(float).apply(lambda x: '{:,g}'.format(x))\n",
    "df = df.replace(['nan', 'None'], '')\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386348e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.50      0.64        94\n",
      "    positive       0.66      0.95      0.78        97\n",
      "\n",
      "    coverage                           0.22       191\n",
      "    fscore_cautious                           0.34       191\n",
      "    efficacy                           0.47       191\n",
      "    accuracy                           0.73       191\n",
      "   macro avg       0.78      0.72      0.71       191\n",
      "weighted avg       0.78      0.73      0.71       191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(flyingsquid_model.score(tie_break_policy=\"abstain\", output_str=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a48ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_row_pattern = re.compile(r'(\\s+)(\\w*)(\\s+)(0\\.[0-9][0-9])(.*)')\n",
    "\n",
    "def get_metrics_row(row, metric, value):\n",
    "    f_string = f'(\\\\1){metric}(\\\\3){value}(\\\\5)'\n",
    "    print(f_string)\n",
    "    res = accuracy_row_pattern.search(row)\n",
    "    first_half = [res.group(1), res.group(2)]\n",
    "    spaces = res.group(3)\n",
    "    second_half = [res.group(4), res.group(5)]\n",
    "    first_half[1] = metric\n",
    "    second_half[0] = value\n",
    "    first_half = \" \".join(first_half)\n",
    "    second_half = \" \".join(second_half)\n",
    "    diff = len(first_half + spaces + second_half) - len(first_half + second_half)\n",
    "    if diff:\n",
    "        output = first_half + \" \" * diff + second_half\n",
    "    else:\n",
    "        output = first_half + \" \" + second_half\n",
    "    return output\n",
    "\n",
    "get_metrics_row(lines[idx], \"efficacy\", \"0.87\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90381e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your training records with the predictions of the label model\n",
    "records_for_training = flyingsquid_model.predict()\n",
    "\n",
    "# log the records to a new dataset in Rubrix\n",
    "rb.log(records_for_training, name=\"flyingsquid_results\")\n",
    "\n",
    "# extract training data\n",
    "training_data = pd.DataFrame(\n",
    "    [\n",
    "        {\"text\": rec.inputs[\"text\"], \"label\": flyingsquid_model.weak_labels.label2int[rec.prediction[0][0]]}\n",
    "        for rec in records_for_training\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ce8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoxy.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ac06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_max, is_tie, prediction, annotation = flyingsquid_model._get_score_objects(verbose=True)\n",
    "\n",
    "coverage = len(prediction[~is_tie]) / len(prediction)\n",
    "\n",
    "coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e75c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define our final classifier\n",
    "classifier = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# fit the classifier\n",
    "classifier.fit(\n",
    "    X=training_data.text.tolist(),\n",
    "    y=training_data.label.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7294c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the test accuracy\n",
    "accuracy = classifier.score(\n",
    "    X=test_df.text.tolist(),\n",
    "    y=test_df.label.tolist()\n",
    ")\n",
    "\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad5d16",
   "metadata": {},
   "source": [
    "# Epoxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94408422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SentenceTransformerModel(object):\n",
    "    \n",
    "    def __init__(self, embedding_model_name):\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "    \n",
    "    def __call__(self, records):\n",
    "        texts = [ x.inputs[\"text\"] for x in records ]\n",
    "        embeddings = self.embedding_model.encode(texts)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubrix.labeling.text_classification import Epoxy\n",
    "embedding_model_name = \"average_word_embeddings_glove.840B.300d\"\n",
    "model = SentenceTransformerModel(embedding_model_name)\n",
    "\n",
    "embeddings = model(weak_labels.records())\n",
    "\n",
    "epoxy = Epoxy(weak_labels, embeddings)\n",
    "epoxy.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoxy._thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your training records with the predictions of the label model\n",
    "records_for_training = epoxy.predict()\n",
    "\n",
    "# log the records to a new dataset in Rubrix\n",
    "rb.log(records_for_training, name=\"epoxy_results_2\")\n",
    "\n",
    "# extract training data\n",
    "training_data = pd.DataFrame(\n",
    "    [\n",
    "        {\"text\": rec.inputs[\"text\"], \"label\": epoxy.weak_labels.label2int[rec.prediction[0][0]]}\n",
    "        for rec in records_for_training\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae96f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview training data\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0391a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define our final classifier\n",
    "classifier = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# fit the classifier\n",
    "classifier.fit(\n",
    "    X=training_data.text.tolist(),\n",
    "    y=training_data.label.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feacb426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the test accuracy\n",
    "accuracy = classifier.score(\n",
    "    X=test_df.text.tolist(),\n",
    "    y=test_df.label.tolist()\n",
    ")\n",
    "\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df00f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rubrix",
   "language": "python",
   "name": "rubrix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
