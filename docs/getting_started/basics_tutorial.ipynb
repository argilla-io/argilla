{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5393fab0-2b6c-40d7-9fc5-419343e4ba26",
   "metadata": {},
   "source": [
    "# Rubrix Basics\n",
    "\n",
    "Here you will find some basic guidelines on how to get started with Rubrix.\n",
    "\n",
    "## How to upload datasets\n",
    "\n",
    "In **Rubrix**, a dataset is a [collection of records](https://rubrix.readthedocs.io/en/stable/reference/webapp/dataset.html), each one containing an input text. \n",
    "\n",
    "This \"collection of records\" can be different depending on the the **task** to be performed **(Text, Token Classification and Text2Text)**, and might contain features such as:\n",
    "\n",
    "- Annotations (the labels for each element of a dataset),\n",
    "- Predictions (the results obtained when a model is applied to a dataset), and/or\n",
    "- Metadata (reference data to identify elements on a dataset). \n",
    "\n",
    "---\n",
    "\n",
    "First of all, you should understand how Rubrix works. Rubrix's working units are **records**, which are basically texts. \n",
    "\n",
    "These texts are part of the aforementioned **datasets**, and are usually in any format **(.CSV, JSON, HuggingFace datasets, XML...)**. To perform any kind of task in any format, there are different ways to upload these datasets, as we will see further on. Besides, Rubrix is **compatible** with most of NLP libraries, so the process is even easier.\n",
    "\n",
    "Let's see how you can upload a dataset to start working with **Rubrix**. After this, you can explore or annotate datasets, apply weak supervision rules, obtain predictions or even training a model. \n",
    "\n",
    "This is a very easy example. As you see, a **Text Classification record** is created from a sentence and logged into Rubrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866426c8-b3af-4307-a3eb-3d50171e4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "\n",
    "#this record consists of one simple sentence\n",
    "record = rb.TextClassificationRecord(text=\"hello world, this is me\")\n",
    "\n",
    "#logging the record into rubrix. This will be a Text Classification task.\n",
    "rb.log(record, \"my_first_record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47ac105-78ba-4211-a29b-496e88797376",
   "metadata": {},
   "source": [
    "![image](../_static/reference/getting_started/first_record.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3fb22a-e24c-418a-83ea-c4ceb07f26df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text classification\n",
    "\n",
    "These tasks focus on categorizing sentences or documents into one or more groups. When we only deal with a category, it is **single-label text classification**, but when we deal with more than one, then we are talking about **multi-label text classification**. In addition to deal with different tasks, **Rubrix** also provides some interesting features, like the **Define rules mode** or the available **metrics** (see next section).\n",
    "\n",
    "In this example, the chosen [dataset](https://www.kaggle.com/datasets/databar/10k-snapchat-reviews) contains 10K reviews about the Snapchat app from App Store. This dataset (available for download) could be used for tasks such as **sentiment analysis**, or **text categorization**. \n",
    "\n",
    "After retrieving the dataset from Kaggle and identifying the column that contains the **text input**, the dataset can be easily uploaded. After this, **100 records** will be available in the **Rubrix UI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ae148b-4d91-49ef-a7d1-6073ce8f2077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>userName</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>isEdited</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Savvanananahhh</td>\n",
       "      <td>4</td>\n",
       "      <td>For the most part I quite enjoy Snapchat it’s ...</td>\n",
       "      <td>False</td>\n",
       "      <td>10/4/20 6:01</td>\n",
       "      <td>Performance issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Idek 9-101112</td>\n",
       "      <td>3</td>\n",
       "      <td>I’m sorry to say it, but something is definite...</td>\n",
       "      <td>False</td>\n",
       "      <td>10/14/20 2:13</td>\n",
       "      <td>What happened?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>William Quintana</td>\n",
       "      <td>3</td>\n",
       "      <td>Snapchat update ruined my story organization! ...</td>\n",
       "      <td>False</td>\n",
       "      <td>7/31/20 19:54</td>\n",
       "      <td>STORY ORGANIZATION RUINED!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          userName  rating  \\\n",
       "0           0    Savvanananahhh       4   \n",
       "1           1     Idek 9-101112       3   \n",
       "2           2  William Quintana       3   \n",
       "\n",
       "                                              review  isEdited           date  \\\n",
       "0  For the most part I quite enjoy Snapchat it’s ...     False   10/4/20 6:01   \n",
       "1  I’m sorry to say it, but something is definite...     False  10/14/20 2:13   \n",
       "2  Snapchat update ruined my story organization! ...     False  7/31/20 19:54   \n",
       "\n",
       "                        title  \n",
       "0          Performance issues  \n",
       "1              What happened?  \n",
       "2  STORY ORGANIZATION RUINED!  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import rubrix as rb\n",
    "\n",
    "#converting the CSV file into a Pandas Dataframe. This dataset has been limited to 100 results.\n",
    "dataset_txt = pd.read_csv(\"snapchat.csv\")[:100] #probar URL \n",
    "\n",
    "dataset_txt.head(3) #displaying the dataframe to see the first three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe9946-9e0f-4be9-ade2-9884d9bda998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the column related to the text input\n",
    "data = dataset_txt.rename(columns={\"review\": \"text\"}) \n",
    "#to be processed with the rb.read_pandas function, the text column must be named with the same name\n",
    "\n",
    "#rubrix is able to read the dataframe and to identify the columns\n",
    "record_txt = rb.read_pandas(data, task=\"TextClassification\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c0cb7-f129-45e7-8784-88908d882104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging the records\n",
    "rb.log(record_txt, \"snapchat_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59944e44-4202-4890-9a45-f99fc3fb2dd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Token Classification\n",
    "\n",
    "The aim of **Token Classification tasks** is to divide the text into **tokens** to put them **labels**. This process is called **tokenize**, and consists of dividing the text into tokens, which are **units of text**. Rubrix can handle different **token classification tasks**, being **Named Entity Recognition (NER)** one of the most remarkable, as its UI is particularly useful for this purpose.\n",
    "\n",
    "This example shows how to tokenize the **input text** from this [Kaggle dataset](https://www.kaggle.com/datasets/mldado/german-online-reviewsratings-of-organic-coffee), which contains reviews of organic coffee in German. After this tokenization, the dataset is ready to be uploaded.\n",
    "\n",
    "In this case, the **tokenization** has been made with **spaCy**- however, there are other libraries such as  [NLTK](https://www.nltk.org/) or [HuggingFace](https://huggingface.co/docs/transformers/main_classes/tokenizer) that also work for this process. The most important thing is to obtain a **tokenized text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502febcb-26f1-4832-8218-4f029ebed697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>brand</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Wenn ich Bohnenkaffee trinke (auf Arbeit trink...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Für mich ist dieser Kaffee ideal. Die Grundvor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Ich persönlich bin insbesondere von dem Geschm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        brand  rating  \\\n",
       "0           0  GEPA Kaffee       5   \n",
       "1           1  GEPA Kaffee       5   \n",
       "2           2  GEPA Kaffee       5   \n",
       "\n",
       "                                              review  \n",
       "0  Wenn ich Bohnenkaffee trinke (auf Arbeit trink...  \n",
       "1  Für mich ist dieser Kaffee ideal. Die Grundvor...  \n",
       "2  Ich persönlich bin insbesondere von dem Geschm...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rubrix as rb\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "dataset_tok = pd.read_csv(\"kaffee_reviews.csv\")[:50] \n",
    "\n",
    "dataset_tok.head(3) #displaying the dataset to see the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3705ea8d-9a4d-4bd2-9935-86304d6c21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is better to leave just the text column\n",
    "dataset_tok = dataset_tok.drop(['brand', 'rating'], axis=1) \n",
    "\n",
    "#renaming the text column in order to upload the dataset to Rubrix\n",
    "dataset_tok = dataset_tok.rename(columns={\"review\": \"text\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a4664fe-0840-4768-b856-79bdbd1dc178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Wenn ich Bohnenkaffee trinke (auf Arbeit trink...</td>\n",
       "      <td>[Wenn, ich, Bohnenkaffee, trinke, (, auf, Arbe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  \\\n",
       "0           0  Wenn ich Bohnenkaffee trinke (auf Arbeit trink...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [Wenn, ich, Bohnenkaffee, trinke, (, auf, Arbe...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#loading a model for German language to tokenize the text\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "text = dataset_tok['text']\n",
    "\n",
    "#this function allows tokenizing the text\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        tokens.append(token.text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "#a column must be created to upload the tokenized text\n",
    "dataset_tok['tokens'] = dataset_tok['text'].apply(tokenize)\n",
    "\n",
    "dataset_tok[:1] #this example shows the text and its tokenized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ff337-7b12-48fc-b8a1-a829a7800d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rubrix is able to read the dataframe and identify the columns\n",
    "\n",
    "record_tok = rb.read_pandas(dataset_tok, task=\"TokenClassification\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee85f1-1a37-4850-9bda-c4e54aa5db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now record can be logged into Rubrix\n",
    "rb.log(record_tok, \"coffee-reviews_de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66683c8-9ed5-4ab9-9937-39eeac9ccab0",
   "metadata": {},
   "source": [
    "### Text2Text\n",
    "\n",
    "These tasks are, basically, **text generation tasks**. They normally require a **text input** to provide an **output**, which can be a translation or a summary, for instance. \n",
    "\n",
    "To generate new text we need a **text input**, so identifying the text in the dataset is key. As this example is made with a HuggingFace dataset, the process is slightly different from the previous ones. In this case, the text input will be retrieved thanks to the [map function](https://huggingface.co/docs/datasets/process#map).\n",
    "\n",
    "This [dataset](https://huggingface.co/datasets/europa_ecdc_tm), aimed for **Machine Translation tasks**, contains texts from the European Centre for Disease Prevention and Control (ECDC). Only the chosen **source language** (English) will be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbce85f-200e-4b54-9650-308395b81770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "#retrieving the dataset from HuggingFace, with its language configuration and the desired split\n",
    "dataset = load_dataset(\"europa_ecdc_tm\", 'en2fr', split=\"train[0:100]\")\n",
    "\n",
    "dataset.to_pandas().head(3) #converting the HF dataset into a dataframe to read its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27d93ad-86f0-4d6c-a31f-5cc1d55235a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will help the map function to retrieve the text input \n",
    "def extract_frphrase(example):\n",
    "    example['text'] = example['translation']['en'] #English as the source language\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c29b34-df79-4f37-930b-4e54c25a320e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454bdcef06784c1aa33bb7fd90e1ae8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Vaccination against hepatitis C is not yet available.',\n",
       " 'HIV infection',\n",
       " 'The human immunodeficiency virus (HIV) remains one of the most important communicable diseases in Europe.',\n",
       " 'It is an infection associated with serious disease, persistently high costs of treatment and care, significant number of deaths and shortened life expectancy.',\n",
       " 'HIV is a virus, which attacks the immune system and causes a lifelong severe illness with a long incubation period.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the map function shows the text input\n",
    "\n",
    "updated_dataset = dataset.map(extract_frphrase)\n",
    "updated_dataset['text'][:5] #displaying the first 5 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb102c-2c02-4368-9dec-a0c014273de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the read_datasets function (similar to read_pandas) is able to process the data\n",
    "ecdc_en = rb.read_datasets(updated_dataset, task=\"Text2Text\") \n",
    "\n",
    "#uploading the dataset\n",
    "rb.log(ecdc_en, \"ecdc_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61895f3-f7c9-44a7-8305-1b5f9af1ed20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How to annotate datasets\n",
    "\n",
    "When it comes to annotating records, **Rubrix** offers two ways to do this: manually in the UI, or via the client by uploading an annotated dataset. \n",
    "\n",
    "### UI annotation\n",
    "\n",
    "**Rubrix** allows users to **manually annotate** records through its intuitive UI. The annotation process is customized and varies depending on the task to be performed, and these annotations can be used to obtain predictions, and to train a model as well. \n",
    "\n",
    "Click [here](https://rubrix.readthedocs.io/en/master/reference/webapp/annotate_records.html#annotate-records) to learn more about **annotation** with Rubrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cadf8a-39b5-45ff-a905-be2f9045ba1e",
   "metadata": {},
   "source": [
    "### Text classification\n",
    "\n",
    "#### Manual annotation\n",
    "\n",
    "After uploading a dataset (plain or annotated) to **Rubrix**, users can use the UI to **manually annotate** records. \n",
    "\n",
    "Taking the previous Text Classification example, users could **create one or more labels** with the **\"Create label\"** button in order to manually annotate the dataset:\n",
    "\n",
    "![image](../_static/reference/getting_started/createlabel_text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c74ff9-0fa9-41f7-93d0-1f4718782654",
   "metadata": {},
   "source": [
    "When labels are ready, the dataset is ready to be annotated. There are features like **bulk annotation**, which can ease the workload:\n",
    "\n",
    "<video width=\"100%\" controls><source src=../_static/reference/getting_started/bulk_annotate1.mp4 type=\"video/mp4\"></video> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b04b82-534a-48a1-b0ac-608ab7dbc2d2",
   "metadata": {},
   "source": [
    "#### Rules in UI\n",
    "\n",
    "_Click [here](https://rubrix.readthedocs.io/en/master/reference/webapp/define_rules.html) to read more about the Define rules mode._\n",
    "\n",
    "The **Define rules mode** is also a good method to quickly annotate these kind of datasets with noisy labels, as it is a \"semiautomatic\" system. These rules apply a specific set of labels to the records that match a given query. Besides, some **metrics** will be available for the created rule.\n",
    "\n",
    "This is an easy example. The chosen dataset, available in [Kaggle](https://www.kaggle.com/datasets/ishantjuyal/emotions-in-text), is an annotated dataset for **multilabel text classification**, which deals with texts and different emotions.\n",
    "\n",
    "<video width=\"100%\" controls><source src=../_static/reference/getting_started/define_rules.mp4 type=\"video/mp4\"></video> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379b2b70-5564-434b-85b1-a377214f02ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Weak supervision\n",
    "\n",
    "_Click [here](https://rubrix.readthedocs.io/en/master/guides/weak-supervision.html) to read more about Weak Supervision with Rubrix._\n",
    "\n",
    "This feature is related to the **Define rules mode**. When saving one or more rules, it is possible to see the information by clicking on the **Manage rules** button.\n",
    "\n",
    "![image](../_static/reference/getting_started/managerules_text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f671d0ff-6bf9-4cec-84c9-f3463c660aa8",
   "metadata": {},
   "source": [
    "By using this feature, it is possible to **annotate** in a rapid, efficient way and to easily obtain information about the labels. Besides, these rules give **accurate** information about the labels efficiency, their coverage or correctness, to name a few features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462f965-e160-43fc-ad78-9f21a82c4b70",
   "metadata": {},
   "source": [
    "### Token classification\n",
    "\n",
    "#### Manual annotation\n",
    "\n",
    "When dealing with **Token classification** datasets, the manual annotation is particularly interesting. It is possible not only to **bulk annotate** or create new labels, but the UI is also useful to easily annotate, as the video shows.\n",
    "\n",
    "In case users want to annotate one or more words, they can highlight the word or sequence and annotate it by using the available labels or creating a new one. After this, the annotation will be marked as such and it will be registered at the **Metrics sidebar**:\n",
    "\n",
    "<video width=\"100%\" controls><source src=../_static/reference/getting_started/ner_annotation.mp4 type=\"video/mp4\"></video> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82666c3-9b3c-449d-925a-c99bb1797ff6",
   "metadata": {},
   "source": [
    "the **Metrics sidebar** can also provide good insights into the data:\n",
    "\n",
    "![image](../_static/reference/getting_started/token_annotation.png)\n",
    "\n",
    "Note that these features are available for both annotated and plain datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6797ca1-6fdd-4684-89f5-49d67430f29b",
   "metadata": {},
   "source": [
    "### Text2Text\n",
    "\n",
    "#### Manual annotation\n",
    "\n",
    "Of course, it is also possible to annotate **preannotated** or **plain** datasets for **Text2Text** tasks. In this case, the **annotation** is inside a text box that can be modified.\n",
    "\n",
    "This example, a **machine learning** task, has been made with the previous dataset for the **Text2Text** example. Now, the **target language** has also been uploaded as the annotation and can be modified:\n",
    "\n",
    "<video width=\"100%\" controls><source src=../_static/reference/getting_started/text2text.mp4 type=\"video/mp4\"></video> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6361a-6a76-4af9-b869-ffa501dd0d92",
   "metadata": {},
   "source": [
    "As this is the previous **Text2Text example**, we can simply add the column which corresponds to the **target language** in the `map function` and reupload the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b72a4-e082-49ec-9d32-035269aa4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we add the column corresponding to the text (source language) and the annotations (target language)\n",
    "def extract_phrase(example):\n",
    "    example['text'] = example['translation']['en']\n",
    "    example['annotation'] = example['translation']['fr']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c61325-3751-4655-8573-1e28192016b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset= dataset.map(extract_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2594e3d-d217-4981-925c-64597bc8cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "\n",
    "#now the dataset will contain both columns (text and annotations)\n",
    "ecdc_en_fr = rb.read_datasets(updated_dataset, task=\"Text2Text\") \n",
    "\n",
    "#uploading the dataset\n",
    "rb.log(ecdc_en_fr, \"ecdc_en_fr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
