{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5393fab0-2b6c-40d7-9fc5-419343e4ba26",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d13ee2-ffeb-46fa-9c62-d77c8328e499",
   "metadata": {},
   "source": [
    "Here you will find some basic guidelines on how to get started with Rubrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0038d1-86b1-4eb9-ada4-ae561ad25aa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How to upload datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f206d4-f70b-4fb1-8d9a-b1eee2fcd74e",
   "metadata": {},
   "source": [
    "In **Rubrix**, a dataset is a [collection of records](../reference/webapp/dataset.md), each one containing an input text. \n",
    "\n",
    "This \"collection of records\" can be different depending on the the **task** to be performed **(Text, Token Classification and Text2Text)**, and contain features such as:\n",
    "\n",
    "- Annotations (the labels for each element of a dataset),\n",
    "- Predictions (the results obtained when a model is applied to a dataset),\n",
    "- Metadata (reference data to identify elements on a dataset). \n",
    "\n",
    "---\n",
    "\n",
    "First of all, you should understand how Rubrix works. Rubrix's working units are **records**, which are basically texts. \n",
    "\n",
    "These texts are part of the aforementioned **datasets**, and are usually in any format **(.CSV, JSON, HuggingFace datasets, XML...)**. \n",
    "To perform any kind of task in any format, there are different ways to upload these datasets, as we will see further on. \n",
    "Besides, Rubrix is **compatible** with most of NLP libraries, so the process is even easier.\n",
    "\n",
    "Let's see how you can upload a dataset to start working with **Rubrix**. \n",
    "After this, you can explore or annotate datasets, apply weak supervision rules, obtain predictions or even training a model. \n",
    "\n",
    "This is a very easy example. \n",
    "As you see, a **Text Classification record** is created from a sentence and logged into Rubrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866426c8-b3af-4307-a3eb-3d50171e4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "\n",
    "# This record consists of one simple sentence\n",
    "record = rb.TextClassificationRecord(text=\"hello world, this is me\")\n",
    "\n",
    "# Logging the record into rubrix.\n",
    "rb.log(record, \"my_first_record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47ac105-78ba-4211-a29b-496e88797376",
   "metadata": {},
   "source": [
    "![image](../_static/getting_started/first_record.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3fb22a-e24c-418a-83ea-c4ceb07f26df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text classification\n",
    "\n",
    "These tasks focus on categorizing sentences or documents into one or more groups. When we only deal with a category, it is **single-label text classification**, but when we deal with more than one, then we are talking about **multi-label text classification**. In addition to deal with different tasks, **Rubrix** also provides some interesting features, like the **Define rules mode** or the available **metrics** (see next section).\n",
    "\n",
    "In this example, the chosen [dataset](https://www.kaggle.com/datasets/databar/10k-snapchat-reviews) contains 10K reviews about the Snapchat app from App Store. This dataset (available for download) could be used for tasks such as **sentiment analysis**, or **text categorization**. \n",
    "\n",
    "After retrieving the dataset from Kaggle and identifying the column that contains the **text input**, the dataset can be easily uploaded. After this, **100 records** will be available in the **Rubrix UI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ae148b-4d91-49ef-a7d1-6073ce8f2077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>userName</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>isEdited</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Savvanananahhh</td>\n",
       "      <td>4</td>\n",
       "      <td>For the most part I quite enjoy Snapchat it’s ...</td>\n",
       "      <td>False</td>\n",
       "      <td>10/4/20 6:01</td>\n",
       "      <td>Performance issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Idek 9-101112</td>\n",
       "      <td>3</td>\n",
       "      <td>I’m sorry to say it, but something is definite...</td>\n",
       "      <td>False</td>\n",
       "      <td>10/14/20 2:13</td>\n",
       "      <td>What happened?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>William Quintana</td>\n",
       "      <td>3</td>\n",
       "      <td>Snapchat update ruined my story organization! ...</td>\n",
       "      <td>False</td>\n",
       "      <td>7/31/20 19:54</td>\n",
       "      <td>STORY ORGANIZATION RUINED!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          userName  rating  \\\n",
       "0           0    Savvanananahhh       4   \n",
       "1           1     Idek 9-101112       3   \n",
       "2           2  William Quintana       3   \n",
       "\n",
       "                                              review  isEdited           date  \\\n",
       "0  For the most part I quite enjoy Snapchat it’s ...     False   10/4/20 6:01   \n",
       "1  I’m sorry to say it, but something is definite...     False  10/14/20 2:13   \n",
       "2  Snapchat update ruined my story organization! ...     False  7/31/20 19:54   \n",
       "\n",
       "                        title  \n",
       "0          Performance issues  \n",
       "1              What happened?  \n",
       "2  STORY ORGANIZATION RUINED!  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import rubrix as rb\n",
    "\n",
    "#converting the CSV file into a Pandas Dataframe. This dataset has been limited to 100 results.\n",
    "dataset_txt = pd.read_csv(\"snapchat.csv\")[:100] #probar URL \n",
    "\n",
    "dataset_txt.head(3) #displaying the dataframe to see the first three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe9946-9e0f-4be9-ade2-9884d9bda998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the column related to the text input\n",
    "data = dataset_txt.rename(columns={\"review\": \"text\"}) \n",
    "#to be processed with the rb.read_pandas function, the text column must be named with the same name\n",
    "\n",
    "#rubrix is able to read the dataframe and to identify the columns\n",
    "record_txt = rb.read_pandas(data, task=\"TextClassification\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c0cb7-f129-45e7-8784-88908d882104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging the records\n",
    "rb.log(record_txt, \"snapchat_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59944e44-4202-4890-9a45-f99fc3fb2dd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Token Classification\n",
    "\n",
    "The aim of **Token Classification tasks** is to divide the text into **tokens** to put them **labels**. This process is called **tokenize**, and consists of dividing the text into tokens, which are **units of text**. Rubrix can handle different **token classification tasks**, being **Named Entity Recognition (NER)** one of the most remarkable, as its UI is particularly useful for this purpose.\n",
    "\n",
    "This example shows how to tokenize the **input text** from this [Kaggle dataset](https://www.kaggle.com/datasets/mldado/german-online-reviewsratings-of-organic-coffee), which contains reviews of organic coffee in German. After this tokenization, the dataset is ready to be uploaded.\n",
    "\n",
    "In this case, the **tokenization** has been made with **spaCy**- however, there are other libraries such as  [NLTK](https://www.nltk.org/) or [HuggingFace](https://huggingface.co/docs/transformers/main_classes/tokenizer) that also work for this process. The most important thing is to obtain a **tokenized text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "502febcb-26f1-4832-8218-4f029ebed697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>brand</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Wenn ich Bohnenkaffee trinke (auf Arbeit trink...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Für mich ist dieser Kaffee ideal. Die Grundvor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Ich persönlich bin insbesondere von dem Geschm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        brand  rating  \\\n",
       "0           0  GEPA Kaffee       5   \n",
       "1           1  GEPA Kaffee       5   \n",
       "2           2  GEPA Kaffee       5   \n",
       "\n",
       "                                              review  \n",
       "0  Wenn ich Bohnenkaffee trinke (auf Arbeit trink...  \n",
       "1  Für mich ist dieser Kaffee ideal. Die Grundvor...  \n",
       "2  Ich persönlich bin insbesondere von dem Geschm...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "dataframe = pd.read_csv(\"kaffee_reviews.csv\")\n",
    "\n",
    "# Display the first three rows of the dataset\n",
    "dataframe.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d2f14-37a0-4a5a-8ae7-86f8e9304fa9",
   "metadata": {},
   "source": [
    "Since Rubrix expects the input text to be in a column named *\"text\"*, let us simply rename the *\"review\"* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3705ea8d-9a4d-4bd2-9935-86304d6c21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to use the review column as input text, so simply rename it\n",
    "dataframe = dataframe.rename(columns={\"review\": \"text\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a4664fe-0840-4768-b856-79bdbd1dc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a german spaCy model to tokenize our text\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Define our tokenize function and apply it to our text\n",
    "def tokenize(text):\n",
    "    return [token.text for token in nlp(text)]\n",
    "\n",
    "dataframe[\"tokens\"] = dataframe[\"text\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4ff337-7b12-48fc-b8a1-a829a7800d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 13:08:52.658 | WARNING  | rubrix.client.datasets:from_pandas:263 - Following columns are not supported by the TokenClassificationRecord model and are ignored: ['Unnamed: 0', 'brand', 'rating']\n"
     ]
    }
   ],
   "source": [
    "import rubrix as rb\n",
    "\n",
    "# Create a Rubrix dataset by reading a pandas DataFrame with required columns\n",
    "dataset = rb.read_pandas(dataframe, task=\"TokenClassification\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee85f1-1a37-4850-9bda-c4e54aa5db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the datset to the Rubrix web app\n",
    "rb.log(dataset, \"coffee-reviews_de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66683c8-9ed5-4ab9-9937-39eeac9ccab0",
   "metadata": {},
   "source": [
    "### Text2Text\n",
    "\n",
    "These tasks are, basically, **text generation tasks**. They normally require a **text input** to provide an **output**, which can be a translation or a summary, for instance. \n",
    "\n",
    "To generate new text we need a **text input**, so identifying the text in the dataset is key. As this example is made with a HuggingFace dataset, the process is slightly different from the previous ones. In this case, the text input will be retrieved thanks to the [map function](https://huggingface.co/docs/datasets/process#map).\n",
    "\n",
    "This [dataset](https://huggingface.co/datasets/europa_ecdc_tm), aimed for **Machine Translation tasks**, contains texts from the European Centre for Disease Prevention and Control (ECDC). Only the chosen **source language** (English) will be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbce85f-200e-4b54-9650-308395b81770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "#retrieving the dataset from HuggingFace, with its language configuration and the desired split\n",
    "dataset = load_dataset(\"europa_ecdc_tm\", 'en2fr', split=\"train[0:100]\")\n",
    "\n",
    "dataset.to_pandas().head(3) #converting the HF dataset into a dataframe to read its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27d93ad-86f0-4d6c-a31f-5cc1d55235a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will help the map function to retrieve the text input \n",
    "def extract_frphrase(example):\n",
    "    example['text'] = example['translation']['en'] #English as the source language\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c29b34-df79-4f37-930b-4e54c25a320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the map function shows the text input\n",
    "\n",
    "updated_dataset = dataset.map(extract_frphrase)\n",
    "updated_dataset['text'][:5] #displaying the first 5 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb102c-2c02-4368-9dec-a0c014273de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the read_datasets function (similar to read_pandas) is able to process the data\n",
    "ecdc_en = rb.read_datasets(updated_dataset, task=\"Text2Text\") \n",
    "\n",
    "#uploading the dataset\n",
    "rb.log(ecdc_en, \"ecdc_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3139dd1-a939-4baa-8d26-bd1214c8cbcd",
   "metadata": {},
   "source": [
    "## How to annotate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfeaf75-143e-4543-85f3-2a6e995dcf06",
   "metadata": {},
   "source": [
    "Rubrix provides several ways to annotate your data. \n",
    "With the intuitive Rubrix web app, you can choose between:\n",
    "\n",
    "1. Manually annotating each record using a dedicated interface for each task type;\n",
    "2. Leveraging a user-provided model by validating its predictions;\n",
    "3. Trying to define heuristic rules to produce \"noisy labels\", a technique also known as \"weak supervision\";\n",
    "\n",
    "Each way has its pros and cons, and the best match largely depends on your individual use case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bccc4-05c7-4c27-920b-5b76eb4acd22",
   "metadata": {},
   "source": [
    "### 1. Manual annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b085744-7c61-4614-b542-c4de2cea9181",
   "metadata": {},
   "source": [
    "The straightforward approach of manual annotations might be necessary if you do not have a suitable model for your use case or cannot come up with good heuristic rules for your dataset. \n",
    "It can also be a good approach if you dispose of a large annotation workforce or require few but unbiased and high-quality labels.\n",
    "\n",
    "Rubrix tries to make this relatively cumbersome approach as painless as possible. \n",
    "Via an intuitive and adaptive UI, its exhaustive search and filter functionalities, and bulk annotation capabilities, Rubrix turns the manual annotation process into an efficient option.  \n",
    "\n",
    "Look at our dedicated [feature reference](../reference/webapp/annotate_records.md) for a detailed and illustrative guide on manually annotating your dataset with Rubrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631840b-9cf7-45e6-9dc4-5f6b24cf0e8b",
   "metadata": {},
   "source": [
    "### 2. Validating predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c46840-0991-4fc0-bbb0-15df33ee242b",
   "metadata": {},
   "source": [
    "![Validate predictions for a token classification dataset](../_static/reference/webapp/annotation_ner.png)\n",
    "\n",
    "Nowadays, many pre-trained or zero-shot models are available online via model repositories like the Hugging Face Hub. \n",
    "Most of the time, you probably will find a model that already suits your specific dataset task to some degree. \n",
    "In Rubrix, you can pre-annotate your data by including predictions from these models in your records.\n",
    "Assuming that the model works reasonably well on your dataset, you can filter for records with high prediction scores and validate the predictions.\n",
    "In this way, you will rapidly annotate part of your data and alleviate the annotation process.\n",
    "\n",
    "One downside of this approach is that your annotations will be subject to the possible biases and mistakes of the pre-trained model.\n",
    "When guided by pre-trained models, it is common to see human annotators get influenced by them.\n",
    "Therefore, it is advisable to avoid pre-annotations when building a rigorous test set for the final model evaluation.\n",
    "\n",
    "Check the [introduction tutorial](../tutorials/01-labeling-finetuning.ipynb) to learn to add predictions to the records. \n",
    "And our [feature reference](../reference/webapp/annotate_records.md#validate-predictions) includes a detailed guide on validating predictions in the Rubrix web app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cbd593-e241-4f27-9a58-6932912ea9f1",
   "metadata": {},
   "source": [
    "### 3. Define rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7ea92-d40f-4d09-a0e8-16b7d7867e6e",
   "metadata": {},
   "source": [
    "![Defining a rule for a multi-label text classification task.](../_static/reference/webapp/define_rules_2.png)\n",
    "\n",
    "Another approach to annotating your data is to develop heuristic rules tailored to your dataset. \n",
    "For example, let us assume you want to classify news articles into the categories of *Finance*, *Sports*, and *Culture*. \n",
    "In this case, a reasonable rule would be to label all articles that include the word \"stock\" as *Finance*. \n",
    "\n",
    "It is easy to see how you can quickly annotate vast amounts of data in this way, which we refer to as *weak supervision*. \n",
    "Rules can get arbitrarily complex and can also include the record's metadata. \n",
    "The downsides of this approach are that it might be challenging to come up with working heuristic rules for some datasets. \n",
    "Furthermore, rules are rarely 100% precise and often conflict with each other, which must be addressed by so-called label models. \n",
    "It is usually a trade-off between the amount of annotated data and the quality of the labels.\n",
    "\n",
    "Check [our guide](../guides/weak-supervision.ipynb) for an extensive introduction to weak supervision with Rubrix. \n",
    "Also, check the [feature reference](../reference/webapp/define_rules.md) for the Define rules mode of the web app and our various tutorials **(TODO: add link once we have the gallery)** to see practical examples of weak supervision workflows. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
