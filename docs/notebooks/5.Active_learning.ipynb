{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install modAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install polling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "import polling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_files('datasets/bbc/', encoding=\"utf-8\", decode_error=\"replace\")\n",
    "# calculate count of each category\n",
    "labels, counts = np.unique(data.target, return_counts=True)\n",
    "# convert data.target_names to np array for fancy indexing\n",
    "labels_str = np.array(data.target_names)[labels]\n",
    "print(dict(zip(labels_str, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "list(t[:80] for t in X_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Vectorizer (word to ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000, decode_error=\"ignore\")\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize training set\n",
    "X_train_vectorized = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "cls = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Active learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "#from modAL.batch import uncertainty_batch_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ActiveLearner(\n",
    "    estimator=cls,\n",
    "    #query_strategy=uncertainty_batch_sampling,# this could be used to gather more instances at each timestep\n",
    "    n_instances=5\n",
    "    #X_training=X_train_vectorized[0:10], y_training=y_train[0:10] # this could be used to bootstrap the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _rubric import rubric\n",
    "from rubric.sdk.models import *\n",
    "from rubric.sdk.api.text_classification import bulk_records, search_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJAcmVjb2duYWkiLCJleHAiOjE2MTQ0NTgzNjl9.PlS29RTTrPMKz0FIWO4Qwk_9U_i1q5ZC_OVHbDqRIaU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = rubric.init(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for labels\n",
    "# active learning\n",
    "X_pool = X_test\n",
    "n_queries = 10\n",
    "for idx in range(n_queries):\n",
    "    \n",
    "    # Vectorize pool examples\n",
    "    vecs = vectorizer.transform(X_pool)\n",
    "    \n",
    "    # Query for uncertain examples\n",
    "    query_idx, query_inst = learner.query(vecs) # we could retrieve and log more each time with n_instances=\n",
    "    \n",
    "    # build rubric records\n",
    "    uncertain_records = []\n",
    "    for idx in query_idx:\n",
    "        predictions = learner.predict_proba(vecs[idx])[0] # not need but might be useful to log\n",
    "        uncertain_records.append(TextClassificationRecord.from_dict({\n",
    "            \"id\":  int(idx),\n",
    "            \"inputs\": {\"text\": str(X_pool[idx])},\n",
    "            \"prediction\": {\"agent\": \"active_learner\", \"labels\": [{'class': labels_str[j], 'confidence': proba} for j,proba in enumerate(predictions)]} # this is not needed but can help the user\n",
    "        }))\n",
    "        \n",
    "    # log query records\n",
    "    rubric.log(uncertain_records, dataset=\"active_learning_example\")\n",
    "    \n",
    "    # TODO: we need to wait a couple of secs, otherwise index is not updated\n",
    "    \n",
    "    # poll rubric until no records left to annotate\n",
    "    polling.poll(\n",
    "        lambda: search_records.sync(\n",
    "            client=cli, \n",
    "            dataset_id=\"active_learning_example\", \n",
    "            json_body=TextClassificationSearchRequest.from_dict({\"query\": {\"status\": [\"Default\"]}})\n",
    "        ).total == 0,\n",
    "        step=10,\n",
    "        poll_forever=True\n",
    "    )\n",
    "    # TODO: now get the annotated labels and teach the learner\n",
    "    results = search_records.sync(\n",
    "            client=cli, \n",
    "            dataset_id=\"active_learning_example\", \n",
    "            json_body=TextClassificationSearchRequest.from_dict({\"query\": {\"status\": [\"Validated\"]}})\n",
    "    )\n",
    "    learner.teach(vecs[query_idx], y_train[query_idx])\n",
    "    \n",
    "    # remove examples from the pool\n",
    "    X_pool = np.delete(X_pool, query_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
