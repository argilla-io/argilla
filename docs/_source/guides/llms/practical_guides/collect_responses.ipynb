{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect responses\n",
    "\n",
    "To collect the responses given by annotators, you can simply load the dataset in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = rg.FeedbackDataset.from_argilla(\"demo_feedback\", workspace=\"recognai\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dataset doesn't have any annotation overlap i.e., all records have one response only, the post-processing stage will be quite simple. You just need to transform the dataset into whatever format you need for training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "\n",
    "Remember to only take into account responses with the `submitted` status."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure and solve disagreements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dataset does have records with more than one `submitted` response, you will need to unify the responses before using the data for training. \n",
    "\n",
    "For example, you can see how the responses from these two users differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which episodes of season four of Game of Thrones did Michelle MacLaren direct?\n",
      "Answer: She directed \"Oathkeeper\" and \"First of His Name\" the fourth and fifth episodes of season four, respectively.\n",
      "--------------------------------------------------\n",
      "Annotator 1\n",
      "Status: submitted\n",
      "Response: {'rating': {'value': 5}}\n",
      "--------------------------------------------------\n",
      "Annotator 2\n",
      "Status: submitted\n",
      "Response: {'rating': {'value': 3}, 'corrected-text': {'value': 'In Season 3 she directed the episodes \"The Bear and the Maiden Fair\" and \"Second Sons\". In Season 4 , she directed another two episodes: \"Oathkeeper\" and \"First of His Name\".'}}\n"
     ]
    }
   ],
   "source": [
    "# example of annotation results for a single record\n",
    "print(f\"Question: {feedback[4]['fields']['question']}\")\n",
    "print(f\"Answer: {feedback[4]['fields']['answer']}\")\n",
    "for ix,response in enumerate(feedback[4]['responses']):\n",
    "    print('-'*50)\n",
    "    print(f\"Annotator {ix+1}\")\n",
    "    print(f\"Status: {response['status']}\")\n",
    "    print(f\"Response: {response['values']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratings often represent a subjective value, meaning that there is no wrong or right answer for these questions. However, since a `RatingQuestion` has a closed set of options, their results can help with visualizing the disagreement between annotators. On the other hand, texts are unique and subjective, making it almost impossible that two annotators will give the same answer for a `TextQuestion`. For this reason, we don't recommend using these responses to measure disagreements.\n",
    "\n",
    "If you want to do an initial exploration of the responses, you can use your preferred library for plotting data. Here are some simple examples of some visualisations that you could do to evaluate the potential disagreement in the responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1: submitted responses per record\n",
    "from collections import Counter,OrderedDict\n",
    "import plotly.express as px\n",
    "\n",
    "count_submitted = Counter()\n",
    "for record in feedback:\n",
    "    if record['responses'] != []:\n",
    "        submitted = [r for r in record['responses'] if r['status']=='submitted']\n",
    "        count_submitted[len(submitted)] += 1\n",
    "count_submitted = OrderedDict(sorted(count_submitted.items()))\n",
    "count_submitted = [{'submitted_responses': k, 'no_records': v} for k,v in count_submitted.items()]\n",
    "\n",
    "\n",
    "fig = px.bar(count_submitted, x='submitted_responses', y='no_records')\n",
    "fig.update_xaxes(title_text=\"No. of submitted responses\", dtick=1)\n",
    "fig.update_yaxes(title_text=\"No. of records\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Plot 1: Submitted responses per record](../../../_static/images/llms/collect_responses_plot_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 2: distance between responses in rating question\n",
    "list_values = []\n",
    "for record_ix,record in enumerate(feedback):\n",
    "    if record['responses'] != []:\n",
    "        submitted = [r for r in record['responses'] if r['status']=='submitted']\n",
    "        if len(submitted) > 1:\n",
    "            for response_ix,response in enumerate(submitted):\n",
    "                list_values.append({'record': str(record_ix+1), 'annotator': response_ix+1, 'value': response['values']['rating']['value']})\n",
    "\n",
    "fig = px.histogram(list_values, x='record', y='value', color='annotator', barmode='group')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Plot 2: Distance between responses in the rating question](../../../_static/images/llms/collect_responses_plot_2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint\n",
    "\n",
    "If you feel like the disagreement between annotators is too high, especially for questions that aren't as subjective, this is a good sign that you should review your annotation guidelines and/or the questions and options."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will explore some techniques you can use to solve disagreements in the responses. These are not the only possible techniques and you should choose them carefully according to the needs of your project and annotation team."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unifying ratings\n",
    "#### Majority vote\n",
    "If a record has more than 2 submitted responses, you can take the most popular option as the final score. In the case of a tie, you can break it choosing a random option or the lowest / highest score.\n",
    "\n",
    "#### Mean score\n",
    "For this technique you can take all responses and calculate the mean score. That will be final rating.\n",
    "\n",
    "#### Lowest / highest score\n",
    "Depending on how the question is formulated, you can take the `max` or `min` value. That will be the final rating.\n",
    "\n",
    "### Unifying texts\n",
    "#### Rate / rank the responses\n",
    "Make a new dataset that includes the texts you have collected in the record fields and ask your annotation team to rate or rank the responses. Then choose the response with the highest score. If there is a tie, choose one of the options randomly or consider duplicating the record as explained [below](#duplicate-the-record).\n",
    "\n",
    "#### Choose based on the annotator\n",
    "Take a subset of the records (enough to get a good representation of responses from each annotator), and rate / rank them as explained in the section above. Then, give each annotator a score based on the preferences of the team. You can use this score to choose text responses over the whole dataset.\n",
    "\n",
    "#### Choose based on answers to other questions\n",
    "You can use the answers to other questions as quality markers. For example, you can assume that whoever gave the lowest score will make a more extensive correction and you may want to choose that as the final text. However, this method does not guarantee that the text will be of good quality.\n",
    "\n",
    "#### Duplicate the record\n",
    "You may consider that the different answers given by your annotation team are all valid options. In this case, you can duplicate the record to keep each answer. Again, this method does not guarantee the quality of the text, so it is recommended to check the quality of the text, for example using a rating question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export or publish a dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to export a dataset, load the dataset using the `from_argilla` method as demonstrated [above](#collect-responses) and turn it into the desired format. Then, you can save it, for example, as a json file or even push it to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# save as json\n",
    "with open(\"my_file.json\", \"w\") as file:\n",
    "    json.dump(feedback, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to a public dataset in the Hugging Face Hub\n",
    "feedback.push_to_huggingface(\"argilla/feedback-dataset\", generate_card=True)\n",
    "\n",
    "# push to a private dataset in the Hugging Face Hub\n",
    "feedback.push_to_huggingface(\"argilla/feedback-dataset\", generate_card=True, private=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hugging Face Hub uses git for version control, meaning that the data will be updated every time that you push using the same dataset name, but previous versions won't be lost. Feedback Datasets saved in the Hub keeping Argilla's format can be loaded ready to be pushed to Argilla as explained [above](#create-and-import-a-dataset)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d98cb9bf90a932b5bf8e86e91214497eb0e38eb318595fbd6fbd5460fe92036"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
