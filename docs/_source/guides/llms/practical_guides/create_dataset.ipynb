{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Feedback Dataset\n",
    "\n",
    "The `FeedbackTask` datasets allow you to combine multiple questions of different kinds, so the first step will be to define the aim of your project and the kind of data and feedback you will need to get there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Python client you'll be able to easily create a `FeedbackTask` dataset, either locally with `rg.FeedbackDataset` or directly pushing it to Argilla with `rg.create_feedback_dataset()`. In this guide, we'll show you how to use both, once the scope of the project is defined, which implies knowing the `fields`, `questions`, and guidelines (if applicable) that you'll need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rg.create_feedback_dataset()` function expects the following arguments:\n",
    "\n",
    "- `name`: The name of the dataset in Argilla.\n",
    "- `workspace` (optional): The name of the workspace where the dataset will be created. If you don't provide one, it will be placed in the default workspace attached to the API key used in `rg.init()`.\n",
    "- `guidelines` (optional): A set of guidelines for the annotators. These will appear in the dataset settings in the UI.\n",
    "- `fields`: The list of fields to show in the record card. The order in which the fields will appear in the UI matches the order of this list.\n",
    "- `questions`: The list of questions to show in the form. The order in which the questions will appear in the UI matches the order of this list.\n",
    "\n",
    "Or, if you prefer to create the dataset locally, you can use `rg.FeedbackDataset`:\n",
    "\n",
    "- `guidelines` (optional): A set of guidelines for the annotators. These will appear in the dataset settings in the UI.\n",
    "- `fields`: The list of fields to show in the record card. The order in which the fields will appear in the UI matches the order of this list.\n",
    "- `questions`: The list of questions to show in the form. The order in which the questions will appear in the UI matches the order of this list."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick example is presented below, where we create a `FeedbackDataset` locally to assess the quality of a reponse in a question-answering task. The `FeedbackDataset` contains two fields, question and answer, and two questions to measure the quality of the answer and to correct it, if needed; and also a guideline for the annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "dataset = rg.FeedbackDataset(\n",
    "    guidelines=\"Please, read the question carefully and try to answer it as accurately as possible.\",\n",
    "    fields=[\n",
    "        rg.TextField(name=\"question\"),\n",
    "        rg.TextField(name=\"answer\"),\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.RatingQuestion(\n",
    "            name=\"answer_quality\",\n",
    "            description=\"How would you rate the quality of the answer?\",\n",
    "            values=[1, 2, 3, 4, 5],\n",
    "        ),\n",
    "        rg.TextQuestion(\n",
    "            name=\"answer_correction\",\n",
    "            description=\"If you think the answer is not accurate, please, correct it.\",\n",
    "            required=False,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, everything will be explained in detail in the following subsections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format records\n",
    "A record in Argilla refers to a data item that requires annotation and can consist of one or multiple fields. For example, your records can include a pair of a prompt and an output. Currently, we only support plain text fields, but we plan to introduce support for markdown and images in the future.\n",
    "\n",
    "Take some time to explore and find data that fits the purpose of your project. If you are planning to use public data, the [Datasets page](https://huggingface.co/datasets) of the Hugging Face Hub is a good place to start.\n",
    "\n",
    "<div class=\"admonition hint\">\n",
    "Always check the licenses of the datasets to make sure you can legally use the dataset for your specfic use case.\n",
    "</div>\n",
    "\n",
    "Once you have a dataset, load it and inspect it to find the fields that you want to use in your Feedback dataset. A quick overview of the data will also help you formulate the right questions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('databricks/databricks-dolly-15k', split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# turn it into a pandas dataframe to get a quick overview of a few examples\n",
    "df = pd.DataFrame(dataset)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create records following Argilla's Feedback Record format [link to Python reference].\n",
    "\n",
    "The name of the fields will need to match the fields set up in the dataset configuration (see [Create and import a dataset](../practical_guides/create_and_import_dataset.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we create the records, we can rename the fields and optionally filter the original dataset\n",
    "records = [rg.FeedbackRecord(fields={\"question\": record[\"instruction\"], \"answer\": record[\"response\"]}) for record in dataset if record[\"category\"]==\"open_qa\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `fields`\n",
    "\n",
    "Before adding records to the `FeedbackDataset` you need to define the fields of the record, which is the data to annotate. So on, the provided fields will be a list of `FieldSchema` objects that define the configuration of that field. For the moment, as of Argilla 1.8.0, just the `TextField` is supported, which as its name suggests, is a plain text field.\n",
    "\n",
    "We have plans to expand the range of supported field types in future releases of Argilla.\n",
    "\n",
    "You can define the fields using the Python SDK providing the following arguments:\n",
    "\n",
    "- `name`: The name of the field, as it will be seen internally.\n",
    "- `title` (optional): The name of the field, as it will be displayed in the UI. Defaults to the `name` value, but capitalized.\n",
    "- `required` (optional): Whether the field is required or not. Defaults to `True`.\n",
    "\n",
    "Note that at least one field must be required, which implies `required=True` for at least one of the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    rg.TextField(name=\"question\", required=True),\n",
    "    rg.TextField(name=\"answer\", required=True),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `questions`\n",
    "\n",
    "To collect feedback for your dataset, you need to formulate questions. The Feedback Task currently supports the following types of questions:\n",
    "\n",
    "- Rating: These questions require annotators to select one option from a list of integer values. This type is useful for collecting numerical scores.\n",
    "- Text: These questions offer annotators a free-text area where they can enter any text. This type is useful for collecting natural language data, such as corrections or explanations.\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "We have plans to expand the range of supported question types in future releases of the Feedback Task.\n",
    "</div>\n",
    "\n",
    "You can define your questions using the Python SDK and set up the following configurations:\n",
    "\n",
    "- `name`: The name of the question, as it will be seen internally.\n",
    "- `title` (optional): The name of the question, as it will be displayed in the UI. Defaults to the `name` value, but capitalized.\n",
    "- `required` (optional): Whether the question is required or not. Defaults to `True`.\n",
    "- `description` (optional): The text to be displayed in the question tooltip in the UI. You can use it to give more context or information to annotators.\n",
    "\n",
    "Additionally, if the question is a `RatingQuestion`, you'll also need to specify:\n",
    "- `values`: The rating options to answer the `RatingQuestion`. It must be a list of integer values, but there's no need of those values to be neither positive, not sequential, it can be any list of unique integers.\n",
    "\n",
    "Note that at least one question must be required, which implies `required=True` for at least one of the questions.\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "The order of the questions in the UI follows the order in which these are added to the dataset in the Python SDK.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of questions to display in the feedback form\n",
    "questions =[\n",
    "    rg.RatingQuestion(\n",
    "        name=\"rating\", \n",
    "        title=\"Rate the quality of the response:\", \n",
    "        description=\"1 = very bad - 5= very good\",\n",
    "        required=True,\n",
    "        values=[1, 2, 3, 4, 5]\n",
    "    ),\n",
    "    rg.TextQuestion(\n",
    "        name=\"corrected-text\",\n",
    "        title=\"Provide a correction to the response:\",\n",
    "        required=False\n",
    "    )\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `guidelines`\n",
    "\n",
    "Once you have decided on the data to show and the questions to ask, it's important to provide clear guidelines to the annotators. These guidelines help them understand the task and answer the questions consistently. You can provide guidelines in two ways:\n",
    "\n",
    "- In the dataset guidelines: this is added as an argument when you create your dataset in the Python SDK (see below). It will appear in the dataset settings in the UI.\n",
    "- As question descriptions: these are added as an argument when you create questions in the Python SDK (see above). This text will appear in a tooltip next to the question in the UI.\n",
    "\n",
    "It is good practice to use at least the dataset guidelines, if not both methods. In the guidelines, you can include a description of the project, details on how to answer each question with examples, instructions on when to discard a record, etc. Question descriptions should be short and provide context to a specific question. They can be a summary of the guidelines to that question, but often times that is not sufficient to align the whole annotation team."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `FeedbackDataset`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TODO: show both alternatives `rg.FeedbackDataset` with the fields, questions, and guidelines defined above, and `rg.create_feedback_dataset()`\n",
    "* TODO: link to `import_export_dataset.ipynb` to see how to upload it to Argilla or HuggingFace Hub.\n",
    "* TODO: add/format records once the dataset has been created not before"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d98cb9bf90a932b5bf8e86e91214497eb0e38eb318595fbd6fbd5460fe92036"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
