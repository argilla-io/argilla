{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d09c1533-97fd-43f7-83ea-f3a56edd1d5e",
   "metadata": {},
   "source": [
    "# ü¶æ Train a Model\n",
    "\n",
    "This guide showcases how to train a model on the `Dataset` classes in the Argilla client.\n",
    "The Dataset classes are lightweight containers for Argilla records. These classes facilitate importing from and exporting to different formats (e.g., `pandas.DataFrame`, `datasets.Dataset`) as well as sharing and versioning Argilla datasets using the Hugging Face Hub. \n",
    "\n",
    "For each record type, there's a corresponding Dataset class called `DatasetFor<RecordType>`.\n",
    "You can look up their API in the [reference section](../reference/python/python_client.rst).\n",
    "\n",
    "There are two ways to train custom models on top of your annotated data:\n",
    "\n",
    "1. Train models using the Argilla training module, which is quick and easy but does not offer specific customization.\n",
    "2. Train with a custom workflow using the prepare for training methods, which requires some configuration but also offers more flexibility to integrate with your existing training workflows.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99a0a8a2",
   "metadata": {},
   "source": [
    "## Train directly\n",
    "\n",
    "Quick and easy but does not offer specific customization.\n",
    "\n",
    "The `ArgillaTrainer` is a wrapper around many of our favorite NLP libraries. It provides a very intuitive abstract workflow to facilitate simple training workflows using decent default pre-set configurations without having to worry about any data transformations from Argilla. We plan on adding more support for tasks and frameworks like OpenAI, AutoTrain and SageMaker in the coming releases.\n",
    "\n",
    "| Framework/Task    | TextClassification | TokenClassification | Text2Text |\n",
    "|-------------------|--------------------|---------------------|-----------|\n",
    "| Transformers      | ‚úîÔ∏è                  | ‚úîÔ∏è                   |           |\n",
    "| spaCy             | ‚úîÔ∏è                  | ‚úîÔ∏è                   |           |\n",
    "| SetFit            | ‚úîÔ∏è                  |                     |           |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b318950",
   "metadata": {},
   "source": [
    "### The `ArgillaTrainer`\n",
    "\n",
    "We can use the `ArgillaTrainer` to train directly using `spacy`, `setfit` and `transformers` as framework variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from argilla.training import ArgillaTrainer\n",
    "\n",
    "trainer = ArgillaTrainer(\n",
    "    name=\"<my_dataset_name>\",\n",
    "    workspace=\"<my_workspace_name>\",\n",
    "    framework=\"<my_framework>\",\n",
    "    train_size=0.8\n",
    ")\n",
    "\n",
    "trainer.train(path=\"<my_model_path>\")\n",
    "records = trainer.predict(\"The ArgillaTrainer is great!\", as_argilla_records=True)\n",
    "rg.log(records=records, name=\"<my_dataset_name>\", workspace=\"<my_workspace_name>\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84725635",
   "metadata": {},
   "source": [
    "### Update training config\n",
    "\n",
    "The trainer also has an `ArgillaTrainer.update_config()` method, which maps `**kwargs` to the respective framework. So, these can be derived from the underlying framework that was used to initialize the trainer. Underneath, you can find an overview of these variables for the supported frameworks. Note that you don't need to pass all of them directly and that the values below are their default configurations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cfa853d",
   "metadata": {},
   "source": [
    "#### SetFit\n",
    "\n",
    "```python\n",
    "# `setfit.SetFitModel`\n",
    "trainer.update_config(\n",
    "    pretrained_model_name_or_path = \"all-MiniLM-L6-v2\",\n",
    "    force_download = False,\n",
    "    resume_download = False,\n",
    "    proxies = None,\n",
    "    token = None,\n",
    "    cache_dir = None,\n",
    "    local_files_only = False\n",
    ")\n",
    "# `setfit.SetFitTrainer`\n",
    "trainer.update_config(\n",
    "    metric = \"accuracy\",\n",
    "    num_iterations = 20,\n",
    "    num_epochs = 1,\n",
    "    learning_rate = 2e-5,\n",
    "    batch_size = 16,\n",
    "    seed = 42,\n",
    "    use_amp = True,\n",
    "    warmup_proportion = 0.1,\n",
    "    distance_metric = \"BatchHardTripletLossDistanceFunction.cosine_distance\",\n",
    "    margin = 0.25,\n",
    "    samples_per_label = 2\n",
    ")\n",
    "```\n",
    "\n",
    "#### spaCy\n",
    "\n",
    "```python\n",
    "# `spacy.training`\n",
    "trainer.update_config(\n",
    "    dev_corpus = \"corpora.dev\",\n",
    "    train_corpus = \"corpora.train\",\n",
    "    seed = 42,\n",
    "    gpu_allocator = 0,\n",
    "    accumulate_gradient = 1,\n",
    "    patience = 1600,\n",
    "    max_epochs = 0,\n",
    "    max_steps = 20000,\n",
    "    eval_frequency = 200,\n",
    "    frozen_components = [],\n",
    "    annotating_components = [],\n",
    "    before_to_disk = None,\n",
    "    before_update = None\n",
    ")\n",
    "```\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "```python\n",
    "# `transformers.AutoModelForTextClassification`\n",
    "trainer.update_config(\n",
    "    pretrained_model_name_or_path = \"distilbert-base-uncased\"\n",
    "    force_download = False\n",
    "    resume_download = False\n",
    "    proxies = None\n",
    "    token = None\n",
    "    cache_dir = None\n",
    "    local_files_only = False\n",
    ")\n",
    "# `transformers.TrainingArguments`\n",
    "trainer.update_config(\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.9,\n",
    "    adam_epsilon = 1e-8,\n",
    "    max_grad_norm = 1,\n",
    "    learning_rate = 5e-5,\n",
    "    num_train_epochs = 3,\n",
    "    max_steps = 0,\n",
    "    log_level = \"passive\",\n",
    "    logging_strategy = \"steps\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = 500,\n",
    "    seed = 42,\n",
    "    push_to_hub = False,\n",
    "    hub_model_id = \"user_name/output_dir_name\",\n",
    "    hub_strategy = \"every_save\",\n",
    "    hub_token = \"1234\",\n",
    "    hub_private_repo = False\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "522dd06e",
   "metadata": {},
   "source": [
    "### An example workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a30e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_rg = rg.DatasetForTokenClassification.from_datasets(\n",
    "    dataset=load_dataset(\"conll2003\", split=\"train[:100]\"),\n",
    "    tags=\"ner_tags\",\n",
    ")\n",
    "rg.log(dataset_rg, name=\"conll2003\", workspace=\"argilla\")\n",
    "\n",
    "trainer = ArgillaTrainer(\n",
    "    name=\"conll2003\",\n",
    "    workspace=\"argilla\",\n",
    "    framework=\"spacy\",\n",
    "    train_size=0.8\n",
    ")\n",
    "trainer.update_config(max_epochs=2)\n",
    "trainer.train(output_dir=\"my_easy_model\")\n",
    "records = trainer.predict(\"The ArgillaTrainer is great!\", as_argilla_records=True)\n",
    "rg.log(records=records, name=\"conll2003\", workspace=\"argilla\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8e62f68",
   "metadata": {},
   "source": [
    "## Train custom workflow \n",
    "\n",
    "Custom workflows give you more flexibility to integrate with your existing training workflows.\n",
    "\n",
    "### Prepare for training\n",
    "If you want to train a model we provide a handy method to prepare your dataset: `DatasetFor*.prepare_for_training()`.\n",
    "It will return a Hugging Face dataset, a spaCy DocBin or a SparkNLP-formatted DataFrame, optimized for the training process with the Hugging Face Trainer, the spaCy CLI or the SparkNLP API. Our [training tutorials](../tutorials/steps/2_training.md) show entire training workflows for your favorite packages.\n",
    "\n",
    "### Train-test split\n",
    "\n",
    "It is possible to directly include train-test splits to the `prepare_for_training` by passing the `train_size` and `test_size` parameters.\n",
    "\n",
    "### Frameworks and Tasks\n",
    "\n",
    "*TextClassification*\n",
    "\n",
    "For text classification tasks, it flattens the inputs into separate columns of the returned dataset and converts the annotations of your records into integers and writes them in a label column:\n",
    "By passing the `framework` variable as `setfit`, `transformers`, `spark-nlp` or `spacy`. This task requires a `DatastForTextClassification`.\n",
    "\n",
    "\n",
    "*TokenClassification*\n",
    "\n",
    "For token classification tasks, it converts the annotations of a record into integers representing BIO tags and writes them in a `ner_tags` column:\n",
    "By passing the `framework` variable as `transformers`, `spark-nlp` or `spacy`.  This task requires a `DatastForTokenClassification`.\n",
    "\n",
    "*Text2Text*\n",
    "\n",
    "For text generation tasks like `summarization` and translation tasks, it converts the annotations of a record `text` and `target` columns.\n",
    "By passing the `framework` variable as `transformers` and `spark-nlp`.  This task requires a `DatastForText2Text`.\n",
    "\n",
    "\n",
    "| Framework/Dataset | TextClassification | TokenClassification | Text2Text |\n",
    "|-------------------|--------------------|---------------------|-----------|\n",
    "| Transformers      | ‚úîÔ∏è                  | ‚úîÔ∏è                   | ‚úîÔ∏è         |\n",
    "| spaCy             | ‚úîÔ∏è                  | ‚úîÔ∏è                   |           |\n",
    "| SetFit            | ‚úîÔ∏è                  |                     |           |\n",
    "| Spark NLP         | ‚úîÔ∏è                  | ‚úîÔ∏è                   | ‚úîÔ∏è         |\n",
    "\n",
    "#### spaCy\n",
    "\n",
    "```python\n",
    "import argilla as rg\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "dataset_rg = rg.load(\"<my_dataset>\")\n",
    "dataset_rg.prepare_for_training(framework=\"spacy\", lang=nlp, train_size=1)\n",
    "# <spacy.tokens._serialize.DocBin object at 0x280613af0>\n",
    "```\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "```python\n",
    "import argilla as rg\n",
    "\n",
    "dataset_rg = rg.load(\"<my_dataset>\")\n",
    "dataset_rg.prepare_for_training(framework=\"transformers\", train_size=1)\n",
    "# {'title': 'My title', 'content': 'My content', 'label': 0}\n",
    "```\n",
    "\n",
    "#### SetFit\n",
    "\n",
    "```python\n",
    "import argilla as rg\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "dataset_rg = rg.load(\"<my_dataset>\")\n",
    "dataset_rg.prepare_for_training(framework=\"setfit\", train_size=1)\n",
    "# {'title': 'My title', 'content': 'My content', 'label': 0}\n",
    "```\n",
    "\n",
    "#### Spark NLP\n",
    "\n",
    "```python\n",
    "import argilla as rg\n",
    "\n",
    "dataset_rg = rg.load(\"<my_dataset>\")\n",
    "dataset_rg.prepare_for_training(framework=\"spark-nlp\", train_size=1)\n",
    "# <pd.DataFrame>\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
