{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5723a326-5578-4c37-846b-266046c12a91",
   "metadata": {},
   "source": [
    "# Semantic search\n",
    "\n",
    "This guide gives an overview of the semantic search features. Since `1.2.0` Argilla supports adding vectors to Argilla records which can then be used for finding the most similar records to a given one. This feature uses vector or semantic search combined with more traditional search (keyword and filter based). \n",
    "\n",
    "Vector search leverages machine learning to capture rich semantic features by embedding items (text, video, images, etc.) into a vector space, which can be then used to find \"semantically\" similar items.\n",
    "\n",
    "In this guide, you'll find how to:\n",
    "\n",
    "- Setup your Elasticsearch or Opensearch endpoint with vector search support.\n",
    "- Encode text into vectors for Argilla records.\n",
    "- Use semantic search.\n",
    "\n",
    "The next section gives a general overview about how semantic search works in Argilla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648c176-0442-4905-b6e7-ed1056fbf83e",
   "metadata": {},
   "source": [
    "## How it works\n",
    "Semantic search in Argilla works as follows:\n",
    "\n",
    "1. One or several vectors can be included in the `vectors` field of Argilla Records. The `vectors` field accepts a dictionary where `keys` represent names and `values` the actual vectors. This is the case because certain use cases might require using several vectors. \n",
    "2. The vectors are stored at indexing time, once the records are logged with `rg.log`. \n",
    "3. If you have stored vectors in your dataset, you can use the semantic search feature in Argilla UI or the `vector` param in the `rg.load` method of the Python Client.\n",
    "\n",
    "In future versions, embedding services might be developed to facilitate steps 1 and 2 and associate vectors to records automatically. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "    \n",
    "It's completely up to the user which encoding or embedding mechanism to use for producing these vectors. In the \"Encode text fields\" section of this document you will find several examples and details about this process, using open source libraries (e.g., Hugging Face) as well as paid services (e.g., Cohere or OpenAI).\n",
    "\n",
    "Currently, Argilla uses vector search only for searching similar records (nearest neighbours) of a given vector. This can be leveraged from Argilla UI as well as the Python Client. In the future, vector search could be leveraged as well for free text queries using Argilla UI.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "## Setup Elasticsearch or Opensearch with vector search support\n",
    "\n",
    "In order to use this feature you should use Elasticsearch at least version `8.5.x` or Opensearch `2.3.0`.\n",
    "\n",
    "\n",
    "We also provide pre-configured docker-compose files in the root of Argilla's [Github repository](https://github.com/argilla-io/argilla):\n",
    "\n",
    "* Elasticsearch: `docker-compose.elasticsearch.yaml`\n",
    "\n",
    "* Opensearch: `docker-compose.opensearch.yaml`\n",
    "\n",
    "\n",
    "### Migrating from Elasticsearch 7.1.0 to 8.5.x\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Warning\n",
    "\n",
    "If you had Argilla running with Elasticsearch 7.1.0 you need to migrate to at least version 8.5.x. Before following the process described below, please read the official [Elasticsearch Migration Guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/migrating-8.5.html) carefully. \n",
    "\n",
    "</div>\n",
    "\n",
    "In order to migrate from Elasticsearch 7.1.0 and keep your datasets you can follow this process:\n",
    "\n",
    "1. Stop your current Elasticsearch service (we assume a migration for a `docker-compose` setup).\n",
    "2. Set the the Elasticsearch image to 7.17.x in your `docker-compose`.\n",
    "3. Start the Elasticsearch service again.\n",
    "4. Once is up and running, stop it again and set the Elasticsearch image to 8.5.x \n",
    "5. Finally, start again the Elasticsearch service. Data should be migrated properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83980b39-a52d-4282-a33e-79abf7dc31b0",
   "metadata": {},
   "source": [
    "## Encode text into vectors for Argilla records\n",
    "The first and most important thing to do before leveraging semantic search is to turn text into a numerical representation: a vector. In practical terms, you can think of a vector as an array or list of numbers. You can associate this list of numbers with an Argilla Record by using the aforementioned `vectors` field. But the question is: **how do you create these vectors?** \n",
    "\n",
    "Over the years, many approaches have been used to turn text into numerical representations. The goal is to \"encode\" meaning, context,  topics, etc.. This can be used to find \"semantically\" similar text. Some of these approaches are: *LSA* (Latent Semantic Analysis), *tf-idf*, *LDA* (Latent Dirichlet Allocation), or *doc2Vec*. More recent methods fall in the category of \"neural\" methods, which leveragage the power of large neural networks to *embed* text into dense vectors (a large array of real numbers). These methods have demonstrated a great ability of capturing semantic features. These methods are powering a new wave of technologies that fall under categories like neural search, semantic search, or vector search. Most of these methods involve using a large language model to encode the full context of a textual snippet, such as a sentence, a paragraph, and more lately larger documents.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "   \n",
    "In the context of Argilla, we intentionally use the term `vector` in favour of `embedding` to emphasize that users can leverage methods other than neural, which might be cheaper to compute, or be more useful for their use cases.\n",
    "</div>\n",
    "\n",
    "In the next sections, we show how to encode text using different models and services and how to add them to Argilla records.\n",
    "\n",
    "### Sentence Transformers\n",
    "SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. There are dozens of [pre-trained models available](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads) on the Hugging Face Hub.\n",
    "\n",
    "The code below will load a dataset from the Hub, encode the `text` field, and create the `vectors` field which will contain only one key (`mini-lm-sentence-transformers`). \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "   \n",
    "Vector keys are arbitrary names that will be used as a name for the vector and shown in the UI if there's more than 1 so users can decide which vector to use for finding similar records. Remember you can associate several vectors to one record by using different keys. \n",
    "</div>\n",
    "\n",
    "\n",
    "To run the code below you need to install `sentence_transformers` and `datasets` with pip: `pip install sentence_transformers datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bcb9f2-323b-46f2-8a3f-6d8727c9ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define fast version of sentence transformers\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"banking77\", split=\"test\")\n",
    "\n",
    "# Encode text field using batched computation\n",
    "dataset = dataset.map(lambda batch: {\"vectors\": encoder.encode(batch[\"text\"])}, batch_size=32, batched=True)\n",
    "\n",
    "# Turn vectors into a dictionary\n",
    "dataset = dataset.map(\n",
    "    lambda r: {\"vectors\": {\"mini-lm-sentence-transformers\": r[\"vectors\"]}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4473a11-0645-4bbd-ad63-b0bc40b1a06d",
   "metadata": {},
   "source": [
    "Our dataset now contains a `vectors` field with the embedding vector generated by the sentence transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d00fd-7491-494d-808c-3f173921e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d0fbd8-b4e0-4847-8e94-fcb4b2d8b55d",
   "metadata": {},
   "source": [
    "This dataset can be transformed into an Argilla Dataset by using the `DatasetForTextClassification.from_datasets` method. Then, this dataset can be logged into Argilla as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a698ce-e130-40e8-87ab-2cc1aaa1a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "rg_ds = rg.DatasetForTextClassification.from_datasets(dataset, annotation=\"label\")\n",
    "\n",
    "rg.log(\n",
    "    name=\"banking77\",\n",
    "    records=rg_ds,\n",
    "    chunk_size=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b2d615-0bed-49e7-90b8-41ee96fe9034",
   "metadata": {},
   "source": [
    "### OpenAI `Embeddings`\n",
    "\n",
    "OpenAI provides a API endpoint called [Embeddings](https://beta.openai.com/docs/api-reference/embeddings) to get a vector representation of a given input that can be easily consumed by machine learning models and algorithms. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Warning\n",
    "\n",
    "Due to the dimension size limitation of Lucene-based backends (Elasticsearch and Opensearch), you can currently only use the `text-similarity-ada-001` model which produces vectors of `1024` dimensions.\n",
    "    \n",
    "</div>\n",
    "\n",
    "The code below will load a dataset from the Hub, encode the `text` field, and create the `vectors` field which will contain only one key (`openai`) using the Embeddings endpoint.\n",
    "\n",
    "To run the code below you need to install `openai` and `datasets` with pip: `pip install openai datasets`.\n",
    "\n",
    "You also need to setup your OpenAI API key as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89572e06-9bfa-4eb8-9d40-a245b8afc245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from datasets import load_dataset\n",
    "\n",
    "openai.api_key = \"<your api key goes here>\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"banking77\", split=\"test\")\n",
    "\n",
    "def get_embedding(texts, model=\"text-similarity-ada-001\"):\n",
    "    response = openai.Embedding.create(input = texts, model=model)\n",
    "    vectors = [item[\"embedding\"] for item in response[\"data\"]]\n",
    "    return vectors\n",
    "\n",
    "# Encode text. Get only 500 vectors for testing, remove the select to do the full dataset\n",
    "dataset = dataset.select(range(500)).map(lambda batch: {\"vectors\": get_embedding(batch[\"text\"])}, batch_size=16, batched=True)\n",
    "\n",
    "# Turn vectors into a dictionary\n",
    "dataset = dataset.map(\n",
    "    lambda r: {\"vectors\": {\"text-similarity-ada-001\": r[\"vectors\"]}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a389cb-3ca8-46ad-ba64-24f779b50191",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d9f8c-8590-4447-a061-9fe1188851af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "rg_ds = rg.DatasetForTextClassification.from_datasets(dataset, annotation=\"label\")\n",
    "\n",
    "rg.log(\n",
    "    name=\"banking77-openai\",\n",
    "    records=rg_ds,\n",
    "    chunk_size=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f0b2f-823a-4ef7-8f95-941f1ca2fc12",
   "metadata": {},
   "source": [
    "### co:here `Co.Embed`\n",
    "\n",
    "[Co:here Co.Embed](https://docs.cohere.ai/reference/embed) is an API endpoint by Cohere which takes a piece of text and turns it into a vector embedding. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Warning\n",
    "\n",
    "Due to the dimension size limitation of Lucene-based backends (Elasticsearch and Opensearch), you can currently only use the `small` model which produces vectors of `1024` dimensions.\n",
    "    \n",
    "</div>\n",
    "\n",
    "The code below will load a dataset from the Hub, encode the `text` field, and create the `vectors` field which will contain only one key (`cohere`) using the Embeddings endpoint.\n",
    "\n",
    "To run the code below you need to install `cohere` and `datasets` with pip: `pip install cohere datasets`.\n",
    "\n",
    "You also need to setup your Cohere API key as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf1360-8ef6-4daf-a0d4-9528a24cac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "api_key = \"<your api key goes here>\"\n",
    "co = cohere.Client(api_key)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"banking77\", split=\"test\")\n",
    "\n",
    "def get_embedding(texts):\n",
    "    return co.embed(texts, model=\"small\").embeddings\n",
    "\n",
    "# Encode text. Get only 1000 vectors for testing, remove the select to do the full dataset\n",
    "dataset = dataset.select(range(1000)).map(lambda batch: {\"vectors\": get_embedding(batch[\"text\"])}, batch_size=16, batched=True)\n",
    "\n",
    "# Turn vectors into a dictionary\n",
    "dataset = dataset.map(\n",
    "    lambda r: {\"vectors\": {\"cohere-embed\": r[\"vectors\"]}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049653c9-b53e-47f0-b4a2-3717ba504be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "rg_ds = rg.DatasetForTextClassification.from_datasets(dataset, annotation=\"label\")\n",
    "\n",
    "rg.log(\n",
    "    name=\"banking77-cohere\",\n",
    "    records=rg_ds,\n",
    "    chunk_size=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74fdc06-40c3-4449-8855-ab8c0776b8a4",
   "metadata": {},
   "source": [
    "#### BERTopic\n",
    "\n",
    "[BERTopic](https://maartengr.github.io/BERTopic/index.html) is a topic modeling technique that leverages ðŸ¤— transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions.\n",
    "\n",
    "Topic modeling outputs can be used too as numerical representations of text or documents. One way to think about it is: a document is a probability distribution over topics, the higher the probability of a topic the more likely the text contains that topic. A consequence of this, is that topically similar documents will have similar distributions over topics. \n",
    "\n",
    "The code below will load a dataset from the Hub and fit a topic model using TF-IDF as the embedding method (you can find other methods in BERTopic's documentation). Using the `calculate_probabilities` parameter the topic model will return vectors with the probability for each topic. We will use this information for the `vectors` field.\n",
    "\n",
    "To run the code below you need to install `bertopic` and `datasets` with pip: `pip install bertopic datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be22b6-21c2-4c18-97d9-fce9c6f31502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import argilla as rg\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "\n",
    "# Load and transform dataset\n",
    "dataset = load_dataset(\"banking77\", split=\"test\")\n",
    "\n",
    "# Configure and fit BERTopic model\n",
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "embeddings = vectorizer.fit_transform(dataset[\"text\"])\n",
    "\n",
    "# Train our topic model using TF-IDF vectors\n",
    "topic_model = BERTopic(calculate_probabilities=True)\n",
    "topics, probs = topic_model.fit_transform(dataset[\"text\"], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e0ea4-3ec7-4c5d-ab86-927f6668b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_ds = rg.DatasetForTextClassification.from_datasets(dataset, annotation=\"label\")\n",
    "\n",
    "records = []\n",
    "\n",
    "# iterate over Argilla dataset and add vectors field from topic model probs\n",
    "for i,record in enumerate(rg_ds):\n",
    "    record.vectors = {\"berttopic\": probs[i].tolist()}\n",
    "    records.append(record)\n",
    "    \n",
    "rg.log(\n",
    "    name=\"banking77-bertopic\",\n",
    "    records=rg_ds,\n",
    "    chunk_size=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85aef80-7600-4e32-88a8-cdad9b87ce75",
   "metadata": {},
   "source": [
    "## Use semantic search\n",
    "\n",
    "This section introduces how to use the semantic search feature from Argilla UI and Argilla Python client.\n",
    "\n",
    "### Argilla UI\n",
    "\n",
    "TODO: Include screenshots and some highlights.\n",
    "\n",
    "### Argilla Python client\n",
    "\n",
    "The `rg.load` methods includes a `vector` parameter which can be used to retrieve similar records to a given vector, and a `limit` parameter to indicate the number of records to be retrieved. This parameter accepts a tuple with the key of the target vector (this should match with one of the keys of the `vectors` dictionary) and the query vector itself.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Warning\n",
    "\n",
    "In order to get good results, make sure you are using the same encoder model for generating the vector used for the query. For example, if your dataset has been encoded with the `all-MiniLM-L6-v2` model from sentence transformers, make sure to use the same model for encoding the text to be used for querying. Another option is to use an existing record in your dataset, which already contains a vector.\n",
    "    \n",
    "</div>\n",
    "\n",
    "#### Sentence Transformers\n",
    "\n",
    "Let's see how to retrieve similar records using the dataset created in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7837c7-d516-49ae-9b2c-245a9f5d8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "# Let's use a user query about a lost credit card\n",
    "embedding = encoder.encode(\"I lost my credit card. What should I do?\")\n",
    "\n",
    "ds = rg.load(\n",
    "    name=\"banking77\",\n",
    "    vector=(\"mini-lm-sentence-transformers\", embedding.tolist()),\n",
    "    limit=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ef3f61-05e1-4dee-8b9a-01754e0aa40a",
   "metadata": {},
   "source": [
    "If the query and vectors are working correctly, we should find queries with a similar topic or intent, and potentially the same label. Let's show the results in a table using the `to_pandas()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f1bbb7-2d99-4260-aa2f-aae6c0eaa205",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_pandas()[[\"text\", \"annotation\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca901c68-3257-4e95-bb9c-134d148affaa",
   "metadata": {},
   "source": [
    "#### Using the `query` param\n",
    "\n",
    "The `vector` param can be combined with the `query` param to combine vector search with traditional search. Let's see a further example: find the most similar records with the `card_arrival` label. To do this we use the Query string DSL described in the [Queries guide](queries.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd832e-483c-48c3-933d-95a48eab283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "# Let's use a user query about a lost credit card\n",
    "embedding = encoder.encode(\"I lost my credit card. What should I do?\")\n",
    "\n",
    "ds = rg.load(\n",
    "    name=\"banking77\",\n",
    "    vector=(\"mini-lm-sentence-transformers\", embedding.tolist()),\n",
    "    limit=20,\n",
    "    query=\"annotated_as:card_arrival\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5390877-9521-45ea-b476-bd2928185adc",
   "metadata": {},
   "source": [
    "In the table below we can see that the first example is a mix between a `lost_or_stolen` and `card_arrival` intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f28c75-d7e1-4e50-a267-3efd3c78cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "ds.to_pandas()[[\"text\", \"annotation\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0ad74-f14b-4301-8b18-0f659e01e3f3",
   "metadata": {},
   "source": [
    "#### OpenAI `Embeddings`\n",
    "\n",
    "Let's do the same with our OpenAI Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12cbea-33ac-4e4b-be17-c92a7fff2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = openai.Embedding.create(\n",
    "    input = [\"I lost my credit card. What should I do?\"], \n",
    "    model=\"text-similarity-ada-001\"\n",
    ")[\"data\"][0][\"embedding\"]\n",
    "\n",
    "\n",
    "ds = rg.load(\n",
    "    name=\"banking77-openai\",\n",
    "    vector=(\"text-similarity-ada-001\", vector),\n",
    "    limit=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854823f7-4112-4be3-aaaa-9833dec91d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_pandas()[[\"text\", \"annotation\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af20ab26-8aa0-4d7c-a5e8-a5f3089f889a",
   "metadata": {},
   "source": [
    "#### co:here `co.Embed`\n",
    "\n",
    "Let's do the same with our Cohere embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8eaa8-6c29-4833-8749-3f51e8ea04b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = co.embed([\"I lost my credit card. What should I do?\"], model=\"small\").embeddings[0]\n",
    "\n",
    "ds = rg.load(\n",
    "    name=\"banking77-cohere\",\n",
    "    vector=(\"cohere-embed\", vector),\n",
    "    limit=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8b198-f44c-4c80-b6e0-ba52a2731c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_pandas()[[\"text\", \"annotation\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
