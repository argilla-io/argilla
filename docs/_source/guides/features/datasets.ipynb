{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09c1533-97fd-43f7-83ea-f3a56edd1d5e",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "This guide showcases some features of the `Dataset` classes in the Argilla client.\n",
    "The Dataset classes are lightweight containers for Argilla records. These classes facilitate importing from and exporting to different formats (e.g., `pandas.DataFrame`, `datasets.Dataset`) as well as sharing and versioning Argilla datasets using the Hugging Face Hub.\n",
    "\n",
    "For each record type there's a corresponding Dataset class called `DatasetFor<RecordType>`.\n",
    "You can look up their API in the [reference section](../reference/python/python_client.rst#module-argilla.client.datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d4c9e-24a1-4a59-8a17-3b6ac1a39c88",
   "metadata": {},
   "source": [
    "## Creating a Dataset\n",
    "\n",
    "Under the hood the Dataset classes store the records in a simple Python list.\n",
    "Therefore, working with a Dataset class is not very different to working with a simple list of records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf8c7f-463d-48ee-944a-3adb57edb159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# Start with a list of Argilla records\n",
    "dataset_rg = rg.DatasetForTextClassification(my_records)\n",
    "\n",
    "# Loop over the dataset\n",
    "for record in dataset_rg:\n",
    "    print(record)\n",
    "\n",
    "# Index into the dataset\n",
    "dataset_rg[0] = rg.TextClassificationRecord(text=\"replace record\")\n",
    "\n",
    "# log a dataset to the Argilla web app\n",
    "rg.log(dataset_rg, \"my_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e9fc0-8563-4727-abca-62205a4de385",
   "metadata": {},
   "source": [
    "The Dataset classes do some extra checks for you, to make sure you do not mix record types when appending or indexing into a dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88889b-12f4-472f-bcbe-fb47be475d02",
   "metadata": {},
   "source": [
    "## Importing a Dataset\n",
    "\n",
    "When you have your data in a [_pandas DataFrame_](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) or a [_datasets Dataset_](https://huggingface.co/docs/datasets/access.html), we provide some neat shortcuts to import this data into a Argilla Dataset. \n",
    "You have to make sure that the data follows the record model of a specific task, otherwise you will get validation errors. \n",
    "Columns in your DataFrame/Dataset that are not supported or recognized, will simply be ignored.\n",
    "\n",
    "The record models of the tasks are explained in the [reference section](../reference/python/python_client.rst#module-argilla.client.models). \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "Due to it's pyarrow nature, data in a `datasets.Dataset` has to follow a slightly different model, that you can look up in the examples of the `Dataset*.from_datasets` [docstrings](../reference/python/python_client.rst#argilla.client.datasets.DatasetForTokenClassification.from_datasets). \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca56d4-2bb5-4c77-a069-7a50ee78b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# import data from a pandas DataFrame\n",
    "dataset_rg = rg.read_pandas(my_dataframe, task=\"TextClassification\")\n",
    "# or\n",
    "dataset_rg = rg.DatasetForTextClassification.from_pandas(my_dataframe)\n",
    "\n",
    "# import data from a datasets Dataset\n",
    "dataset_rg = rg.read_datasets(my_dataset, task=\"TextClassification\")\n",
    "# or\n",
    "dataset_rg = rg.DatasetForTextClassification.from_datasets(my_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a0e4d-f05b-4635-af54-885933f3bc9a",
   "metadata": {},
   "source": [
    "We also provide helper arguments you can use to read almost arbitrary datasets for a given task from the [Hugging Face Hub](https://huggingface.co/datasets).\n",
    "They map certain input arguments of the Argilla records to columns of the given dataset.\n",
    "Let's have a look at a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41209b56-0dce-4045-8a4f-ffc00f962a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "\n",
    "# the \"poem_sentiment\" dataset has columns \"verse_text\" and \"label\"\n",
    "dataset_rg = rg.DatasetForTextClassification.from_datasets(\n",
    "    dataset=load_dataset(\"poem_sentiment\", split=\"test\"),\n",
    "    text=\"verse_text\",\n",
    "    annotation=\"label\",\n",
    ")\n",
    "\n",
    "# the \"snli\" dataset has the columns \"premise\", \"hypothesis\" and \"label\"\n",
    "dataset_rg = rg.DatasetForTextClassification.from_datasets(\n",
    "    dataset=load_dataset(\"snli\", split=\"test\"),\n",
    "    inputs=[\"premise\", \"hypothesis\"],\n",
    "    annotation=\"label\",\n",
    ")\n",
    "\n",
    "# the \"conll2003\" dataset has the columns \"id\", \"tokens\", \"pos_tags\", \"chunk_tags\" and \"ner_tags\"\n",
    "rg.DatasetForTokenClassification.from_datasets(\n",
    "    dataset=load_dataset(\"conll2003\", split=\"test\"),\n",
    "    tags=\"ner_tags\",\n",
    ")\n",
    "\n",
    "# the \"xsum\" dataset has the columns \"id\", \"document\" and \"summary\"\n",
    "rg.DatasetForTextGeneration.from_datasets(\n",
    "    dataset=load_dataset(\"xsum\", split=\"test\"),\n",
    "    text=\"document\",\n",
    "    annotation=\"summary\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de341ab4-6be4-499b-9f4e-0eb4546fa753",
   "metadata": {},
   "source": [
    "You can also use the shortcut `rg.read_datasets(dataset=..., task=..., **kwargs)` where the keyword arguments are passed on to the corresponding `from_datasets()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb290a71-ad07-496f-b167-ad80d91fa633",
   "metadata": {},
   "source": [
    "## Sharing on Hugging Face\n",
    "\n",
    "You can easily share your Argilla dataset with your community via the Hugging Face Hub.\n",
    "For this you just need to export your Argilla Dataset to a `datasets.Dataset` and [push it to the hub](https://huggingface.co/docs/datasets/upload_dataset.html?highlight=push_to_hub#upload-from-python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6d70b-0b91-4efb-94b6-6b7710c105c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# load your annotated dataset from the Argilla web app\n",
    "dataset_rg = rg.load(\"my_dataset\")\n",
    "\n",
    "# export your Argilla Dataset to a datasets Dataset\n",
    "dataset_ds = dataset_rg.to_datasets()\n",
    "\n",
    "# push the dataset to the Hugging Face Hub\n",
    "dataset_ds.push_to_hub(\"my_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696605dd-be87-4ae6-b367-b0cdabfaf39f",
   "metadata": {},
   "source": [
    "Afterward, your community can easily access your annotated dataset and log it directly to the Argilla web app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a4792-bc91-4d64-8465-b2bccf23502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# download the dataset from the Hugging Face Hub\n",
    "dataset_ds = load_dataset(\"user/my_dataset\", split=\"train\")\n",
    "\n",
    "# read in dataset, assuming its a dataset for text classification\n",
    "dataset_rg = rg.read_datasets(dataset_ds, task=\"TextClassification\")\n",
    "\n",
    "# log the dataset to the Argilla web app\n",
    "rg.log(dataset_rg, \"dataset_by_user\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5b02c-a24c-40d0-80ce-6332fba15318",
   "metadata": {},
   "source": [
    "## Prepare dataset for training\n",
    "\n",
    "If you want to train a Hugging Face transformer or a spaCy NER pipeline, we provide a handy method to prepare your dataset: `DatasetFor*.prepare_for_training()`.\n",
    "It will return a Hugging Face dataset or a spaCy DocBin, optimized for the training process with the Hugging Face Trainer or the spaCy cli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1d63a-7940-46ff-85e9-062ab785b828",
   "metadata": {},
   "source": [
    "### TextClassification\n",
    "For text classification tasks, it flattens the inputs into separate columns of the returned dataset and converts the annotations of your records into integers and writes them in a label column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3bcc58-fdc9-427a-8ffd-ef37e67e03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rg = rg.DatasetForTextClassification(\n",
    "    [\n",
    "        rg.TextClassificationRecord(\n",
    "            inputs={\"title\": \"My title\", \"content\": \"My content\"}, annotation=\"news\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_rg.prepare_for_training()[0]\n",
    "# Output:\n",
    "# {'title': 'My title', 'content': 'My content', 'label': 0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907714da-5496-4a49-bc6d-2d9008d0082f",
   "metadata": {},
   "source": [
    "### TokenClassification\n",
    "\n",
    "For token classification tasks, it converts the annotations of a record into integers representing BIO tags and writes them in a `ner_tags` column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c72cb-599a-4073-a018-6f9746e17e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rg = rg.DatasetForTokenClassification(\n",
    "    [\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=\"I live in Madrid\",\n",
    "            tokens=[\"I\", \"live\", \"in\", \"Madrid\"],\n",
    "            annotation=[(\"LOC\", 10, 15)],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_rg.prepare_for_training()[0]\n",
    "# Output:\n",
    "# {..., 'tokens': ['I', 'live', 'in', 'Madrid'], 'ner_tags': [0, 0, 0, 1], ...}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83522c04",
   "metadata": {},
   "source": [
    "## Dataset settings\n",
    "\n",
    "Argilla datasets have certain *settings* that you can configure via the `rg.*Settings` classes, for example `rg.TextClassificationSettings`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51750c7c",
   "metadata": {},
   "source": [
    "### Define a labeling schema\n",
    "\n",
    "You can define a labeling schema for your Argilla dataset, which fixes the allowed labels for your predictions and annotations.\n",
    "Once you set a labeling schema, each time you log to the corresponding dataset, Argilla will perform validations of the added predictions and annotations to make sure they comply with the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# Define labeling schema\n",
    "settings = rg.TextClassificationSettings(label_schema=[\"A\", \"B\", \"C\"])\n",
    "\n",
    "# Apply settings to a new or already existing dataset\n",
    "rg.configure_dataset(name=\"my_dataset\", settings=settings)\n",
    "\n",
    "# Logging to the newly created dataset triggers the validation checks\n",
    "rg.log(rg.TextClassificationRecord(text=\"text\", annotation=\"D\"), \"my_dataset\")\n",
    "# BadRequestApiError: Argilla server returned an error with http status: 400\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "39f4e3bd8ecb53b4a2ef9bccb982583dac0632e40e094b10b94294b76eaa26cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
