{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™üèΩ Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Once you have uploaded and annotated your dataset in Argilla, you are ready to prepare it for training a model. Most NLP models today are trained via [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) and need input-output pairs to serve as training examples for the model. The input part of such pairs is usually the text itself, while the output is the corresponding annotation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual extraction\n",
    "\n",
    "The exact data format for training a model depends on your [training framework](#how-to-train-a-model) and the task you are tackling (text classification, token classification, etc.). Argilla is framework agnostic; you can always manually extract from the records what you need for the training. \n",
    "\n",
    "The extraction happens using the [client library](../../reference/python/python_client.rst) within a Python script, a Jupyter notebook, or another IDE. First, we have to load the annotated dataset from Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "dataset = rg.load(\"my_annotated_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "    \n",
    "If you follow a weak supervision approach, the steps are slightly different. \n",
    "We refer you to our [weak supervision guide](../../guides/techniques/weak_supervision.ipynb) for a complete workflow.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Then we can iterate over the records and extract our training examples. For example, let's assume you want to train a text classifier with a [sklearn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that takes as input a text and outputs a label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the inputs and labels in Python lists\n",
    "inputs, labels = [], []\n",
    "\n",
    "# Iterate over the records in the dataset\n",
    "for record in dataset:\n",
    "    \n",
    "    # We only want records with annotations\n",
    "    if record.annotation:\n",
    "        inputs.append(record.text)\n",
    "        labels.append(record.annotation)\n",
    "\n",
    "# Train the model\n",
    "sklearn_pipeline.fit(inputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a few frameworks and tasks, Argilla provides a convenient method to automatically extract training examples in a suitable format from a dataset. \n",
    "\n",
    "For example: If you want to train a [transformers](https://huggingface.co/docs/transformers/index) model for text classification, you can load an annotated dataset for text classification and call the `prepare_for_training()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.load(\"my_annotated_dataset\")\n",
    "\n",
    "dataset_for_training = dataset.prepare_for_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the returned `dataset_for_training`, you can continue following the steps to [fine-tune a pre-trained model](https://huggingface.co/docs/transformers/training#finetune-a-pretrained-model) with the [transformers library](https://huggingface.co/docs/transformers/index). \n",
    "\n",
    "Check the dedicated [dataset guide](../../guides/features/datasets.ipynb#prepare-dataset-for-training) for more examples of the `prepare_for_training()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train your model\n",
    "\n",
    "Argilla helps you to create and curate training data. **It is not a framework for training a model.** You can use Argilla complementary with other excellent open-source frameworks that focus on developing and training NLP models.\n",
    "\n",
    "Here we list three of the most commonly used open-source libraries, but many more are available and may be more suited for your specific use case:\n",
    "\n",
    " - [transformers](https://huggingface.co/docs/transformers/index): This library provides thousands of pre-trained models for various NLP tasks and modalities. Its excellent documentation focuses on fine-tuning those models to your specific use case;\n",
    " - [spaCy](https://spacy.io/): This library also comes with pre-trained models built into a pipeline tackling multiple tasks simultaneously. Since its a purely NLP library, it comes with much more NLP features than just model training;\n",
    " - [scikit-learn](https://scikit-learn.org/stable/): This de facto standard library is a powerful swiss army knife for machine learning with some NLP support. Usually, their NLP models lack the performance when compared to transformers or spacy, but give it a try if you want to train a lightweight model quickly; \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf5ecc3fecf17575e278cfc1ec050b1bf24f64b662a1b5421daed12bdd8255ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
