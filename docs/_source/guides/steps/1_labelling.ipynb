{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑ Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When labelling, we generally differentiate between manual labelling and co-operative or programmatic labelling. During co-operative labelling, we use external input like rules and inference prediction to kickstart our annotation process. Manual labelling is always needed and having a good understanding of the problem is key to getting started, however, co-operative labelling can significantly reduce your time spend labelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Labelling\n",
    "\n",
    "![Manual annotations of a sentiment classification task](../../_static/reference/webapp/features-metrics.png)\n",
    "\n",
    "The straightforward approach of manual annotations might be necessary if you do not have a suitable model for your use case or cannot come up with good heuristic rules for your dataset. \n",
    "It can also be a good approach if you dispose of a large annotation workforce or require few but unbiased and high-quality labels.\n",
    "\n",
    "Argilla tries to make this relatively cumbersome approach as painless as possible. \n",
    "Via an intuitive and adaptive UI, its exhaustive search and filter functionalities, and bulk annotation capabilities, Argilla turns the manual annotation process into an efficient option.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An annotation guideline\n",
    "\n",
    "Before starting the annotation process with a team, it is important to align the different truths everyone in the team thinks they have. Because the same text is going to be annotated by multiple annotators independently or we might want to revisit an old dataset later on. Besides a set of obvious mistakes, we also often encounter uncertain grey areas. Consider the following phrase for NER-annotation `Harry Potter and the prisoner of Azkaban` can be interpreted in many ways. The entire phrase is as the movie title, `Harry Potter` is a person, and `Azkaban` is location. Maybe we don¬¥t even want to annotate fictional locations and characters. Therefore, it is important to define these assumptions beforehand and iterate over them together with the team. Take a look at [the blog](https://snorkel.ai/data-annotation/) from our friend over at `snorkel` for more context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Rubrix UI\n",
    "\n",
    "Look at our dedicated [feature reference](../../reference/webapp/features.html) for a detailed and illustrative guide on manually annotating your dataset with Argilla for specific NLP Tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-annotate\n",
    "\n",
    "As stated previously, we can also pre-annotate our data using ML models. Within [Hugging Face](https://huggingface.co/models) it is quite likely you will find a model that already suits your specific dataset task to some degree. In Argilla, you can pre-annotate your data by including predictions from these models in your records. Assuming that the model works reasonably well on your dataset, you can filter for records with high prediction scores and validate the predictions. In this way, you will rapidly annotate part of your data and alleviate the annotation process.\n",
    "\n",
    "One downside of this approach is that your annotations will be subject to the possible biases and mistakes of the pre-trained model. When guided by pre-trained models, it is common to see human annotators get influenced by them. Therefore, it is advisable to avoid pre-annotations when building a rigorous test set for the final model evaluation.\n",
    "\n",
    "Check the [introduction tutorial](../../tutorials//notebooks/labelling-tokenclassification-spacy-pretrained.ipynb) to learn to add predictions to the records. \n",
    "And our [feature reference](../../reference/webapp/features.md) includes a detailed guide on validating predictions in the Argilla web app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero and few-shot models\n",
    "\n",
    "A special kind of pre-annotation models that can be found on [Hugging Face](https://huggingface.co/models) are zero and few-shot models. These"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak supervision rules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf5ecc3fecf17575e278cfc1ec050b1bf24f64b662a1b5421daed12bdd8255ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
