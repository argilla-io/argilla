{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63a57b3-afa4-41d6-910d-7f7a3c60d864",
   "metadata": {},
   "source": [
    "## üë®üèΩ‚Äçüíª Deploying\n",
    "\n",
    "Argilla currently gives users several ways to log model predictions. \n",
    "\n",
    "This brief guide introduces the different methods and expected usages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb191af2-e3af-4c54-8aa3-8e6fb781c234",
   "metadata": {},
   "source": [
    "### Using `rg.monitor`\n",
    "\n",
    "For widely-used libraries Argilla includes an \"auto-monitoring\" option via the `rg.monitor` method. Currently supported libraries are Hugging Face Transformers and spaCy, if you'd like to see another library supported feel free to add a discussion or issue on GitHub.\n",
    "\n",
    "`rg.monitor` will wrap HF and spaCy pipelines so every time you call them, the output of these calls will be logged into the dataset of your choice, as a background process, in a non-blocking way. Additionally, `rg.monitor` will add several tags to your dataset such as the library build version, the model name, the language, etc. This should also work for custom (private) pipelines, not only the Hub's or official spaCy models.\n",
    "\n",
    "It is worth noting that this feature is useful beyond monitoring, and can be used for data collection (e.g., bootstrapping data annotation with pre-trained pipelines), model development (e.g., error analysis), and model evaluation (e.g., combined with data annotation to obtain evaluation metrics).\n",
    "\n",
    "Let's see it in action using the IMDB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd93c3d-507e-4c23-a786-30e51d0697b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"test[0:1000]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd39d2-ade8-4351-87a2-0df664c27037",
   "metadata": {},
   "source": [
    "#### Hugging Face Transformer Pipelines\n",
    "\n",
    "Argilla currently supports monitoring `text-classification` and `zero-shot-classification` pipelines, but `token-classification` and `text-generation` pipelines will be added in coming releases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b18b8-0a27-4169-82d7-52a6909813bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import argilla as rg\n",
    "\n",
    "nlp = pipeline(\n",
    "    \"sentiment-analysis\", return_all_scores=True, padding=True, truncation=True\n",
    ")\n",
    "nlp = rg.monitor(nlp, dataset=\"nlp_monitoring\")\n",
    "\n",
    "dataset.map(lambda example: {\"prediction\": nlp(example[\"text\"])})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e901e-8a7e-46b8-9a78-655826ea307d",
   "metadata": {},
   "source": [
    "Once the `map` operation starts, you can start browsing the predictions in the Web-app.\n",
    "The default Argilla installation comes with **Elastic's Kibana** pre-configured, so you can easily build custom monitoring dashboards and alerts (for your team and other stakeholders)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729dba46-cd79-412b-a5b6-9aaadbf4c699",
   "metadata": {},
   "source": [
    "Record-level metadata is a key element of Argilla datasets, enabling users to do fine-grained analysis and dataset slicing. Let's see how we can log metadata while using `rg.monitor`. Let's use the label in ag_news to add a news_category field for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7331ce60-4e70-42e2-b5a6-4ef37ca9c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c791a9e-0b73-4e75-9018-f06c8457fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.map(\n",
    "    lambda example: {\n",
    "        \"prediction\": nlp(example[\"text\"], metadata={\"news_category\": example[\"label\"]})\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701842d3-0f12-4ac5-8a0c-6ddefb8a09e8",
   "metadata": {},
   "source": [
    "#### spaCy\n",
    "\n",
    "Argilla currently supports monitoring the NER pipeline component, but `textcat` will be added soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab03c18-3ffb-4e48-adfc-8f2890d2f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import argilla as rg\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = rg.monitor(nlp, dataset=\"nlp_monitoring_spacy\")\n",
    "\n",
    "dataset.map(lambda example: {\"prediction\": nlp(example[\"text\"])})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1131c32-3036-49ca-92cc-c8f2775f76c8",
   "metadata": {},
   "source": [
    "Once the `map` operation starts, you can start browsing the predictions in the Web-app:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936cbaec-223c-4902-beac-0a50dfe55b7b",
   "metadata": {},
   "source": [
    "#### Flair\n",
    "\n",
    "Argilla currently supports monitoring Flair NER pipelines component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a5ffb-35e0-485b-b99e-566b1a53043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = rg.monitor(\n",
    "    SequenceTagger.load(\"flair/ner-english\"), dataset=\"flair-example\", sample_rate=1.0\n",
    ")\n",
    "\n",
    "# make example sentence\n",
    "sentence = Sentence(\"George Washington went to Washington\")\n",
    "\n",
    "# predict NER tags. This will log the prediction in Argilla\n",
    "tagger.predict(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c560f7f-5287-4a76-8b4e-bce7cc718b3c",
   "metadata": {},
   "source": [
    "The following logs the predictions over the IMDB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb08dd-075f-4edd-9122-71f9973cba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(example):\n",
    "    tagger.predict(Sentence(example[\"text\"]))\n",
    "    return {\"prediction\": True}\n",
    "\n",
    "\n",
    "dataset.map(make_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987c362-61b3-4682-aa7b-693bed30d3ae",
   "metadata": {},
   "source": [
    "### Using async `rg.log` \n",
    "\n",
    "You can monitor your own models without adding a response delay by using the `background` param in `rg.log()`.\n",
    "\n",
    "Let's see an example using [BentoML](https://www.bentoml.com/) with a spaCy NER pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e24b1-b665-4e44-be52-62e024927643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45740aee-e022-452d-8f77-fb8872eb7d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile spacy_model.py\n",
    "\n",
    "\n",
    "from bentoml import BentoService, api, artifacts, env\n",
    "from bentoml.adapters import JsonInput\n",
    "from bentoml.frameworks.spacy import SpacyModelArtifact\n",
    "\n",
    "import argilla as rg\n",
    "\n",
    "\n",
    "@env(infer_pip_packages=True)\n",
    "@artifacts([SpacyModelArtifact(\"nlp\")])\n",
    "class SpacyNERService(BentoService):\n",
    "\n",
    "    @api(input=JsonInput(), batch=True)\n",
    "    def predict(self, parsed_json_list):\n",
    "        result, rb_records = ([], [])\n",
    "        for index, parsed_json in enumerate(parsed_json_list):\n",
    "            doc = self.artifacts.nlp(parsed_json[\"text\"])\n",
    "            prediction = [{\"entity\": ent.text, \"label\": ent.label_} for ent in doc.ents]\n",
    "            rb_records.append(\n",
    "                rg.TokenClassificationRecord(\n",
    "                    text=doc.text,\n",
    "                    tokens=[t.text for t in doc],\n",
    "                    prediction=[\n",
    "                        (ent.label_, ent.start_char, ent.end_char) for ent in doc.ents\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "            result.append(prediction)\n",
    "        \n",
    "        rg.log(\n",
    "            name=\"monitor-for-spacy-ner\",\n",
    "            records=rb_records,\n",
    "            tags={\"framework\": \"bentoml\"},\n",
    "            background=True, \n",
    "            verbose=False\n",
    "        ) # By using the background=True, the model latency won't be affected\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126b826f-c708-4b1d-898d-960058a7635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_model import SpacyNERService\n",
    "\n",
    "svc = SpacyNERService()\n",
    "svc.pack(\"nlp\", nlp)\n",
    "\n",
    "saved_path = svc.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fadd39-87ed-4b7c-9f62-7cfdba0e2813",
   "metadata": {},
   "source": [
    "You can predict some data without serving the model. Just launch following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69736fa1-e263-4086-9a53-a0be399b22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml run SpacyNERService:latest predict --input \"{\\\"text\\\":\\\"I am driving BMW\\\"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fe7dd-3ef0-42c6-bb2b-25a52a95c582",
   "metadata": {},
   "source": [
    "If you're running Argilla in local, go to http://localhost:6900/datasets/argilla/monitor-for-spacy-ner and see that the new dataset `monitor-for-spacy-ner` contains your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b49ea-7384-423d-b2d8-ac6280b7a200",
   "metadata": {},
   "source": [
    "### Using ASGI middleware\n",
    "\n",
    "For using the ASGI middleware, see this [tutorial](../../tutorials/notebooks/deploying-texttokenclassification-fastapi.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "39f4e3bd8ecb53b4a2ef9bccb982583dac0632e40e094b10b94294b76eaa26cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
