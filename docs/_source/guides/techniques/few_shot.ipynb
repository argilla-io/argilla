{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî´ Few-shot classification\n",
    "\n",
    "Zero and few-shot models can create reasonably well predictions while requiring zero or only a few training samples. For zero-shot classification models, you generally only provide a label. These models are more popular for `TextClassification` tasks, but there are also some examples for `TokenClassification`. These models generally perform okay out of the box but custom models generally performs better. That is why these kind of model are generally used to get a head start with [labelling](../../guides/steps/1_labelling.html) before [training](../../guides/steps/2_training.html) a tailor-made model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-shot with `SetFit`\n",
    "\n",
    "A more in-depth overview can be found in our tutorial about [SetFit](../../tutorials/notebooks/training-textclassification-setfit-fewshot.html). For now, we will just show a short overview of that tutorial. We have great dataset integration with `transformers`, `json` and `pandas`? Check our [Datasets](../guides/features/datasets.html) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "import argilla as rg\n",
    "\n",
    "# load a dataset from the hub\n",
    "unlabelled = (\n",
    "    load_dataset(\"imdb\", split=\"unsupervised\").shuffle(seed=42).select(range(100))\n",
    ")\n",
    "unlabelled = rg.DatasetForTextClassification.from_datasets(unlabelled)\n",
    "rg.log(unlabelled, \"imdb_unlabelled\")\n",
    "\n",
    "# Go to Argilla and label ca. 8 examples per label.\n",
    "\n",
    "# Load the handlabelled dataset from Argilla\n",
    "train_ds = rg.load(\"imdb_unlabelled\").prepare_for_training()\n",
    "test_ds = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "# Load SetFit model from Hub\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    batch_size=16,\n",
    "    num_iterations=20,  # The number of text pairs to generate\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "# Share the model and data with the world.\n",
    "train_ds.push_to_hub(\"setfit-mini-imdb-data\")\n",
    "trainer.push_to_hub(\"setfit-mini-imdb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-shot with `transformers`\n",
    "A good collection of zero-shot models can be found on the [Hugging Face model page](https://huggingface.co/models?pipeline_tag=zero-shot-classification&sort=downloads). For this example we will \n",
    "use the most popular one `facebook/bart-large-mnli`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load a dataset from the hub\n",
    "dataset = load_dataset(\"imdb\", split=\"unsupervised\")\n",
    "\n",
    "# load a model pipeline\n",
    "nlp = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    framework=\"pt\",\n",
    ")\n",
    "\n",
    "# deploy and monitor your model\n",
    "nlp = rg.monitor(nlp, dataset=\"transformers-mini-imdb\")\n",
    "dataset.map(\n",
    "    lambda example: {\"prediction\": nlp(example[\"text\"], [\"positive\", \"negative\"])}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-shot `concise-concepts`\n",
    "\n",
    "A more elaborate example of the usage of concise-concepts can be found in our [blogs](https://www.argilla.io/blog/concise-concepts-rubrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import concise_concepts\n",
    "\n",
    "# create some testdata\n",
    "data = {\n",
    "    \"fruit\": [\"apple\", \"pear\", \"orange\"],\n",
    "    \"vegetable\": [\"broccoli\", \"spinach\", \"tomato\"],\n",
    "    \"meat\": [\"beef\", \"pork\", \"fish\"],\n",
    "}\n",
    "text = \"Heat the oil in a large pan and add the Onion, celery and carrots.\"\n",
    "\n",
    "# load a spaCy concise-concepts pipeline\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"ner\"])\n",
    "nlp.add_pipe(\"concise_concepts\", config={\"data\": data, \"ent_score\": True})\n",
    "\n",
    "# deploy and monitor your model\n",
    "rg.monitor(nlp, dataset=\"concise-concepts-fruits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-shot `flair`\n",
    "\n",
    "We will use the NER dataset ‚ÄúWNUT 17: Emerging and Rare entity recognition‚Äù, which focuses on unusual, previously-unseen entities in the context of emerging discussions. This is the same dataset we use in our tutorial on [flair](../../tutorials/notebooks/labelling-tokenclassification-flair-fewshot.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from flair.models import TARSTagger\n",
    "from flair.data import Sentence\n",
    "\n",
    "# download dataset\n",
    "dataset = load_dataset(\"wnut_17\", split=\"test\")\n",
    "labels = [\"corporation\", \"creative-work\", \"group\", \"location\", \"person\", \"product\"]\n",
    "\n",
    "# load zero-shot NER tagger\n",
    "tars = TARSTagger.load(\"tars-ner\")\n",
    "tars.add_and_switch_to_new_task(\"task 1\", labels, label_type=\"ner\")\n",
    "\n",
    "# log data into Rubrix\n",
    "records = []\n",
    "for record in dataset.select(range(100)):\n",
    "    input_text = \" \".join(record[\"tokens\"])\n",
    "\n",
    "    sentence = Sentence(input_text)\n",
    "    tars.predict(sentence)\n",
    "    prediction = [\n",
    "        (entity.get_labels()[0].value, entity.start_position, entity.end_position)\n",
    "        for entity in sentence.get_spans(\"ner\")\n",
    "    ]\n",
    "\n",
    "    # building TokenClassificationRecord\n",
    "    records.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=input_text,\n",
    "            tokens=[token.text for token in sentence],\n",
    "            prediction=prediction,\n",
    "            prediction_agent=\"tars-ner\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# log the records to Argilla\n",
    "rg.log(records, name=\"tars_ner_wnut_17\", metadata={\"split\": \"test\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf5ecc3fecf17575e278cfc1ec050b1bf24f64b662a1b5421daed12bdd8255ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
