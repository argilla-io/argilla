{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020957ce-46fa-4799-8321-716e62900269",
   "metadata": {},
   "source": [
    "# ðŸ‘® Weak Supervision\n",
    "\n",
    "\n",
    "This guide gives you a brief introduction to weak supervision with Argilla.\n",
    "\n",
    "Argilla currently supports weak supervision for multi-class text classification use cases, but we'll be adding support for multilabel text classification and token classification (e.g., Named Entity Recognition) soon.\n",
    "\n",
    "![Labeling workflow](../../_static/images/guides/weak_supervision/weak_supervision.png \"Labeling workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb86995-f158-4392-86b7-eb5305a9ec3b",
   "metadata": {},
   "source": [
    "## Argilla weak supervision in a nutshell\n",
    "\n",
    "The recommended workflow for weak supervision is:\n",
    "\n",
    "- Log an unlabelled dataset into Argilla\n",
    "- Use the `Annotate` mode for hand- and/or bulk-labelling a test set. This test is key to measure the quality and performance of your rules.\n",
    "- Use the `Define rules` mode for testing and defining rules. Rules are defined with search queries (using ES query string DSL).\n",
    "- Use the Python client for reading rules, defining additional rules if needed, and train a label (for building a training set) or a downstream model (for building an end classifier).\n",
    "\n",
    "The next sections cover the main components of this workflow. \n",
    "\n",
    "### Weak labeling using the UI\n",
    "\n",
    "Since version 0.8.0 you can find and define rules directly in the UI. \n",
    "The [Define rules mode](../../reference/webapp/pages.html#metrics) is found in the right side bar of the [Dataset page](../../reference/webapp/pages.html#dataset).\n",
    "The video below shows how you can interactively find and save rules with the UI. \n",
    "\n",
    "### Weak supervision from Python\n",
    "\n",
    "Doing weak supervision with Argilla should be straightforward. Keeping the same spirit as other parts of the library, you can virtually use any weak supervision library or method, such as Snorkel or Flyingsquid. \n",
    "\n",
    "Argilla weak supervision support is built around two basic abstractions:\n",
    "\n",
    "\n",
    "### `Rule`\n",
    "A rule encodes an heuristic for labeling a record.\n",
    "\n",
    "Heuristics can be defined using [Elasticsearch's queries](../../reference/webapp/features.html#search-records):\n",
    "\n",
    "```python\n",
    "plz = Rule(query=\"plz OR please\", label=\"SPAM\")\n",
    "```\n",
    "\n",
    "or with Python functions (similar to Snorkel's labeling functions, which you can use as well):\n",
    "\n",
    "```python\n",
    "def contains_http(record: rg.TextClassificationRecord) -> Optional[str]:\n",
    "    if \"http\" in record.inputs[\"text\"]:\n",
    "        return \"SPAM\"\n",
    "```\n",
    "\n",
    "Besides textual features, Python labeling functions can exploit metadata features:\n",
    "\n",
    "```python\n",
    "def author_channel(record: rg.TextClassificationRecord) -> Optional[str]:\n",
    "    # the word channel appears in the comment author name\n",
    "    if \"channel\" in record.metadata[\"author\"]:\n",
    "        return \"SPAM\"\n",
    "```\n",
    "\n",
    "A rule should either return a string value, that is a weak label, or a `None` type in case of abstention.\n",
    "\n",
    "\n",
    "### `Weak Labels`\n",
    "\n",
    "Weak Labels objects bundle and apply a set of rules to the records of a Argilla dataset. Applying a rule to a record means assigning a weak label or abstaining.\n",
    "\n",
    "This abstraction provides you with the building blocks for training and testing weak supervision \"denoising\", \"label\" or even \"end\" models:\n",
    "\n",
    "```python\n",
    "rules = [contains_http, author_channel]\n",
    "weak_labels = WeakLabels(\n",
    "    rules=rules, \n",
    "    dataset=\"weak_supervision_yt\"\n",
    ")\n",
    "\n",
    "# returns a summary of the applied rules\n",
    "weak_labels.summary()\n",
    "```\n",
    "\n",
    "More information about these abstractions can be found in [the Python Labeling module docs](../../reference/python/python_labeling.rst).\n",
    "\n",
    "## Built-in label models\n",
    "\n",
    "To make things even easier for you, we provide wrapper classes around the most common label models, that directly consume a `WeakLabels` object.\n",
    "This makes working with those models a breeze.\n",
    "Take a look at the list of built-in models in the [labeling module docs](../../reference/python/python_labeling.rst).\n",
    "\n",
    "\n",
    "## Detailed Workflow\n",
    "\n",
    "A typical workflow to use weak supervision is:\n",
    "\n",
    "1. Create a Argilla dataset with your raw dataset. If you actually have some labelled data you can log it into the the same dataset.\n",
    "2. Define a set of weak labeling rules with the Rules definition mode in the UI.\n",
    "3. Create a `WeakLabels` object and apply the rules. You can load the rules from your dataset and add additional rules and labeling functions using Python. Typically, you'll iterate between this step and step 2.\n",
    "4. Once you are satisfied with your weak labels, use the matrix of the `WeakLabels` instance with your library/method of choice to build a training set or even train a downstream text classification model.\n",
    "\n",
    "\n",
    "This guide shows you an end-to-end example using Snorkel, Flyingsquid and Weasel. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52dd8f-fb8e-4909-9f6d-0a8856afa999",
   "metadata": {},
   "source": [
    "## Example dataset\n",
    "\n",
    "We'll be using a well-known dataset for weak supervision examples, the [YouTube Spam Collection](http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/) dataset, which is a binary classification task for detecting spam comments in Youtube videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b1e00af-c6f9-42aa-81aa-2976c9591b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Alessandro leite</td>\n",
       "      <td>2014-11-05T22:21:36</td>\n",
       "      <td>pls http://www10.vakinha.com.br/VaquinhaE.aspx...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Salim Tayara</td>\n",
       "      <td>2014-11-02T14:33:30</td>\n",
       "      <td>if your like drones, plz subscribe to Kamal Ta...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Phuc Ly</td>\n",
       "      <td>2014-01-20T15:27:47</td>\n",
       "      <td>go here to check the views :3ï»¿</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DropShotSk8r</td>\n",
       "      <td>2014-01-19T04:27:18</td>\n",
       "      <td>Came here to check the views, goodbye.ï»¿</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>css403</td>\n",
       "      <td>2014-11-07T14:25:48</td>\n",
       "      <td>i am 2,126,492,636 viewer :Dï»¿</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            author                 date  \\\n",
       "0           0  Alessandro leite  2014-11-05T22:21:36   \n",
       "1           1      Salim Tayara  2014-11-02T14:33:30   \n",
       "2           2           Phuc Ly  2014-01-20T15:27:47   \n",
       "3           3      DropShotSk8r  2014-01-19T04:27:18   \n",
       "4           4            css403  2014-11-07T14:25:48   \n",
       "\n",
       "                                                text  label  video  \n",
       "0  pls http://www10.vakinha.com.br/VaquinhaE.aspx...   -1.0      1  \n",
       "1  if your like drones, plz subscribe to Kamal Ta...   -1.0      1  \n",
       "2                     go here to check the views :3ï»¿   -1.0      1  \n",
       "3            Came here to check the views, goodbye.ï»¿   -1.0      1  \n",
       "4                      i am 2,126,492,636 viewer :Dï»¿   -1.0      1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "train_df = pd.read_csv(\"../../tutorials/notebooks/data/yt_comments_train.csv\")\n",
    "test_df = pd.read_csv(\"../../tutorials/notebooks/data/yt_comments_test.csv\")\n",
    "\n",
    "# preview data\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fcd253-f8b5-40ba-a9a2-9d068e19c1cc",
   "metadata": {},
   "source": [
    "## 1. Create a Argilla dataset with unlabelled data and test data\n",
    "\n",
    "Let's load the train (non-labelled) and the test (containing labels) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78571aa4-314d-4b4e-b933-c3292213b27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2231871634c4b648c4fc489239ca3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1836 records logged to http://localhost:6900/datasets/argilla/weak_supervision_yt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='weak_supervision_yt', processed=1836, failed=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# build records from the train dataset\n",
    "records = [\n",
    "    rg.TextClassificationRecord(\n",
    "        text=row.text, metadata={\"video\": row.video, \"author\": row.author}\n",
    "    )\n",
    "    for i, row in train_df.iterrows()\n",
    "]\n",
    "\n",
    "# build records from the test dataset with annotation\n",
    "labels = [\"HAM\", \"SPAM\"]\n",
    "records += [\n",
    "    rg.TextClassificationRecord(\n",
    "        text=row.text,\n",
    "        annotation=labels[row.label],\n",
    "        metadata={\"video\": row.video, \"author\": row.author},\n",
    "    )\n",
    "    for i, row in test_df.iterrows()\n",
    "]\n",
    "\n",
    "# log records to Argilla\n",
    "rg.log(records, name=\"weak_supervision_yt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafda4f-45c0-49d6-9c37-7473c6888ebe",
   "metadata": {},
   "source": [
    "After this step, you have a fully browsable dataset available that you can access via the [Argilla web app](../../reference/webapp/index.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde95ce0-6e1e-4c9e-aff2-ac12643b9a48",
   "metadata": {},
   "source": [
    "## 2. Defining rules\n",
    "\n",
    "Let's now define some of the rules proposed in the tutorial [Snorkel Intro Tutorial: Data Labeling](https://www.snorkel.org/use-cases/01-spam-tutorial). \n",
    "Most of these rules can be defined directly with our web app in the [Define rules mode](../../reference/webapp/define_rules.md) and [Elasticsearch's query strings](../../reference/webapp/features.html#search-records). \n",
    "Afterward, you can conveniently load them into your notebook with the [load_rules function](../../reference/python/python_labeling.rst).\n",
    "\n",
    "Rules can also be defined programmatically as shown below. Depending on your use case and team structure you can mix and match both interfaces (UI or Python).\n",
    "\n",
    "Let's see here some programmatic rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045700ef-62b7-44e2-95f9-0f966236303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.labeling.text_classification import Rule, WeakLabels\n",
    "\n",
    "#  rules defined as Elasticsearch queries\n",
    "check_out = Rule(query=\"check out\", label=\"SPAM\")\n",
    "plz = Rule(query=\"plz OR please\", label=\"SPAM\")\n",
    "subscribe = Rule(query=\"subscribe\", label=\"SPAM\")\n",
    "my = Rule(query=\"my\", label=\"SPAM\")\n",
    "song = Rule(query=\"song\", label=\"HAM\")\n",
    "love = Rule(query=\"love\", label=\"HAM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d4d7f-9a95-4905-a38b-3907d5293c4f",
   "metadata": {},
   "source": [
    "You can also define plain Python labeling functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7b3554c-9646-4f2b-ab33-13518566bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# rules defined as Python labeling functions\n",
    "def contains_http(record: rg.TextClassificationRecord):\n",
    "    if \"http\" in record.inputs[\"text\"]:\n",
    "        return \"SPAM\"\n",
    "\n",
    "\n",
    "def short_comment(record: rg.TextClassificationRecord):\n",
    "    return \"HAM\" if len(record.inputs[\"text\"].split()) < 5 else None\n",
    "\n",
    "\n",
    "def regex_check_out(record: rg.TextClassificationRecord):\n",
    "    return (\n",
    "        \"SPAM\" if re.search(r\"check.*out\", record.inputs[\"text\"], flags=re.I) else None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6eba2",
   "metadata": {},
   "source": [
    "You can load your predefined rules and convert them to Rule instances, and add them to dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fa36602",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling_rules_df = pd.read_csv(\"../../tutorials/notebooks/labeling_rules.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de8bf31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>your</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rich</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>film</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>meeting</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>help</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    query label\n",
       "0           0     your  SPAM\n",
       "1           1     rich  SPAM\n",
       "2           2     film   HAM\n",
       "3           3  meeting   HAM\n",
       "4           4     help   HAM"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeling_rules_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0712bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "predefined_labeling_rules = []\n",
    "for index, row in labeling_rules_df.iterrows():\n",
    "    predefined_labeling_rules.append(\n",
    "        Rule(row[\"query\"], row[\"label\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c93ec-b136-43cf-af50-96c956b65f12",
   "metadata": {},
   "source": [
    "## 3. Building and analyzing weak labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caaf60f4-c5f5-492d-ac0d-52a25cfb5ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e618e6e8fdd44c42bb8b51bf06b3806c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing rules:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d190ad6f88d4231875d0eae3454019a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying rules:   0%|          | 0/3672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from argilla.labeling.text_classification import load_rules, add_rules\n",
    "\n",
    "# bundle our rules in a list\n",
    "rules = [\n",
    "    check_out,\n",
    "    plz,\n",
    "    subscribe,\n",
    "    my,\n",
    "    song,\n",
    "    love\n",
    "]\n",
    "\n",
    "labeling_functions = [  \n",
    "    contains_http,\n",
    "    short_comment,\n",
    "    regex_check_out\n",
    "]\n",
    "\n",
    "# add rules to dataset\n",
    "add_rules(dataset=\"weak_supervision_yt\", rules=rules)\n",
    "\n",
    "\n",
    "# add the predefined rules loaded from external file\n",
    "add_rules(dataset=\"weak_supervision_yt\", rules=predefined_labeling_rules)\n",
    "\n",
    "# load all the rules available in the dataset including interactively defined in the UI \n",
    "dataset_labeling_rules = load_rules(dataset=\"weak_supervision_yt\")\n",
    "\n",
    "# extend the labeling rules with labeling functions\n",
    "dataset_labeling_rules.extend(labeling_functions)\n",
    "\n",
    "# apply the final rules to the dataset\n",
    "weak_labels = WeakLabels(dataset=\"weak_supervision_yt\", rules=dataset_labeling_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0584b8b-4b0d-4857-9e8b-9823d9172634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>coverage</th>\n",
       "      <th>annotated_coverage</th>\n",
       "      <th>overlaps</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check out</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.224401</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.224401</td>\n",
       "      <td>0.031590</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plz OR please</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.104575</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.036492</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subscribe</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.082244</td>\n",
       "      <td>0.031590</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.192810</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.168845</td>\n",
       "      <td>0.062636</td>\n",
       "      <td>84</td>\n",
       "      <td>12</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>song</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.118192</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.070806</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>0.790698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.090959</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.071351</td>\n",
       "      <td>0.034858</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.052832</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.041939</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rich</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>{}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meeting</th>\n",
       "      <td>{}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.023965</td>\n",
       "      <td>0.023965</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_http</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.106209</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.245098</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.101307</td>\n",
       "      <td>0.064270</td>\n",
       "      <td>168</td>\n",
       "      <td>16</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.226580</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.226035</td>\n",
       "      <td>0.032135</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>{HAM, SPAM}</td>\n",
       "      <td>0.762527</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.458061</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>708</td>\n",
       "      <td>84</td>\n",
       "      <td>0.893939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       label  coverage  annotated_coverage  overlaps  \\\n",
       "check out             {SPAM}  0.224401               0.176  0.224401   \n",
       "plz OR please         {SPAM}  0.104575               0.088  0.098039   \n",
       "subscribe             {SPAM}  0.101852               0.120  0.082244   \n",
       "my                    {SPAM}  0.192810               0.192  0.168845   \n",
       "song                   {HAM}  0.118192               0.172  0.070806   \n",
       "love                   {HAM}  0.090959               0.140  0.071351   \n",
       "your                  {SPAM}  0.052832               0.088  0.041939   \n",
       "rich                  {SPAM}  0.000545               0.000  0.000000   \n",
       "film                      {}  0.000000               0.000  0.000000   \n",
       "meeting                   {}  0.000000               0.000  0.000000   \n",
       "help                   {HAM}  0.027778               0.036  0.023965   \n",
       "contains_http         {SPAM}  0.106209               0.024  0.078431   \n",
       "short_comment          {HAM}  0.245098               0.368  0.101307   \n",
       "regex_check_out       {SPAM}  0.226580               0.180  0.226035   \n",
       "total            {HAM, SPAM}  0.762527               0.880  0.458061   \n",
       "\n",
       "                 conflicts  correct  incorrect  precision  \n",
       "check out         0.031590       88          0   1.000000  \n",
       "plz OR please     0.036492       44          0   1.000000  \n",
       "subscribe         0.031590       60          0   1.000000  \n",
       "my                0.062636       84         12   0.875000  \n",
       "song              0.037037       68         18   0.790698  \n",
       "love              0.034858       56         14   0.800000  \n",
       "your              0.019608       38          6   0.863636  \n",
       "rich              0.000000        0          0        NaN  \n",
       "film              0.000000        0          0        NaN  \n",
       "meeting           0.000000        0          0        NaN  \n",
       "help              0.023965        0         18   0.000000  \n",
       "contains_http     0.055556       12          0   1.000000  \n",
       "short_comment     0.064270      168         16   0.913043  \n",
       "regex_check_out   0.032135       90          0   1.000000  \n",
       "total             0.147059      708         84   0.893939  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show some stats about the rules, see the `summary()` docstring for details\n",
    "weak_labels.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf263ad",
   "metadata": {},
   "source": [
    "You can remove the rules which are wrong from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49a6806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_informative_rules = [\n",
    "    Rule(\"rich\", \"SPAM\"),\n",
    "    Rule(\"film\", \"HAM\"),\n",
    "    Rule(\"meeting\", \"HAM\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e33e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.labeling.text_classification import delete_rules\n",
    "delete_rules(dataset=\"weak_supervision_yt\", rules=not_informative_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd384863",
   "metadata": {},
   "source": [
    "You can update the rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc90c539",
   "metadata": {},
   "source": [
    "help\t{HAM}\t0.027778\t0.036\t0.023965\t0.023965\t0\t18\t0.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e664fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "help_rule = Rule(\"help\", label=\"SPAM\")\n",
    "help_rule.update_at_dataset(dataset=\"weak_supervision_yt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9d3f9",
   "metadata": {},
   "source": [
    "Lets load the rules again and apply weak labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07c52c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rules = labeling_functions + load_rules(dataset=\"weak_supervision_yt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b9bc0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61986dd3ce504b68817db68690a1057d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing rules:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bcf5b33fea4cbba0bfeac426e74d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying rules:   0%|          | 0/3672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weak_labels = WeakLabels(dataset=\"weak_supervision_yt\", rules=final_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbb7978a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>coverage</th>\n",
       "      <th>annotated_coverage</th>\n",
       "      <th>overlaps</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>contains_http</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.106209</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.245098</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.101307</td>\n",
       "      <td>0.064270</td>\n",
       "      <td>168</td>\n",
       "      <td>16</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.226580</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.226035</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check out</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.224401</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.224401</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plz OR please</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.104575</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.023420</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subscribe</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.082244</td>\n",
       "      <td>0.025054</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.192810</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.168845</td>\n",
       "      <td>0.050654</td>\n",
       "      <td>84</td>\n",
       "      <td>12</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>song</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.118192</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.070806</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>0.790698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.090959</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.071351</td>\n",
       "      <td>0.034858</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.052832</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.041939</td>\n",
       "      <td>0.015795</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.023965</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>{HAM, SPAM}</td>\n",
       "      <td>0.761983</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.458061</td>\n",
       "      <td>0.126906</td>\n",
       "      <td>726</td>\n",
       "      <td>66</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       label  coverage  annotated_coverage  overlaps  \\\n",
       "contains_http         {SPAM}  0.106209               0.024  0.078431   \n",
       "short_comment          {HAM}  0.245098               0.368  0.101307   \n",
       "regex_check_out       {SPAM}  0.226580               0.180  0.226035   \n",
       "check out             {SPAM}  0.224401               0.176  0.224401   \n",
       "plz OR please         {SPAM}  0.104575               0.088  0.098039   \n",
       "subscribe             {SPAM}  0.101852               0.120  0.082244   \n",
       "my                    {SPAM}  0.192810               0.192  0.168845   \n",
       "song                   {HAM}  0.118192               0.172  0.070806   \n",
       "love                   {HAM}  0.090959               0.140  0.071351   \n",
       "your                  {SPAM}  0.052832               0.088  0.041939   \n",
       "help                  {SPAM}  0.027778               0.036  0.023965   \n",
       "total            {HAM, SPAM}  0.761983               0.880  0.458061   \n",
       "\n",
       "                 conflicts  correct  incorrect  precision  \n",
       "contains_http     0.049020       12          0   1.000000  \n",
       "short_comment     0.064270      168         16   0.913043  \n",
       "regex_check_out   0.027778       90          0   1.000000  \n",
       "check out         0.027778       88          0   1.000000  \n",
       "plz OR please     0.023420       44          0   1.000000  \n",
       "subscribe         0.025054       60          0   1.000000  \n",
       "my                0.050654       84         12   0.875000  \n",
       "song              0.037037       68         18   0.790698  \n",
       "love              0.034858       56         14   0.800000  \n",
       "your              0.015795       38          6   0.863636  \n",
       "help              0.003813       18          0   1.000000  \n",
       "total             0.126906      726         66   0.916667  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4364c17-4a5d-464b-8f2b-36f50c81b174",
   "metadata": {},
   "source": [
    "## 4. Using the weak labels\n",
    "\n",
    "At this step you have at least two options:\n",
    "\n",
    "1. Use the weak labels for training a \"denoising\" or label model to build a less noisy training set. Highly popular options for this are [Snorkel](https://snorkel.org/) or [Flyingsquid](https://github.com/HazyResearch/flyingsquid). After this step, you can train a downstream model with the \"clean\" labels.\n",
    "\n",
    "2. Use the weak labels directly with recent \"end-to-end\" (e.g., [Weasel](https://github.com/autonlab/weasel)) or joint models (e.g., [COSINE](https://github.com/yueyu1030/COSINE)).\n",
    "\n",
    "\n",
    "Let's see some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab5f3a-f158-4090-9933-3615435256a5",
   "metadata": {},
   "source": [
    "### A simple majority vote\n",
    "\n",
    "As a first example we will show you, how to use the `WeakLabels` object together with a simple majority vote model, which is arguably the most straightforward label model.\n",
    "On a per-record basis, it simply counts the votes for each label returned by the rules, and takes the majority vote.\n",
    "Argilla provides a neat implementation of this logic in its `MajorityVoter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7eb42d90-b2ec-4b26-ae0c-165b26a87458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.labeling.text_classification import MajorityVoter\n",
    "\n",
    "# instantiate the majority vote label model by simply providing the weak labels object\n",
    "majority_model = MajorityVoter(weak_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee31a144-a426-4d27-a7aa-13cc1c8dc3bf",
   "metadata": {},
   "source": [
    "In contrast to the other label models we will discuss further down, the majority voter does not need to be fitted.\n",
    "You can directly check its performance by simply calling its `score()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5acfce87-56e7-4f2f-8572-de797a22938d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HAM       0.94      0.99      0.96       216\n",
      "        SPAM       0.99      0.93      0.96       204\n",
      "\n",
      "    accuracy                           0.96       420\n",
      "   macro avg       0.96      0.96      0.96       420\n",
      "weighted avg       0.96      0.96      0.96       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check its performance\n",
    "print(majority_model.score(output_str=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb83339-8909-4c44-b032-a2e6208f49f9",
   "metadata": {},
   "source": [
    "An accuracy of 0.96 seems surprisingly high, but you need to keep in mind that we simply excluded the records from the evaluation, for which the model abstained (that is a tie in the votes or no votes at all).\n",
    "So let's account for this and correct the accuracy by assuming the model performs like a random classifier for these abstained records:\n",
    "\n",
    "> $accuracy_c = frac_{non} \\times accuracy + frac_{abs} \\times accuracy_{random}$\n",
    "\n",
    "where $frac_{non}$ is the fraction of non-abstained records and $frac_{abs}$ the fraction of abstained records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78061952-6569-4241-9bc4-87caf6ad3e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_c: 0.6839999999999999\n"
     ]
    }
   ],
   "source": [
    "# calculate fractions using the support metric (see above)\n",
    "frac_non = 200 / len(weak_labels.annotation())\n",
    "frac_abs = 1 - (200 / len(weak_labels.annotation()))\n",
    "\n",
    "# accuracy without abstentions: 0.96; accuracy of random classifier: 0.5\n",
    "print(\"accuracy_c:\", frac_non * 0.96 + frac_abs * 0.5)\n",
    "# accuracy_c: 0.868\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dcbe2e-8078-4d81-8cb2-a2bb621b4d34",
   "metadata": {},
   "source": [
    "As we will see further down, **an accuracy of 0.868** is still a very decent baseline.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "To get a noisy estimate of the corrected accuracy, you can also set the _\"tie_break_policy\"_ argument: `majority_model.score(..., tie_break_policy=\"random\")`.\n",
    "    \n",
    "</div>\n",
    "\n",
    "When predicting weak labels to train a down-stream model, however, you probably want to discard the abstentions.\n",
    "Calling the `predict()` method on the majority voter, excludes the abstentions by default and only returns records without annotations.\n",
    "These are normally used to build a training set for a downstream model.\n",
    "\n",
    "You can quickly explore the predicted records with Argilla, before building a training set for training a downstream text classifier. \n",
    "This step is useful for validation, manual revision, or defining score thresholds for accepting labels from your label model (for example, only considering labels with a score greater then 0.8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d745e9f4-d445-422e-a45d-371a086a3aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71064e643fd5441d84f1751ebd10f0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106 records logged to http://localhost:6900/datasets/argilla/majority_voter_results\n"
     ]
    }
   ],
   "source": [
    "# get your training records with the predictions of the label model\n",
    "records_for_training = majority_model.predict()\n",
    "\n",
    "# optional: log the records to a new dataset in Argilla\n",
    "rg.log(records_for_training, name=\"majority_voter_results\")\n",
    "\n",
    "# extract training data\n",
    "training_data = pd.DataFrame(\n",
    "    [{\"text\": rec.text, \"label\": rec.prediction[0][0]} for rec in records_for_training]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "250d800e-85ba-4cc5-9fcf-a7105f94fbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this song is better then monster by eminemï»¿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys! My mom said if i got 100 subs before...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi everyone this is cool check out sexy and i ...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love this shit but I disliked it because it'...</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For all you ladies out there......  Check out ...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>awesomeï»¿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>Goodï»¿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2103</th>\n",
       "      <td>Check out this video on YouTube:ï»¿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>hey guys look im aware im spamming and it piss...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>Subscribe to My CHANNELï»¿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2106 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label\n",
       "0           this song is better then monster by eminemï»¿   HAM\n",
       "1     Hey guys! My mom said if i got 100 subs before...  SPAM\n",
       "2     hi everyone this is cool check out sexy and i ...  SPAM\n",
       "3     I love this shit but I disliked it because it'...   HAM\n",
       "4     For all you ladies out there......  Check out ...  SPAM\n",
       "...                                                 ...   ...\n",
       "2101                                           awesomeï»¿   HAM\n",
       "2102                                              Goodï»¿   HAM\n",
       "2103                  Check out this video on YouTube:ï»¿  SPAM\n",
       "2104  hey guys look im aware im spamming and it piss...  SPAM\n",
       "2105                           Subscribe to My CHANNELï»¿  SPAM\n",
       "\n",
       "[2106 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview training data\n",
    "training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186a612-57a4-4118-97fb-927ed87df96d",
   "metadata": {},
   "source": [
    "### Label model with Snorkel\n",
    "\n",
    "Snorkel's label model is by far the most popular option for using weak supervision, and Argilla provides built-in support for it. \n",
    "Using Snorkel with Argilla's `WeakLabels` is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0af30dfa-b401-4e96-984d-c59753411557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install snorkel -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eccb50bb-edfb-40a2-9e4c-094e22284df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 2798.11epoch/s]\n"
     ]
    }
   ],
   "source": [
    "from argilla.labeling.text_classification import Snorkel\n",
    "\n",
    "# we pass our WeakLabels instance to our Snorkel label model\n",
    "snorkel_model = Snorkel(weak_labels)\n",
    "\n",
    "# we fit the model\n",
    "snorkel_model.fit(lr=0.001, n_epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9947561-f177-46cd-a965-bf78ed11cd0a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "The `Snorkel` label model is not suited for multi-label classification tasks and does not support them.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6346d-1a34-4efa-87de-f3e69b7550ca",
   "metadata": {},
   "source": [
    "When fitting the snorkel model, we recommend performing a quick grid search for the learning rate `lr` and the number of epochs `n_epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c82339f-561d-4c59-89ee-c49ddbe8da83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HAM       0.94      0.94      0.94       228\n",
      "        SPAM       0.93      0.93      0.93       212\n",
      "\n",
      "    accuracy                           0.94       440\n",
      "   macro avg       0.94      0.94      0.94       440\n",
      "weighted avg       0.94      0.94      0.94       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we check its performance\n",
    "print(snorkel_model.score(output_str=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3e81f-e1ac-4203-8641-e721981aa0a6",
   "metadata": {},
   "source": [
    "At first sight, the model seems to perform worse than the majority vote baseline.\n",
    "However, let's again correct the accuracy for the abstentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a9700f2-5e88-4926-8f2f-d3acada5c166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_c: 0.6880999999999999\n"
     ]
    }
   ],
   "source": [
    "# calculate fractions using the support metric (see above)\n",
    "frac_non = 209 / len(weak_labels.annotation())\n",
    "frac_abs = 1 - (209 / len(weak_labels.annotation()))\n",
    "\n",
    "# accuracy without abstentions: 0.95; accuracy of random classifier: 0.5\n",
    "print(\"accuracy_c:\", frac_non * 0.95 + frac_abs * 0.5)\n",
    "# accuracy_c: 0.8761999999999999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075463f3-8695-4d28-af24-8860b2efd691",
   "metadata": {},
   "source": [
    "Now we can see that with **an accuracy of 0.876**, its performance over the whole test set is actually slightly better.\n",
    "\n",
    "After fitting your label model, you can quickly explore its predictions, before building a training set for training a downstream text classifier. \n",
    "This step is useful for validation, manual revision, or defining score thresholds for accepting labels from your label model (for example, only considering labels with a score greater then 0.8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cba3fd76-8b58-4ae3-bc58-9d82c039bef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab533a7dfd54f2685cb1bac978df994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2358 records logged to http://localhost:6900/datasets/argilla/snorkel_results\n"
     ]
    }
   ],
   "source": [
    "# get your training records with the predictions of the label model\n",
    "records_for_training = snorkel_model.predict()\n",
    "\n",
    "# optional: log the records to a new dataset in Argilla\n",
    "rg.log(records_for_training, name=\"snorkel_results\")\n",
    "\n",
    "# extract training data\n",
    "training_data = pd.DataFrame(\n",
    "    [{\"text\": rec.text, \"label\": rec.prediction[0][0]} for rec in records_for_training]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e18b365-5412-47c7-b0e6-e140bd0c2dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this song is better then monster by eminemï»¿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys! My mom said if i got 100 subs before...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi everyone this is cool check out sexy and i ...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my favorite songï»¿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love this shit but I disliked it because it'...</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>Check out this video on YouTube:ï»¿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>http://woobox.com/33gxrf/brt0u5 FREE CS GO!!!!ï»¿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>hey guys look im aware im spamming and it piss...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>Subscribe to My CHANNELï»¿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2357</th>\n",
       "      <td>http://www.twitch.tv/daconnormcï»¿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2358 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label\n",
       "0           this song is better then monster by eminemï»¿   HAM\n",
       "1     Hey guys! My mom said if i got 100 subs before...  SPAM\n",
       "2     hi everyone this is cool check out sexy and i ...  SPAM\n",
       "3                                     my favorite songï»¿  SPAM\n",
       "4     I love this shit but I disliked it because it'...   HAM\n",
       "...                                                 ...   ...\n",
       "2353                  Check out this video on YouTube:ï»¿  SPAM\n",
       "2354    http://woobox.com/33gxrf/brt0u5 FREE CS GO!!!!ï»¿   HAM\n",
       "2355  hey guys look im aware im spamming and it piss...  SPAM\n",
       "2356                           Subscribe to My CHANNELï»¿  SPAM\n",
       "2357                   http://www.twitch.tv/daconnormcï»¿   HAM\n",
       "\n",
       "[2358 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview training data\n",
    "training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0123c-a7a5-4a35-82fc-a5a616cd5000",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "For an example of how to use the `WeakLabels` object with Snorkel's raw `LabelModel` class, you can check out the [WeakLabels reference](../../reference/python/python_labeling.rst).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f69fc-7f2f-4b57-8262-a0a6c2f92329",
   "metadata": {},
   "source": [
    "### Label model with FlyingSquid\n",
    "\n",
    "FlyingSquid is a powerful method developed by [Hazy Research](https://hazyresearch.stanford.edu/), a research group from Stanford behind ground-breaking work on programmatic data labeling, including Snorkel.\n",
    "FlyingSquid uses a closed-form solution for fitting the label model with great speed gains and similar performance.\n",
    "Just like for Snorkel, Argilla provides built-in support for FlyingSquid, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc55482c-02f6-4e7a-8637-0aa50e4e9211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install flyingsquid pgmpy -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5ede78a-0fe6-4f2b-87b1-3d0f06f0c3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pgmpy\\models\\MarkovModel.py:8: FutureWarning: MarkovModel has been renamed to MarkovNetwork. Please use MarkovNetwork class, MarkovModel will be removed in future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from argilla.labeling.text_classification import FlyingSquid\n",
    "\n",
    "# we pass our WeakLabels instance to our FlyingSquid label model\n",
    "flyingsquid_model = FlyingSquid(weak_labels)\n",
    "\n",
    "# we fit the model\n",
    "flyingsquid_model.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b834d9-ca48-480f-89ee-493dd974ba95",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "The `FlyingSquid` label model is not suited for multi-label classification tasks and does not support them.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d81dbf5-edf9-4c8c-a876-5fcb16d73090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HAM       0.94      0.92      0.93       228\n",
      "        SPAM       0.92      0.93      0.93       212\n",
      "\n",
      "    accuracy                           0.93       440\n",
      "   macro avg       0.93      0.93      0.93       440\n",
      "weighted avg       0.93      0.93      0.93       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we check its performance\n",
    "print(flyingsquid_model.score(output_str=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b287726d-c4c6-424b-9ed8-e8453702744c",
   "metadata": {},
   "source": [
    "Again, let's correct the accuracy for the abstentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4641b855-ef30-4d84-a917-532a7ba12be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_c: 0.67974\n"
     ]
    }
   ],
   "source": [
    "# calculate fractions using the support metric (see above)\n",
    "frac_non = 209 / len(weak_labels.annotation())\n",
    "frac_abs = 1 - (209 / len(weak_labels.annotation()))\n",
    "\n",
    "# accuracy without abstentions: 0.93; accuracy of random classifier: 0.5\n",
    "print(\"accuracy_c:\", frac_non * 0.93 + frac_abs * 0.5)\n",
    "# accuracy_c: 0.85948\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678757c6-9ceb-46f2-949f-5fd10cb8685b",
   "metadata": {},
   "source": [
    "Here, it really seems that with **an accuracy of 0.859**, the performance over the whole test set is actually slightly worse than the baseline of the majority vote.\n",
    "\n",
    "After fitting your label model, you can quickly explore its predictions, before building a training set for training a downstream text classifier. \n",
    "This step is useful for validation, manual revision, or defining score thresholds for accepting labels from your label model (for example, only considering labels with a score greater then 0.8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff30478c-fdda-445f-ad49-cac01b8bde71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74028fd44974455fbe935b50e2818202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2358 records logged to http://localhost:6900/datasets/argilla/flyingsquid_results\n"
     ]
    }
   ],
   "source": [
    "# get your training records with the predictions of the label model\n",
    "records_for_training = flyingsquid_model.predict()\n",
    "\n",
    "# log the records to a new dataset in Argilla\n",
    "rg.log(records_for_training, name=\"flyingsquid_results\")\n",
    "\n",
    "# extract training data\n",
    "training_data = pd.DataFrame(\n",
    "    [{\"text\": rec.text, \"label\": rec.prediction[0][0]} for rec in records_for_training]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d641340-f82b-4af8-b86b-7c06eaf59f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this song is better then monster by eminemï»¿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys! My mom said if i got 100 subs before...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi everyone this is cool check out sexy and i ...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my favorite songï»¿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love this shit but I disliked it because it'...</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>Check out this video on YouTube:ï»¿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>http://woobox.com/33gxrf/brt0u5 FREE CS GO!!!!ï»¿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>hey guys look im aware im spamming and it piss...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>Subscribe to My CHANNELï»¿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2357</th>\n",
       "      <td>http://www.twitch.tv/daconnormcï»¿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2358 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label\n",
       "0           this song is better then monster by eminemï»¿   HAM\n",
       "1     Hey guys! My mom said if i got 100 subs before...  SPAM\n",
       "2     hi everyone this is cool check out sexy and i ...  SPAM\n",
       "3                                     my favorite songï»¿  SPAM\n",
       "4     I love this shit but I disliked it because it'...   HAM\n",
       "...                                                 ...   ...\n",
       "2353                  Check out this video on YouTube:ï»¿  SPAM\n",
       "2354    http://woobox.com/33gxrf/brt0u5 FREE CS GO!!!!ï»¿  SPAM\n",
       "2355  hey guys look im aware im spamming and it piss...  SPAM\n",
       "2356                           Subscribe to My CHANNELï»¿  SPAM\n",
       "2357                   http://www.twitch.tv/daconnormcï»¿  SPAM\n",
       "\n",
       "[2358 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview training data\n",
    "training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cec1b4-97c8-45a0-9fc4-06e663b9ee4d",
   "metadata": {},
   "source": [
    "### Joint Model with Weasel\n",
    "\n",
    "[Weasel](https://github.com/autonlab/weasel) lets you train downstream models end-to-end using directly weak labels.\n",
    "In contrast to Snorkel or FlyingSquid, which are two-stage approaches, Weasel is a one-stage method that jointly trains the label and the end model at the same time.\n",
    "For more details check out the [End-to-End Weak Supervision paper](https://arxiv.org/abs/2107.02233) presented at NeurIPS 2021.\n",
    "\n",
    "In this guide we will show you, how you can **train a Hugging Face transformers** model directly **with weak labels using Weasel**.\n",
    "Since Weasel uses [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) for the training, some basic knowledge of PyTorch is helpful, but not strictly necessary.\n",
    "\n",
    "Let's start with installing the Weasel python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1ac1d61-5a40-44dc-a340-46bbdb8852eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting weasel[all]\n",
      "  Cloning https://github.com/autonlab/weasel to c:\\users\\ufukh\\appdata\\local\\temp\\pip-install-4zxnfbo0\\weasel_39696956984e47bfbd9af0f94b6cab25\n",
      "  Resolved https://github.com/autonlab/weasel to commit c6bccc5cb919b9cc6f4296a0ac34fcd128f628d4\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch>=1.7.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from weasel[all]) (1.12.1)\n",
      "Requirement already satisfied: pytorch-lightning>=1.3.2 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from weasel[all]) (1.8.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from weasel[all]) (1.1.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from weasel[all]) (6.0)\n",
      "Requirement already satisfied: wandb>=0.10.30 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from weasel[all]) (0.13.5)\n",
      "Requirement already satisfied: hydra-core>=1.1.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from weasel[all]) (1.2.0)\n",
      "Requirement already satisfied: ipython in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from weasel[all]) (8.4.0)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from weasel[all]) (6.15.2)\n",
      "Requirement already satisfied: jupyter in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from weasel[all]) (1.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from weasel[all]) (3.6.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from weasel[all]) (0.12.1)\n",
      "Requirement already satisfied: rich in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from weasel[all]) (12.6.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from hydra-core>=1.1.0->weasel[all]) (4.9.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from hydra-core>=1.1.0->weasel[all]) (21.3)\n",
      "Requirement already satisfied: omegaconf~=2.2 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from hydra-core>=1.1.0->weasel[all]) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from pytorch-lightning>=1.3.2->weasel[all]) (1.23.4)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from pytorch-lightning>=1.3.2->weasel[all]) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from pytorch-lightning>=1.3.2->weasel[all]) (4.64.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from pytorch-lightning>=1.3.2->weasel[all]) (2.10.1)\n",
      "Requirement already satisfied: lightning-utilities==0.3.* in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from pytorch-lightning>=1.3.2->weasel[all]) (0.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from pytorch-lightning>=1.3.2->weasel[all]) (4.3.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from pytorch-lightning>=1.3.2->weasel[all]) (2022.10.0)\n",
      "Requirement already satisfied: fire in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from lightning-utilities==0.3.*->pytorch-lightning>=1.3.2->weasel[all]) (0.4.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (5.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (59.5.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (1.11.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (2.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (8.0.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (0.4.0)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (1.16.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (1.0.11)\n",
      "Requirement already satisfied: pathtools in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (0.1.2)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (3.19.6)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (1.3.2)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (3.1.29)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from wandb>=0.10.30->weasel[all]) (2.28.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipykernel->weasel[all]) (6.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipykernel->weasel[all]) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipykernel->weasel[all]) (1.5.6)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipykernel->weasel[all]) (24.0.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipykernel->weasel[all]) (1.5.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipykernel->weasel[all]) (7.4.3)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipykernel->weasel[all]) (5.5.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipython->weasel[all]) (5.1.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipython->weasel[all]) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipython->weasel[all]) (3.0.20)\n",
      "Requirement already satisfied: stack-data in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipython->weasel[all]) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipython->weasel[all]) (0.4.5)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipython->weasel[all]) (0.18.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipython->weasel[all]) (2.11.2)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipython->weasel[all]) (0.7.5)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from jupyter->weasel[all]) (8.0.2)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from jupyter->weasel[all]) (6.4.4)\n",
      "Requirement already satisfied: notebook in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from jupyter->weasel[all]) (6.4.12)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from jupyter->weasel[all]) (7.2.2)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from jupyter->weasel[all]) (5.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from matplotlib->weasel[all]) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from matplotlib->weasel[all]) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from matplotlib->weasel[all]) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from matplotlib->weasel[all]) (4.37.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from matplotlib->weasel[all]) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from matplotlib->weasel[all]) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from matplotlib->weasel[all]) (1.0.5)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from rich->weasel[all]) (0.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from scikit-learn->weasel[all]) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from scikit-learn->weasel[all]) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from scikit-learn->weasel[all]) (1.9.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from seaborn->weasel[all]) (1.5.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning>=1.3.2->weasel[all]) (3.8.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from GitPython>=1.0.0->wandb>=0.10.30->weasel[all]) (4.0.9)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from jedi>=0.16->ipython->weasel[all]) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->weasel[all]) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->weasel[all]) (4.11.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from pandas>=0.25->seaborn->weasel[all]) (2022.5)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->weasel[all]) (0.2.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from requests<3,>=2.0.0->wandb>=0.10.30->weasel[all]) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from requests<3,>=2.0.0->wandb>=0.10.30->weasel[all]) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from requests<3,>=2.0.0->wandb>=0.10.30->weasel[all]) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from requests<3,>=2.0.0->wandb>=0.10.30->weasel[all]) (3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (3.4.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (2.13.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (1.50.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (1.3.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipywidgets->jupyter->weasel[all]) (3.0.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from ipywidgets->jupyter->weasel[all]) (4.0.3)\n",
      "Requirement already satisfied: bleach in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbconvert->jupyter->weasel[all]) (5.0.1)\n",
      "Requirement already satisfied: nbformat>=5.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbconvert->jupyter->weasel[all]) (5.7.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbconvert->jupyter->weasel[all]) (1.2.1)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbconvert->jupyter->weasel[all]) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbconvert->jupyter->weasel[all]) (0.2.2)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbconvert->jupyter->weasel[all]) (3.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbconvert->jupyter->weasel[all]) (5.0.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbconvert->jupyter->weasel[all]) (0.7.0)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbconvert->jupyter->weasel[all]) (2.0.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbconvert->jupyter->weasel[all]) (1.5.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbconvert->jupyter->weasel[all]) (4.11.1)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbconvert->jupyter->weasel[all]) (2.1.1)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from notebook->jupyter->weasel[all]) (0.2.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from notebook->jupyter->weasel[all]) (0.13.1)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from notebook->jupyter->weasel[all]) (0.14.1)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from notebook->jupyter->weasel[all]) (21.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from notebook->jupyter->weasel[all]) (1.8.0)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from qtconsole->jupyter->weasel[all]) (2.3.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from stack-data->ipython->weasel[all]) (0.2.2)\n",
      "Requirement already satisfied: executing in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from stack-data->ipython->weasel[all]) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from stack-data->ipython->weasel[all]) (2.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.3.2->weasel[all]) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.3.2->weasel[all]) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.3.2->weasel[all]) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.3.2->weasel[all]) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.3.2->weasel[all]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.3.2->weasel[all]) (21.4.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.10.30->weasel[all]) (5.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from importlib-metadata>=3.6->nbconvert->jupyter->weasel[all]) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel->weasel[all]) (304)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbformat>=5.1->nbconvert->jupyter->weasel[all]) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from nbformat>=5.1->nbconvert->jupyter->weasel[all]) (4.16.0)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from terminado>=0.8.3->notebook->jupyter->weasel[all]) (2.0.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from argon2-cffi->notebook->jupyter->weasel[all]) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter->weasel[all]) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from bleach->nbconvert->jupyter->weasel[all]) (0.5.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning>=1.3.2->weasel[all]) (2.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->weasel[all]) (0.18.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.3.2->weasel[all]) (3.2.2)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->weasel[all]) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->weasel[all]) (2.21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/autonlab/weasel 'C:\\Users\\ufukh\\AppData\\Local\\Temp\\pip-install-4zxnfbo0\\weasel_39696956984e47bfbd9af0f94b6cab25'\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install git+https://github.com/autonlab/weasel#egg=weasel[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffab015-2204-4986-b415-98ba95228736",
   "metadata": {},
   "source": [
    "The first step is to obtain our weak labels.\n",
    "For this we use the same rules and data set as in the examples above (Snorkel and FlyingSquid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc17f25f-a4c2-47a3-b8b2-ba9bf1966b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8dc271f99541288e43be8090e1c506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing rules:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0094f0b37dbe46d7a2a36ed1e5958e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying rules:   0%|          | 0/3672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain our weak labels\n",
    "weak_labels = WeakLabels(rules=rules, dataset=\"weak_supervision_yt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6273d8-5765-4979-b3eb-3b648b789796",
   "metadata": {},
   "source": [
    "In a second step we instantiate our end model, which in our case will be a pre-trained transformer from the Hugging Face Hub.\n",
    "Here we choose the small ELECTRA model by Google that shows excellent performance given its moderate number of parameters.\n",
    "Due to its size, you can fine-tune it on your CPU within a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "598eac51-5ae9-4d16-aeca-df8c10a00f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from weasel.models.downstream_models.transformers import Transformers\n",
    "\n",
    "# instantiate our transformers end model\n",
    "end_model = Transformers(\"google/electra-small-discriminator\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcec749-929f-439b-969a-620380319363",
   "metadata": {},
   "source": [
    "With our end-model at hand, we can now instantiate the Weasel model.\n",
    "Apart from the end-model, it also includes a neural encoder that tries to estimate latent labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59a8cc63-aed4-490d-9930-72e474c8836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'end_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['end_model'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from weasel.models import Weasel\n",
    "\n",
    "# instantiate our weasel end-to-end model\n",
    "weasel = Weasel(\n",
    "    end_model=end_model,\n",
    "    num_LFs=len(weak_labels.rules),\n",
    "    n_classes=2,\n",
    "    encoder={\"hidden_dims\": [32, 10]},\n",
    "    optim_encoder={\"name\": \"adam\", \"lr\": 1e-4},\n",
    "    optim_end_model={\"name\": \"adam\", \"lr\": 5e-5},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d48a81-2a0c-4958-9b49-093c48beb0d5",
   "metadata": {},
   "source": [
    "Afterwards, we wrap our data in the `TransformersDataModule`, so that Weasel and PyTorch Lightning can work with it.\n",
    "In this step we also tokenize the data. \n",
    "Here we need to be careful to use the corresponding tokenizer to our end model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "defa1b28-cf3a-4c4e-9ccf-1ac67777ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from weasel.datamodules.transformers_datamodule import (\n",
    "    TransformersDataModule,\n",
    "    TransformersCollator,\n",
    ")\n",
    "\n",
    "# tokenizer for our transformers end model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "# tokenize train and test data\n",
    "X_train = [\n",
    "    tokenizer(rec.text, truncation=True)\n",
    "    for rec in weak_labels.records(has_annotation=False)\n",
    "]\n",
    "X_test = [\n",
    "    tokenizer(rec.text, truncation=True)\n",
    "    for rec in weak_labels.records(has_annotation=True)\n",
    "]\n",
    "\n",
    "# instantiate data module\n",
    "datamodule = TransformersDataModule(\n",
    "    label_matrix=weak_labels.matrix(has_annotation=False),\n",
    "    X_train=X_train,\n",
    "    collator=TransformersCollator(tokenizer),\n",
    "    X_test=X_test,\n",
    "    Y_test=weak_labels.annotation(),\n",
    "    batch_size=8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b35f33-a9e1-4289-84d5-bcc1d4c1c200",
   "metadata": {},
   "source": [
    "Now we have everything ready to start the training of our Weasel model.\n",
    "For the training process, Weasel relies on the excellent [PyTorch Lightning Trainer](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html).\n",
    "It provides tons of options and features to optimize the training process, but the defaults below should give you reasonable results.\n",
    "Keep in mind that you are fine-tuning a full-blown transformer model, albeit a small one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a90b1ed-f2c6-4993-bf62-c15964cec3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:446: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "2022-11-18 19:04:40.953 | INFO     | lightning_utilities.core.rank_zero:_info:45 - GPU available: False, used: False\n",
      "2022-11-18 19:04:41.019 | INFO     | lightning_utilities.core.rank_zero:_info:45 - TPU available: False, using: 0 TPU cores\n",
      "2022-11-18 19:04:41.020 | INFO     | lightning_utilities.core.rank_zero:_info:45 - IPU available: False, using: 0 IPUs\n",
      "2022-11-18 19:04:41.020 | INFO     | lightning_utilities.core.rank_zero:_info:45 - HPU available: False, using: 0 HPUs\n",
      "2022-11-18 19:04:41.239 | INFO     | pytorch_lightning.callbacks.model_summary:summarize:83 - \n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | end_model     | Transformers | 13.5 M\n",
      "1 | encoder       | MLPEncoder   | 770   \n",
      "2 | accuracy_func | Softmax      | 0     \n",
      "-----------------------------------------------\n",
      "13.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.6 M    Total params\n",
      "54.200    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4bae3baecd465a895dd4ed08c41704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "You are trying to `self.log()` but it is not managed by the `Trainer` control flow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ufukh\\OneDrive\\Documents\\Recognai\\rubrix\\docs\\_source\\guides\\techniques\\weak_supervision.ipynb Cell 72\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/guides/techniques/weak_supervision.ipynb#Y111sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/guides/techniques/weak_supervision.ipynb#Y111sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     gpus\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,  \u001b[39m# >= 1 to use GPU(s)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/guides/techniques/weak_supervision.ipynb#Y111sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/guides/techniques/weak_supervision.ipynb#Y111sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     logger\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/guides/techniques/weak_supervision.ipynb#Y111sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[pl\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mVal/accuracy\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m)],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/guides/techniques/weak_supervision.ipynb#Y111sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/guides/techniques/weak_supervision.ipynb#Y111sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# fit the model end-to-end\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/guides/techniques/weak_supervision.ipynb#Y111sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/guides/techniques/weak_supervision.ipynb#Y111sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     model\u001b[39m=\u001b[39;49mweasel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/guides/techniques/weak_supervision.ipynb#Y111sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     datamodule\u001b[39m=\u001b[39;49mdatamodule,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/guides/techniques/weak_supervision.ipynb#Y111sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:582\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 582\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    583\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    584\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:624\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    617\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[0;32m    619\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    620\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    621\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    622\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    623\u001b[0m )\n\u001b[1;32m--> 624\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[0;32m    626\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1061\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1059\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> 1061\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   1063\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1140\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1153\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[0;32m   1152\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[0;32m   1155\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1225\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1223\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m-> 1225\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1229\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:206\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_run_end()\n\u001b[0;32m    207\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:180\u001b[0m, in \u001b[0;36mEvaluationLoop.on_run_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mepoch_end_reached()\n\u001b[0;32m    179\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_epoch_end(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outputs)\n\u001b[0;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m []  \u001b[39m# free memory\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m# hook\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:288\u001b[0m, in \u001b[0;36mEvaluationLoop._evaluation_epoch_end\u001b[1;34m(self, outputs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[39m# call the model epoch end\u001b[39;00m\n\u001b[0;32m    287\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_epoch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_epoch_end\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 288\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(hook_name, output_or_outputs)\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1305\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1302\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m   1304\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1305\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1307\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\weasel\\models\\weasel.py:321\u001b[0m, in \u001b[0;36mWeasel.validation_epoch_end\u001b[1;34m(self, outputs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_epoch_end\u001b[39m(\u001b[39mself\u001b[39m, outputs: List[Any]):\n\u001b[1;32m--> 321\u001b[0m     stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mend_model\u001b[39m.\u001b[39;49mvalidation_epoch_end(outputs)\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_dict(stats, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\weasel\\models\\downstream_models\\base_model.py:171\u001b[0m, in \u001b[0;36mDownstreamBaseModel.validation_epoch_end\u001b[1;34m(self, outputs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_decision_thresh() \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39madjust_thresh\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madjust_thresh):\n\u001b[0;32m    167\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_decision_thresh(\n\u001b[0;32m    168\u001b[0m         get_optimal_decision_threshold(preds, Y)\n\u001b[0;32m    169\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_log(Y, preds, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mVal\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\weasel\\models\\downstream_models\\base_model.py:158\u001b[0m, in \u001b[0;36mDownstreamBaseModel._evaluation_log\u001b[1;34m(self, Y, preds, split, verbose)\u001b[0m\n\u001b[0;32m    156\u001b[0m     stats[\u001b[39m'\u001b[39m\u001b[39mdecision_thresh\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_decision_thresh()\n\u001b[0;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_dict(stats, prog_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m stats\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\core\\module.py:511\u001b[0m, in \u001b[0;36mLightningModule.log_dict\u001b[1;34m(self, dictionary, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, rank_zero_only)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39m\"\"\"Log a dictionary of values at once.\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \n\u001b[0;32m    479\u001b[0m \u001b[39mExample::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39m        would produce a deadlock as not all processes would perform this log call.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m dictionary\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 511\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog(\n\u001b[0;32m    512\u001b[0m         name\u001b[39m=\u001b[39;49mk,\n\u001b[0;32m    513\u001b[0m         value\u001b[39m=\u001b[39;49mv,\n\u001b[0;32m    514\u001b[0m         prog_bar\u001b[39m=\u001b[39;49mprog_bar,\n\u001b[0;32m    515\u001b[0m         logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[0;32m    516\u001b[0m         on_step\u001b[39m=\u001b[39;49mon_step,\n\u001b[0;32m    517\u001b[0m         on_epoch\u001b[39m=\u001b[39;49mon_epoch,\n\u001b[0;32m    518\u001b[0m         reduce_fx\u001b[39m=\u001b[39;49mreduce_fx,\n\u001b[0;32m    519\u001b[0m         enable_graph\u001b[39m=\u001b[39;49menable_graph,\n\u001b[0;32m    520\u001b[0m         sync_dist\u001b[39m=\u001b[39;49msync_dist,\n\u001b[0;32m    521\u001b[0m         sync_dist_group\u001b[39m=\u001b[39;49msync_dist_group,\n\u001b[0;32m    522\u001b[0m         add_dataloader_idx\u001b[39m=\u001b[39;49madd_dataloader_idx,\n\u001b[0;32m    523\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    524\u001b[0m         rank_zero_only\u001b[39m=\u001b[39;49mrank_zero_only,\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\pytorch_lightning\\core\\module.py:390\u001b[0m, in \u001b[0;36mLightningModule.log\u001b[1;34m(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    385\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to `self.log()` but the loop\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms result collection is not registered\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m yet. This is most likely because you are trying to log in a `predict` hook,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    387\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt support logging\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    388\u001b[0m     )\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_fx_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    391\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to `self.log()` but it is not managed by the `Trainer` control flow\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m     )\n\u001b[0;32m    394\u001b[0m on_step, on_epoch \u001b[39m=\u001b[39m _FxValidator\u001b[39m.\u001b[39mcheck_logging_and_get_default_levels(\n\u001b[0;32m    395\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_fx_name, on_step\u001b[39m=\u001b[39mon_step, on_epoch\u001b[39m=\u001b[39mon_epoch\n\u001b[0;32m    396\u001b[0m )\n\u001b[0;32m    398\u001b[0m \u001b[39m# make sure user doesn't introduce logic for multi-dataloaders\u001b[39;00m\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: You are trying to `self.log()` but it is not managed by the `Trainer` control flow"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# instantiate the pytorch-lightning trainer\n",
    "trainer = pl.Trainer(\n",
    "    gpus=0,  # >= 1 to use GPU(s)\n",
    "    max_epochs=2,\n",
    "    logger=None,\n",
    "    callbacks=[pl.callbacks.ModelCheckpoint(monitor=\"Val/accuracy\", mode=\"max\")],\n",
    ")\n",
    "\n",
    "# fit the model end-to-end\n",
    "trainer.fit(\n",
    "    model=weasel,\n",
    "    datamodule=datamodule,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f22be-7555-431b-b416-d1b57ef46128",
   "metadata": {},
   "source": [
    "After the training we can call the `Trainer.test` method to check the final performance. \n",
    "The model should achieve a test accuracy of around 0.94."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06a5e6-680f-47e1-bb74-1e97917c59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()\n",
    "# {'accuracy': 0.94, ...}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ef7cc-9c4f-444a-a6a0-5e4bac36c04f",
   "metadata": {},
   "source": [
    "To use the model for inference, you can either use its *predict* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9daafb6-937a-46d4-932d-4fd7296b9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text for the inference\n",
    "text = \"In my head this is like 2 years ago.. Time FLIES\"\n",
    "\n",
    "# Get predictions for the example text\n",
    "predicted_probs, predicted_label = weasel.predict(tokenizer(text, return_tensors=\"pt\"))\n",
    "\n",
    "# Map predicted int to label\n",
    "weak_labels.int2label[int(predicted_label)]  # HAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aabc852-4e91-4d4b-86b3-da1e9eea4f04",
   "metadata": {},
   "source": [
    "Or you can instantiate one of the popular transformers pipelines, providing directly the end-model and the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8aed5-6618-4170-b144-87bd8863e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# modify the id2label mapping of the model\n",
    "weasel.end_model.model.config.id2label = weak_labels.int2label\n",
    "\n",
    "# create transformers pipeline\n",
    "classifier = pipeline(\n",
    "    \"text-classification\", model=weasel.end_model.model, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# use pipeline for predictions\n",
    "classifier(text)  # [{'label': 'HAM', 'score': 0.6110987663269043}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e161db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('argilla')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "83e13ff0de9ea08cace169d1016bf08ce368842307fd88824f08736a0a9ca04b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
