{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üà¥üàØÔ∏è Token Classification\n",
    "\n",
    "Token classification kind-of-tasks are NLP tasks aimed to divide the input text into words, or syllables, and assign certain values to them. Think about giving each word in a sentence its grammatical category, or highlight which parts of a medical report belong to a certain speciality. There are some popular ones like NER or POS-tagging. For this part of the article, we will use [spaCy](https://spacy.io/) with Argilla to track and monitor Token Classification tasks.\n",
    "\n",
    "Remember to install spaCy and datasets, or running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets -qqq\n",
    "%pip install -U spacy -qqq\n",
    "%pip install protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER\n",
    "Named entity recognition (NER) is the task of tagging entities in text with their corresponding type. Approaches typically use *BIO* notation, which differentiates the beginning (**B**) and the inside (**I**) of entities. **O** is used for non-entity tokens.\n",
    "\n",
    "For this tutorial, we're going to use the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub. It contains all explicit time references in a dataset of 52,183 novels whose full text is available via Project Gutenberg. From extracts of novels, we are surely going to find some NER entities. We will also use the `en_core_web_trf` pretrained English model, a Roberta-based spaCy model. If you do not have them installed, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf #Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load our dataset\n",
    "dataset = load_dataset(\"gutenberg_time\", split=\"train[0:20]\")\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for record in dataset:\n",
    "    # We only need the text of each instance\n",
    "    text = record[\"tok_context\"]\n",
    "\n",
    "    # spaCy Doc creation\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Prediction entities with the tuples (label, start character, end character)\n",
    "    entities = [(ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "\n",
    "    # Pre-tokenized input text\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    # Argilla TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=entities,\n",
    "            prediction_agent=\"en_core_web_trf\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Logging into Argilla\n",
    "rg.log(\n",
    "    records=records,\n",
    "    name=\"ner\",\n",
    "    tags={\n",
    "        \"task\": \"NER\",\n",
    "        \"family\": \"token-classification\",\n",
    "        \"dataset\": \"gutenberg-time\",\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging\n",
    "\n",
    "A POS tag (or part-of-speech tag) is a special label assigned to each word in a text corpus to indicate the part of speech and often also other grammatical categories such as tense, number, case etc. POS tags are used in corpus searches and in-text analysis tools and algorithms.\n",
    "\n",
    "We will be repeating duo for this second spaCy example, with the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub and the `en_core_web_trf` pretrained English model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load our dataset\n",
    "dataset = load_dataset(\"gutenberg_time\", split=\"train[0:10]\")\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for record in dataset:\n",
    "    # We only need the text of each instance\n",
    "    text = record[\"tok_context\"]\n",
    "\n",
    "    # spaCy Doc creation\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Creating the prediction entity as a list of tuples (tag, start_char, end_char)\n",
    "    prediction = [(token.pos_, token.idx, token.idx + len(token)) for token in doc]\n",
    "\n",
    "    # Argilla TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=[token.text for token in doc],\n",
    "            prediction=prediction,\n",
    "            prediction_agent=\"en_core_web_trf\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Logging into Argilla\n",
    "rg.log(\n",
    "    records=records,\n",
    "    name=\"pos-tagging\",\n",
    "    tags={\n",
    "        \"task\": \"pos-tagging\",\n",
    "        \"family\": \"token-classification\",\n",
    "        \"dataset\": \"gutenberg-time\",\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slot Filling\n",
    "\n",
    "The goal of Slot Filling is to identify, from a running dialog different slots, which one correspond to different parameters of the user‚Äôs query. For instance, when a user queries for nearby restaurants, key slots for location and preferred food are required for a dialog system to retrieve the appropriate information. Thus, the goal is to look for specific pieces of information in the request and tag the corresponding tokens accordingly.\n",
    "\n",
    "We made a tutorial on this matter for our open-source NLP library, [biome.text](https://recognai.github.io/biome-text/v3.0.0/). We will use similar procedures here, focusing on the logging of the information. If you want to see in-depth explanations on how the pipelines are made, please visit [the tutorial](https://recognai.github.io/biome-text/v3.0.0/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.html#training-a-sequence-tagger-for-slot-filling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by downloading biome.text and importing it alongside Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U biome-text\n",
    "exit(0)  # Force restart of the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "from biome.text import (\n",
    "    Pipeline,\n",
    "    Dataset,\n",
    "    PipelineConfiguration,\n",
    "    VocabularyConfiguration,\n",
    "    Trainer,\n",
    ")\n",
    "from biome.text.configuration import FeaturesConfiguration, WordFeatures, CharFeatures\n",
    "from biome.text.modules.configuration import Seq2SeqEncoderConfiguration\n",
    "from biome.text.modules.heads import TokenClassificationConfiguration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will use the [SNIPS data set](https://github.com/sonos/nlu-benchmark/tree/master/2017-06-custom-intent-engines) adapted by [Su Zhu](https://github.com/sz128/slot_filling_and_intent_detection_of_SLU/tree/master/data/snips)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/train.json\n",
    "!curl -O https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/valid.json\n",
    "!curl -O https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/test.json\n",
    "\n",
    "train_ds = Dataset.from_json(\"train.json\")\n",
    "valid_ds = Dataset.from_json(\"valid.json\")\n",
    "test_ds = Dataset.from_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we need to configure our biome.text Pipeline. More information on this configuration [here](https://recognai.github.io/biome-text/v3.0.0/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.html#configure-your-biome-text-pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_feature = WordFeatures(\n",
    "    embedding_dim=300,\n",
    "    weights_file=\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\",\n",
    ")\n",
    "\n",
    "char_feature = CharFeatures(\n",
    "    embedding_dim=32,\n",
    "    encoder={\n",
    "        \"type\": \"gru\",\n",
    "        \"bidirectional\": True,\n",
    "        \"num_layers\": 1,\n",
    "        \"hidden_size\": 32,\n",
    "    },\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "features_config = FeaturesConfiguration(word=word_feature, char=char_feature)\n",
    "\n",
    "encoder_config = Seq2SeqEncoderConfiguration(\n",
    "    type=\"gru\",\n",
    "    bidirectional=True,\n",
    "    num_layers=1,\n",
    "    hidden_size=128,\n",
    ")\n",
    "\n",
    "labels = {tag[2:] for tags in train_ds[\"labels\"] for tag in tags if tag != \"O\"}\n",
    "\n",
    "for ds in [train_ds, valid_ds, test_ds]:\n",
    "    ds.rename_column_(\"labels\", \"tags\")\n",
    "\n",
    "head_config = TokenClassificationConfiguration(\n",
    "    labels=list(labels),\n",
    "    label_encoding=\"BIO\",\n",
    "    top_k=1,\n",
    "    feedforward={\n",
    "        \"num_layers\": 1,\n",
    "        \"hidden_dims\": [128],\n",
    "        \"activations\": [\"relu\"],\n",
    "        \"dropout\": [0.1],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = PipelineConfiguration(\n",
    "    name=\"slot_filling_tutorial\",\n",
    "    features=features_config,\n",
    "    encoder=encoder_config,\n",
    "    head=head_config,\n",
    ")\n",
    "\n",
    "pl = Pipeline.from_config(pipeline_config)\n",
    "\n",
    "vocab_config = VocabularyConfiguration(min_count={\"word\": 2}, include_valid_data=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    pipeline=pl,\n",
    "    train_dataset=train_ds,\n",
    "    valid_dataset=valid_ds,\n",
    "    vocab_config=vocab_config,\n",
    "    trainer_config=None,\n",
    ")\n",
    "\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained our model, we can go ahead and log the predictions to Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_json(\"test.json\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for record in dataset[0:10][\"text\"]:\n",
    "    # We only need the text of each instance\n",
    "    text = \" \".join(word for word in record)\n",
    "\n",
    "    # Predicting tags and entities given the input text\n",
    "    prediction = pl.predict(text=text)\n",
    "\n",
    "    # Creating the prediction entity as a list of tuples (tag, start_char, end_char)\n",
    "    prediction = [\n",
    "        (token[\"label\"], token[\"start\"], token[\"end\"])\n",
    "        for token in prediction[\"entities\"][0]\n",
    "    ]\n",
    "\n",
    "    # Argilla TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=record,\n",
    "            prediction=prediction,\n",
    "            prediction_agent=\"biome_slot_filling_tutorial\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Logging into Argilla\n",
    "rg.log(\n",
    "    records=records,\n",
    "    name=\"slot-filling\",\n",
    "    tags={\n",
    "        \"task\": \"slot-filling\",\n",
    "        \"family\": \"token-classification\",\n",
    "        \"dataset\": \"SNIPS\",\n",
    "    },\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "39f4e3bd8ecb53b4a2ef9bccb982583dac0632e40e094b10b94294b76eaa26cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
