{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üà¥üàØÔ∏è Token Classification\n",
    "\n",
    "Token classification kind-of-tasks are NLP tasks aimed to divide the input text into words, or syllables, and assign certain values to them. Think about giving each word in a sentence its grammatical category, or highlight which parts of a medical report belong to a certain speciality. There are some popular ones like NER or POS-tagging. For this part of the article, we will use [spaCy](https://spacy.io/) with Argilla to track and monitor Token Classification tasks.\n",
    "\n",
    "Remember to install spaCy and datasets, or running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets -qqq\n",
    "%pip install -U spacy -qqq\n",
    "%pip install protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER\n",
    "Named entity recognition (NER) is the task of tagging entities in text with their corresponding type. Approaches typically use *BIO* notation, which differentiates the beginning (**B**) and the inside (**I**) of entities. **O** is used for non-entity tokens.\n",
    "\n",
    "For this tutorial, we're going to use the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub. It contains all explicit time references in a dataset of 52,183 novels whose full text is available via Project Gutenberg. From extracts of novels, we are surely going to find some NER entities. We will also use the `en_core_web_trf` pretrained English model, a Roberta-based spaCy model. If you do not have them installed, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf #Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load our dataset\n",
    "dataset = load_dataset(\"gutenberg_time\", split=\"train[0:20]\")\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for record in dataset:\n",
    "    # We only need the text of each instance\n",
    "    text = record[\"tok_context\"]\n",
    "\n",
    "    # spaCy Doc creation\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Prediction entities with the tuples (label, start character, end character)\n",
    "    entities = [(ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "\n",
    "    # Pre-tokenized input text\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    # Argilla TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=entities,\n",
    "            prediction_agent=\"en_core_web_trf\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Logging into Argilla\n",
    "rg.log(\n",
    "    records=records,\n",
    "    name=\"ner\",\n",
    "    tags={\n",
    "        \"task\": \"NER\",\n",
    "        \"family\": \"token-classification\",\n",
    "        \"dataset\": \"gutenberg-time\",\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging\n",
    "\n",
    "A POS tag (or part-of-speech tag) is a special label assigned to each word in a text corpus to indicate the part of speech and often also other grammatical categories such as tense, number, case etc. POS tags are used in corpus searches and in-text analysis tools and algorithms.\n",
    "\n",
    "We will be repeating duo for this second spaCy example, with the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub and the `en_core_web_trf` pretrained English model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load our dataset\n",
    "dataset = load_dataset(\"gutenberg_time\", split=\"train[0:10]\")\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for record in dataset:\n",
    "    # We only need the text of each instance\n",
    "    text = record[\"tok_context\"]\n",
    "\n",
    "    # spaCy Doc creation\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Creating the prediction entity as a list of tuples (tag, start_char, end_char)\n",
    "    prediction = [(token.pos_, token.idx, token.idx + len(token)) for token in doc]\n",
    "\n",
    "    # Argilla TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=[token.text for token in doc],\n",
    "            prediction=prediction,\n",
    "            prediction_agent=\"en_core_web_trf\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Logging into Argilla\n",
    "rg.log(\n",
    "    records=records,\n",
    "    name=\"pos-tagging\",\n",
    "    tags={\n",
    "        \"task\": \"pos-tagging\",\n",
    "        \"family\": \"token-classification\",\n",
    "        \"dataset\": \"gutenberg-time\",\n",
    "    },\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "39f4e3bd8ecb53b4a2ef9bccb982583dac0632e40e094b10b94294b76eaa26cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
