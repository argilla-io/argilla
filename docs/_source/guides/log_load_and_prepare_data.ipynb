{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09c1533-97fd-43f7-83ea-f3a56edd1d5e",
   "metadata": {},
   "source": [
    "# üßë‚Äçüíª Log, load, and prepare data\n",
    "\n",
    "This guide showcases some features of the `Dataset` classes in the Argilla client.\n",
    "The Dataset classes are lightweight containers for Argilla records. These classes facilitate importing from and exporting to different formats (e.g., `pandas.DataFrame`, `datasets.Dataset`) as well as sharing and versioning Argilla datasets using the Hugging Face Hub.\n",
    "\n",
    "For each record type there's a corresponding Dataset class called `DatasetFor<RecordType>`.\n",
    "You can look up their API in the [reference section](../reference/python/python_client.rst#module-argilla.client.datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d4c9e-24a1-4a59-8a17-3b6ac1a39c88",
   "metadata": {},
   "source": [
    "## Create a Dataset\n",
    "\n",
    "The main component of the Argilla data model is called a record. A dataset in Argilla is a collection of these records. \n",
    "Records can be of different types depending on the currently supported tasks:\n",
    "\n",
    " 1. `TextClassificationRecord`\n",
    " 2. `TokenClassificationRecord`\n",
    " 3. `Text2TextRecord`\n",
    " \n",
    "The most critical attributes of a record that are common to all types are:\n",
    "\n",
    " - `text`: The input text of the record (Required);\n",
    " - `annotation`: Annotate your record in a task-specific manner (Optional);\n",
    " - `prediction`: Add task-specific model predictions to the record (Optional);\n",
    " - `metadata`: Add some arbitrary metadata to the record (Optional);-√±\n",
    " \n",
    "In Argilla, records are created programmatically using the [client library](../reference/python/python_client.rst) within a Python script, a [Jupyter notebook](https://jupyter.org/), or another IDE.\n",
    "\n",
    "\n",
    "Let's see how to create and upload a basic record to the Argilla web app  (make sure Argilla is already installed on your machine as described in the [setup guide](../getting_started/installation/installation.md)):\n",
    "\n",
    "\n",
    "Under the hood the Dataset classes store the records in a simple Python list.\n",
    "Therefore, working with a Dataset class is not very different to working with a simple list of records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf8c7f-463d-48ee-944a-3adb57edb159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argilla as rg\n",
    "\n",
    "# Create a TextClassificationRecord\n",
    "record = rg.TextClassificationRecord(\n",
    "    text=\"Hello world, this is me!\",\n",
    "    prediction=[(\"LABEL1\", 0.8), (\"LABEL2\", 0.2)],\n",
    "    annotation=\"LABEL1\",\n",
    "    multi_label=False,\n",
    ")\n",
    "\n",
    "# Create a TokenClassificationRecord\n",
    "record = rg.TokenClassificationRecord(\n",
    "    text=\"Michael is a professor at Harvard\",\n",
    "    tokens=[\"Michael\", \"is\", \"a\", \"professor\", \"at\", \"Harvard\"],\n",
    "    prediction=[(\"NAME\", 0, 7), (\"LOC\", 26, 33)],\n",
    ")\n",
    "\n",
    "# Create a Text2TextRecord\n",
    "record = rg.Text2TextRecord(\n",
    "    text=\"My name is Sarah and I love my dog.\",\n",
    "    prediction=[\"Je m'appelle Sarah et j'aime mon chien.\"],\n",
    ")\n",
    "\n",
    "# Start with a list of Argilla records\n",
    "# Note that each dataset can only contain records of the same type (e.g. TextClassificationRecord)\n",
    "dataset_rg = rg.DatasetForTextClassification([record])\n",
    "\n",
    "# Loop over the dataset\n",
    "for record in dataset_rg:\n",
    "    print(record)\n",
    "\n",
    "# Index into the dataset\n",
    "dataset_rg[0] = rg.TextClassificationRecord(text=\"replace record\")\n",
    "\n",
    "# log a dataset to the Argilla web app\n",
    "rg.log(\n",
    "    recors=dataset_rg, \n",
    "    name=\"my_dataset\",\n",
    "    tags={\"example\": \"test\"},\n",
    "    background=True, \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e9fc0-8563-4727-abca-62205a4de385",
   "metadata": {},
   "source": [
    "The Dataset classes do some extra checks for you, to make sure you do not mix record types when appending or indexing into a dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a65408",
   "metadata": {},
   "source": [
    "## Logging for deployments\n",
    "\n",
    "Argilla currently gives users several ways to log model predictions besides the `rg.log` async method. \n",
    "\n",
    "### Log with `rg.monitor`\n",
    "\n",
    "For widely-used libraries Argilla includes an \"auto-monitoring\" option via the `rg.monitor` method. Currently supported libraries are Hugging Face Transformers and spaCy, if you'd like to see another library supported feel free to add a discussion or issue on GitHub.\n",
    "\n",
    "`rg.monitor` will wrap HF and spaCy pipelines so every time you call them, the output of these calls will be logged into the dataset of your choice, as a background process, in a non-blocking way. Additionally, `rg.monitor` will add several tags to your dataset such as the library build version, the model name, the language, etc. This should also work for custom (private) pipelines, not only the Hub's or official spaCy models.\n",
    "\n",
    "It is worth noting that this feature is useful beyond monitoring, and can be used for data collection (e.g., bootstrapping data annotation with pre-trained pipelines), model development (e.g., error analysis), and model evaluation (e.g., combined with data annotation to obtain evaluation metrics).\n",
    "\n",
    "#### Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42322f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import argilla as rg\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = rg.monitor(nlp, dataset=\"nlp_monitoring_spacy\")\n",
    "\n",
    "dataset.map(lambda example: {\"prediction\": nlp(example[\"text\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a63566",
   "metadata": {},
   "source": [
    "#### Using transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5652d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import argilla as rg\n",
    "\n",
    "nlp = pipeline(\n",
    "    \"sentiment-analysis\", return_all_scores=True, padding=True, truncation=True\n",
    ")\n",
    "nlp = rg.monitor(nlp, dataset=\"nlp_monitoring\")\n",
    "\n",
    "dataset.map(lambda example: {\"prediction\": nlp(example[\"text\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f35f3b",
   "metadata": {},
   "source": [
    "#### Using flAIr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = rg.monitor(\n",
    "    SequenceTagger.load(\"flair/ner-english\"), dataset=\"flair-example\", sample_rate=1.0\n",
    ")\n",
    "\n",
    "# make example sentence\n",
    "sentence = Sentence(\"George Washington went to Washington\")\n",
    "\n",
    "# predict NER tags. This will log the prediction in Argilla\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc89f8",
   "metadata": {},
   "source": [
    "### Log using ASGI middleware\n",
    "\n",
    "For using the ASGI middleware, see this [tutorial](../../tutorials/notebooks/deploying-texttokenclassification-fastapi.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b5f0b",
   "metadata": {},
   "source": [
    "## Load a Dataset\n",
    "\n",
    "It is very straightforward to simply load a dataset. This can be done using `rg.load`. Additionally, you can check our [query page](query_datasets.html) for custom info about querying and you can check our [vector page](label_records_with_semanticsearch.html) for info about vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54eeaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "rg.load(\n",
    "    name=\"my_dataset\",\n",
    "    query=\"my AND query\",\n",
    "    limit=42\n",
    "    ids=[\"id1\", \"id2\", \"id3\"],\n",
    "    vectors=[\"vector1\", \"vector2\", \"vector3\"],\n",
    "    fields=[\"id\",\"inputs\",\"text\"]\n",
    "    sort=[(\"event_timestamp\", \"desc\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f9b20",
   "metadata": {},
   "source": [
    "## Update a Dataset\n",
    "\n",
    "Argilla datasets have certain *settings* that you can configure via the `rg.*Settings` classes, for example `rg.TextClassificationSettings`.\n",
    "\n",
    "### Define a labeling schema\n",
    "\n",
    "You can define a labeling schema for your Argilla dataset, which fixes the allowed labels for your predictions and annotations.\n",
    "Once you set a labeling schema, each time you log to the corresponding dataset, Argilla will perform validations of the added predictions and annotations to make sure they comply with the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce01a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# Define labeling schema\n",
    "settings = rg.TextClassificationSettings(label_schema=[\"A\", \"B\", \"C\"])\n",
    "\n",
    "# Apply settings to a new or already existing dataset\n",
    "rg.configure_dataset(name=\"my_dataset\", settings=settings)\n",
    "\n",
    "# Logging to the newly created dataset triggers the validation checks\n",
    "rg.log(rg.TextClassificationRecord(text=\"text\", annotation=\"D\"), \"my_dataset\")\n",
    "# BadRequestApiError: Argilla server returned an error with http status: 400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec833d8-b0a1-415a-9d31-5ef1205d3ebb",
   "metadata": {},
   "source": [
    "#### Migrating labels created from UI to label schema\n",
    "\n",
    "For old Argilla versions, labels created from the UI were not included as part of a labelling schema. Instead, the UI used the dataset metadata index in Elastic Search to store this information.\n",
    "\n",
    "If you want to move this info to the corresponding label schema, you can execute the next code snippet:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78506c9e-56b0-4ca1-80eb-a5d3a816169b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Warning\n",
    "\n",
    "From Argilla version v1.4.0, all labels will be created using the new label schema settings. Be sure to migrate datasets with labels created using the UI to the proper label schema.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6175c-bb8d-4867-bc14-a7768585c549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from argilla.client import api\n",
    "\n",
    "rg.init()\n",
    "client = api.active_client()\n",
    "\n",
    "# This metadata key was used by the UI to store created labels in datasets\n",
    "CUSTOM_DATA_KEY = \"rubrix.recogn.ai/ui/custom/userData.v1\"\n",
    "datasets = [dataset for dataset in client.http_client.get(\"/api/datasets\") if CUSTOM_DATA_KEY in dataset[\"metadata\"]]\n",
    "print(f\"Found {len(datasets)} datasets to migrate\")\n",
    "for ds in datasets:\n",
    "    metadata = ds[\"metadata\"]\n",
    "    task = ds[\"task\"]\n",
    "    name = ds[\"name\"]\n",
    "    workspace = ds[\"owner\"]  # owner will be replaced by `workspace` in newer versions\n",
    "\n",
    "    if task == \"TextClassification\":  # Build text classification settings\n",
    "        labels = metadata[CUSTOM_DATA_KEY][\"labels\"]\n",
    "        settings = rg.TextClassificationSettings(label_schema=set(labels))\n",
    "    elif task == \"TokenClassification\":  # Build token classification settings\n",
    "        labels = metadata[CUSTOM_DATA_KEY][\"entities\"]\n",
    "        settings = rg.TokenClassificationSettings(label_schema=set(labels))\n",
    "    else:\n",
    "        raise Exception(f\"No labels key for task {task}. {dataset}\")\n",
    "\n",
    "    # Setting the dataset workspace to work with current dataset\n",
    "    rg.set_workspace(workspace)\n",
    "\n",
    "    # We will complete labels schema with labels found in dataset records.\n",
    "    # This will avoid errors on label schema validation (when labels in records are not present in the label schema)\n",
    "    metrics = client.compute_metric(name=name, metric=\"dataset_labels\")\n",
    "    for label in metrics.results[\"labels\"]:\n",
    "        settings.label_schema.add(label)\n",
    "    print(f\"Settings labels for dataset '{name}': {settings}\")\n",
    "    rg.configure_dataset(name=name, settings=settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed77791",
   "metadata": {},
   "source": [
    "### Update Records\n",
    "\n",
    "It is possible to update records from your Argilla datasets using our Python API. This approach works the same way as an upsert in a normal database, based on the record `id`. You can update any arbitrary parameters and they will be over-written if you use the `id` of the original record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# read all records in the dataset or define a specific search via the `query` parameter\n",
    "record = rg.load(\"my_first_dataset\")\n",
    "\n",
    "# modify first record metadata (if no previous metadata dict you might need to create it)\n",
    "record[0].metadata[\"my_metadata\"] = \"im a new value\"\n",
    "\n",
    "# log record to update it, this will keep everything but add my_metadata field and value\n",
    "rg.log(name=\"my_first_dataset\", records=record[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88889b-12f4-472f-bcbe-fb47be475d02",
   "metadata": {},
   "source": [
    "## Import a Dataset\n",
    "\n",
    "When you have your data in a [_pandas DataFrame_](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) or a [_datasets Dataset_](https://huggingface.co/docs/datasets/access.html), we provide some neat shortcuts to import this data into a Argilla Dataset. \n",
    "You have to make sure that the data follows the record model of a specific task, otherwise you will get validation errors. \n",
    "Columns in your DataFrame/Dataset that are not supported or recognized, will simply be ignored.\n",
    "\n",
    "The record models of the tasks are explained in the [reference section](../reference/python/python_client.rst#module-argilla.client.models). \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "Due to it's pyarrow nature, data in a `datasets.Dataset` has to follow a slightly different model, that you can look up in the examples of the `Dataset*.from_datasets` [docstrings](../reference/python/python_client.rst#argilla.client.datasets.DatasetForTokenClassification.from_datasets). \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca56d4-2bb5-4c77-a069-7a50ee78b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# import data from a pandas DataFrame\n",
    "dataset_rg = rg.read_pandas(my_dataframe, task=\"TextClassification\")\n",
    "# or\n",
    "dataset_rg = rg.DatasetForTextClassification.from_pandas(my_dataframe)\n",
    "\n",
    "# import data from a datasets Dataset\n",
    "dataset_rg = rg.read_datasets(my_dataset, task=\"TextClassification\")\n",
    "# or\n",
    "dataset_rg = rg.DatasetForTextClassification.from_datasets(my_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a0e4d-f05b-4635-af54-885933f3bc9a",
   "metadata": {},
   "source": [
    "We also provide helper arguments you can use to read almost arbitrary datasets for a given task from the [Hugging Face Hub](https://huggingface.co/datasets).\n",
    "They map certain input arguments of the Argilla records to columns of the given dataset.\n",
    "Let's have a look at a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41209b56-0dce-4045-8a4f-ffc00f962a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "\n",
    "# the \"poem_sentiment\" dataset has columns \"verse_text\" and \"label\"\n",
    "dataset_rg = rg.DatasetForTextClassification.from_datasets(\n",
    "    dataset=load_dataset(\"poem_sentiment\", split=\"test\"),\n",
    "    text=\"verse_text\",\n",
    "    annotation=\"label\",\n",
    ")\n",
    "\n",
    "# the \"snli\" dataset has the columns \"premise\", \"hypothesis\" and \"label\"\n",
    "dataset_rg = rg.DatasetForTextClassification.from_datasets(\n",
    "    dataset=load_dataset(\"snli\", split=\"test\"),\n",
    "    inputs=[\"premise\", \"hypothesis\"],\n",
    "    annotation=\"label\",\n",
    ")\n",
    "\n",
    "# the \"conll2003\" dataset has the columns \"id\", \"tokens\", \"pos_tags\", \"chunk_tags\" and \"ner_tags\"\n",
    "rg.DatasetForTokenClassification.from_datasets(\n",
    "    dataset=load_dataset(\"conll2003\", split=\"test\"),\n",
    "    tags=\"ner_tags\",\n",
    ")\n",
    "\n",
    "# the \"xsum\" dataset has the columns \"id\", \"document\" and \"summary\"\n",
    "rg.DatasetForText2Text.from_datasets(\n",
    "    dataset=load_dataset(\"xsum\", split=\"test\"),\n",
    "    text=\"document\",\n",
    "    annotation=\"summary\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de341ab4-6be4-499b-9f4e-0eb4546fa753",
   "metadata": {},
   "source": [
    "You can also use the shortcut `rg.read_datasets(dataset=..., task=..., **kwargs)` where the keyword arguments are passed on to the corresponding `from_datasets()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ccb75",
   "metadata": {},
   "source": [
    "## Migrating dataset from \"emtpy\" workspaces\n",
    "\n",
    "For old argilla versions, datasets could be created with no workspace inf (`-` workspace in UI). New versions expect to work with datasets in a given workspace.\n",
    "\n",
    "If you have datasets within an empty workspaces, you should move them to a specific workspace to keep your workflow compatible with the latest versions\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Warning\n",
    "\n",
    "From argilla version v1.4.0, datasets with empty workspace won't be supported anymore, and you should migrate them before upgrading argilla.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758c202-4b4c-4275-8aa8-047539ea2df0",
   "metadata": {},
   "source": [
    "Here is a code snippet showing how to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b0075",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from argilla.client import api\n",
    "\n",
    "rg.init()\n",
    "rg_client = api.active_client()\n",
    "\n",
    "new_workspace = \"<put-target-workspace-here>\"\n",
    "\n",
    "empty_workspace_datasets = [\n",
    "    ds[\"name\"]\n",
    "    for ds in rg_client.http_client.get(\"/api/datasets\")\n",
    "    # filtering dataset with no workspace (use `\"owner\"` if you're running this code with server versions <=1.3.0)\n",
    "    if not ds.get(\"workspace\", None)\n",
    "]\n",
    "\n",
    "rg.set_workspace(\"\")  # working from the \"empty\" workspace\n",
    "\n",
    "for dataset in empty_workspace_datasets:\n",
    "    rg.copy(dataset, dataset, new_workspace)\n",
    "\n",
    "# Dataset are normally copied to the provided workspace\n",
    "# You should delete datasets with no workspace\n",
    "# In that case, uncomment following lines\n",
    "# for dataset in empty_workspace_datasets:\n",
    "#    rg.delete(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f945b5c",
   "metadata": {},
   "source": [
    "## Reindex a Dataset\n",
    "\n",
    "Sometimes updates require us to reindex the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a3998",
   "metadata": {},
   "source": [
    "### Argilla Metrics\n",
    "\n",
    "For our internally computed metrics, this can be done by simply, loading and logging the same records back to the same index. This is because our internal metrics are computed and updated during logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76207d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "dataset = \"my-outdated-dataset\"\n",
    "ds = rg.load(dataset) \n",
    "rg.log(ds, dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92935ce",
   "metadata": {},
   "source": [
    "### Elasticsearch\n",
    "\n",
    "For Elastic indices, re-indexing requires a bit more effort. To be certain of a proper re-indexing, we requires loading the records, and storing them within a completely new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9628ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "dataset = \"my-outdated-dataset\"\n",
    "ds = rg.load(dataset) \n",
    "new_dataset = \"my-new-dataset\"\n",
    "rg.log(ds, new_dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb290a71-ad07-496f-b167-ad80d91fa633",
   "metadata": {},
   "source": [
    "## Share on Hugging Face\n",
    "\n",
    "You can easily share your Argilla dataset with your community via the Hugging Face Hub.\n",
    "For this you just need to export your Argilla Dataset to a `datasets.Dataset` and [push it to the hub](https://huggingface.co/docs/datasets/upload_dataset.html?highlight=push_to_hub#upload-from-python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6d70b-0b91-4efb-94b6-6b7710c105c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# load your annotated dataset from the Argilla web app\n",
    "dataset_rg = rg.load(\"my_dataset\")\n",
    "\n",
    "# export your Argilla Dataset to a datasets Dataset\n",
    "dataset_ds = dataset_rg.to_datasets()\n",
    "\n",
    "# push the dataset to the Hugging Face Hub\n",
    "dataset_ds.push_to_hub(\"my_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696605dd-be87-4ae6-b367-b0cdabfaf39f",
   "metadata": {},
   "source": [
    "Afterward, your community can easily access your annotated dataset and log it directly to the Argilla web app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a4792-bc91-4d64-8465-b2bccf23502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# download the dataset from the Hugging Face Hub\n",
    "dataset_ds = load_dataset(\"user/my_dataset\", split=\"train\")\n",
    "\n",
    "# read in dataset, assuming its a dataset for text classification\n",
    "dataset_rg = rg.read_datasets(dataset_ds, task=\"TextClassification\")\n",
    "\n",
    "# log the dataset to the Argilla web app\n",
    "rg.log(dataset_rg, \"dataset_by_user\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5b02c-a24c-40d0-80ce-6332fba15318",
   "metadata": {},
   "source": [
    "## Prepare dataset for training\n",
    "\n",
    "If you want to train a Hugging Face transformer or a spaCy NER pipeline, we provide a handy method to prepare your dataset: `DatasetFor*.prepare_for_training()`.\n",
    "It will return a Hugging Face dataset, a spaCy DocBin or a SparkNLP-fromatted DataFrame, optimized for the training process with the Hugging Face Trainer, the spaCy cli or the SparkNLP API. Our [libraries deepdive](../libraries/libraries.html) and [training tutorials](../../tutorials/steps/2_training.md), show entire training workflows for your favorite packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae027b",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "It is possible to directly include train-test splits to the `prepare_for_training` by passing the `train_size` and `test_size` parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1d63a-7940-46ff-85e9-062ab785b828",
   "metadata": {},
   "source": [
    "### TextClassification\n",
    "For text classification tasks, it flattens the inputs into separate columns of the returned dataset and converts the annotations of your records into integers and writes them in a label column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3bcc58-fdc9-427a-8ffd-ef37e67e03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "dataset_rg = rg.DatasetForTextClassification(\n",
    "    [\n",
    "        rg.TextClassificationRecord(\n",
    "            inputs={\"title\": \"My title\", \"content\": \"My content\"}, annotation=\"news\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_rg.prepare_for_training(framework=\"transformers\")[0]\n",
    "# Output:\n",
    "# {'title': 'My title', 'content': 'My content', 'label': 0}\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "dataset_rg.prepare_for_training(framework=\"spacy\", lang=nlp)\n",
    "# Output:\n",
    "# <spacy.tokens._serialize.DocBin object at 0x280613af0>\n",
    "\n",
    "\n",
    "dataset_rg.prepare_for_training(framework=\"spark-nlp\")\n",
    "# Output:\n",
    "# <pd.DataFrame>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907714da-5496-4a49-bc6d-2d9008d0082f",
   "metadata": {},
   "source": [
    "### TokenClassification\n",
    "\n",
    "For token classification tasks, it converts the annotations of a record into integers representing BIO tags and writes them in a `ner_tags` column:\n",
    "By passing the `framework` variable as `transformers`, `spark-nlp` or `spacy`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c72cb-599a-4073-a018-6f9746e17e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "dataset_rg = rg.DatasetForTokenClassification(\n",
    "    [\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=\"I live in Madrid\",\n",
    "            tokens=[\"I\", \"live\", \"in\", \"Madrid\"],\n",
    "            annotation=[(\"LOC\", 10, 16)],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_rg.prepare_for_training(framework=\"transformers\")[0]\n",
    "# Output:\n",
    "# {..., 'tokens': ['I', 'live', 'in', 'Madrid'], 'ner_tags': [0, 0, 0, 1], ...}+\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "dataset_rg.prepare_for_training(framework=\"spacy\", lang=nlp)\n",
    "# Output:\n",
    "# <spacy.tokens._serialize.DocBin object at 0x280613af0>\n",
    "\n",
    "dataset_rg.prepare_for_training(framework=\"spark-nlp\")\n",
    "# Output:\n",
    "# <pd.DataFrame>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14246b6",
   "metadata": {},
   "source": [
    "### Text2Text\n",
    "\n",
    "For text generation tasks like `summarization` and translation tasks, it converts the annotations of a record `text` and `target` columns.\n",
    "By passing the `framework` variable as `transformers`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "dataset_rg = rg.DatasetForText2Text()(\n",
    "    [\n",
    "        rg.Text2TextRecord()(\n",
    "            text=\"I live in Madrid\",\n",
    "            annotation=\"I live in Spain\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_rg.prepare_for_training(framework=\"transformers\")[0]\n",
    "# Output:\n",
    "# {..., 'tokens': ['I', 'live', 'in', 'Madrid'], 'ner_tags': [0, 0, 0, 1], ...}+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2584bca9d226488c39a669ff1ce19d7ca5f410e2d3aa9b82f20653edd0d96bfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
