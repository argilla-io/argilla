{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Metrics to Evaluate Your Model\n",
    "\n",
    "In this part of our end-to-end series, we will evaluate the annotation results of our dataset using the `metrics` module. To see the previous steps, you can refer to the tutorials such as [creating the dataset](./create-dataset-001.ipynb), [adding responses and suggestions](./add-resoponses) or [training your model](./train-model-006.ipynb). Feel free to check out the [practical guides](../../../../practical_guides/practical_guides.md) page for more in-depth information.\n",
    "\n",
    "After having your dataset annotated by the annotators, it is strongly recommended to evaluate the annotation results. This is especially important if you are planning to use the dataset for training your model. The `metrics` module provides you with the necessary tools to compare the annotated data against suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![workflow](../../../../_static/tutorials/end2end/base/workflow_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Pull the Dataset](#Pull-the-Dataset)\n",
    "    1. [From Argilla](#From-Argilla)\n",
    "    2. [From HuggingFace Hub](#From-HuggingFace-Hub)\n",
    "2. [Unify Responses](#Unify-Responses)\n",
    "3. [Annotator Metrics](#Annotator-Metrics)\n",
    "    1. [Metrics per Annotator](#Metrics-per-Annotator)\n",
    "    2. [Metrics for Unified Responses](#Metrics-for-Unified-Responses)\n",
    "4. [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Argilla\n",
    "\n",
    "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
    "\n",
    "**Deploy Argilla on Hugging Face Spaces:** If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:\n",
    "\n",
    "[![deploy on spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/new-space?template=argilla/argilla-template-space)\n",
    "\n",
    "For details about configuring your deployment, check the [official Hugging Face Hub guide](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla).\n",
    "\n",
    "**Launch Argilla using Argilla's quickstart Docker image**: This is the recommended option if you want [Argilla running on your local machine](../../../../getting_started/quickstart.md). Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
    "\n",
    "For more information on deployment options, please check the Deployment section of the documentation.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Tip\n",
    "\n",
    "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
    "\n",
    "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
    "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install our dependencies and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install argilla\n",
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from argilla.client.feedback.metrics.annotator_metrics import AnnotatorMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook we will need some credentials to push and load datasets from `Argilla` and `ðŸ¤— Hub`, let's set them in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argilla credentials\n",
    "api_url = \"http://localhost:6900\"  # \"https://<YOUR-HF-SPACE>.hf.space\"\n",
    "api_key = DEFAULT_API_KEY  # admin.apikey\n",
    "# Huggingface credentials\n",
    "hf_token = \"hf_...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in to Argilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.init(api_url=api_url, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Telemetry\n",
    "We gain valuable insights from how you interact with our tutorials. To improve ourselves in offering you the most suitable content, using the following lines of code will help us understand that this tutorial is serving you effectively. Though this is entirely anonymous, you can choose to skip this step if you prefer. For more info, please check out the Telemetry page.\n",
    "\n",
    "```python\n",
    "from argilla.utils.telemetry import tutorial_running\n",
    "\n",
    "tutorial_running()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull the Dataset\n",
    "\n",
    "To employ metrics, we can pull a dataset that consists of multiple annotations per record. We can do this either from Argilla or HuggingFace Hub. Let us see how we can pull it from both sources.\n",
    "\n",
    "### From Argilla\n",
    "\n",
    "We can pull the dataset from Argilla by using the `from_argilla` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.FeedbackDataset.from_argilla(\"argilla/go_emotions_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From HuggingFace Hub\n",
    "\n",
    "We can also pull the dataset from HuggingFace Hub. Similarly, we can use the `from_huggingface` method to pull the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.FeedbackDataset.from_huggingface(\"argilla/go_emotions_raw\", split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note \n",
    "\n",
    "The dataset pulled from HuggingFace Hub is an instance of `FeedbackDataset` whereas the dataset pulled from Argilla is an instance of `RemoteFeedbackDataset`. The difference between the two is that the former is a local one and the changes made on it stay locally. On the other hand, the latter is a remote one and the changes made on it are directly reflected on the dataset on the Argilla server, which can make your process faster.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us briefly examine what our dataset looks like. It is a dataset that consists of data items with the field `text`. For each record, we have multiple annotations that label the text with at least one sentiment. Let us see an example of a text and the given responses. In this example, the record has been annotated by 3 annotators and one of them has labeled the text with one sentiment while the other two have labeled it with two sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  And not all children's hospitals need the same stuff, so call and ask what they need. But I like your tip. You're correct. \n",
      "responses: [['neutral'], ['approval', 'desire'], ['approval', 'love']]\n"
     ]
    }
   ],
   "source": [
    "print(\"text:\", dataset[5].fields[\"text\"])\n",
    "print(\"responses:\", [dataset[5].responses[i].values[\"label\"].value for i in range(len(dataset[5].responses))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have multiple annotations per record in your project, it is a good practice to unify the responses to have a single response per record. This is preferable as it makes the dataset more consistent and easier to work with. Let us see how we can unify the responses with Argilla. First, we create a strategy to unify the responses. We go with the `majority` vote strategy, which means that we will keep the responses that have been suggested by the majority of the annotators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = rg.MultiLabelQuestionStrategy(\"majority\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_unified_responses(\n",
    "    question=dataset.question_by_name(\"label\"),\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a record to see how the responses have been unified. In our case, the responses have been unified to `approval` as it is the majority vote among the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [UnifiedValueSchema(value=['approval'], strategy=<RatingQuestionStrategy.MAJORITY: 'majority'>)]}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.records[5].unified_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotator Metrics\n",
    "\n",
    "Annotator metrics refer to the metrics where the responses from each annotator (or the unified response) are compared against the suggestions we have added to the model. This type of metric is useful to see how each annotator individually or all annotators collectively perform on the dataset. Argilla offers various metrics and you can access the list of the available ones by referring to the [metrics](#linkt-to-metrics) page. Alternatively, you can discover the available metrics by using the `allowed_metrics` method.\n",
    "\n",
    "The question type we have in the current dataset is `MultiLabelQuestion`. By using the `allowed_metrics` method, we can see the metrics below, which are available for this question type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy', 'f1-score', 'precision', 'recall', 'confusion-matrix']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = AnnotatorMetric(dataset=dataset, question_name=\"label\")\n",
    "metric.allowed_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics per Annotator\n",
    "\n",
    "In AnnotatorMetrics, we have two options to pick the ground truths, against which we want to make the comparison: responses or suggestions. You can have more info regarding this on [this page](#annotator-metrics).\n",
    "\n",
    "Let us first select responses as our ground truths and compare the suggestions to them. \n",
    "\n",
    "To calculate the metrics per annotator, we can straightforwardly use the `compute_response_metrics` on our dataset. We set the arguments as the question name and the name of the metrics we want to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_metrics = dataset.compute_responses_metrics(question_name=\"label\", metric_names=[\"accuracy\", \"precision\", \"recall\", \"f1-score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dictionary whose keys are the IDs of annotators and values are the metrics at hand. We can look at the score of any annotator by giving the particular ID.\n",
    "\n",
    "We see that the annotator below has a total of 182 annotations while their scores are in mid between 0.4 and 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AnnotatorMetricResult(metric_name='accuracy', count=182, result=0.5714285714285714),\n",
       " AnnotatorMetricResult(metric_name='precision', count=182, result=0.4427905213343358),\n",
       " AnnotatorMetricResult(metric_name='recall', count=182, result=0.5377066798941799),\n",
       " AnnotatorMetricResult(metric_name='f1-score', count=182, result=0.428750352375672)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_metrics[\"00000000-0000-0000-0000-000000000004\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, we can alternatively use the suggestions as our ground truths. Let us see how the annotators perform when we use the suggestions as our ground truths. Besides, we will be calculating all possible ones by feeding `metric.allowed_metrics` to the `compute_response_metrics` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions_metrics = dataset.compute_suggestions_metrics(question_name=\"label\", metric_names=metric.allowed_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that the annotator below has a total of 182 annotations while their scores are in mid between 0.4 and 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AnnotatorMetricResult(metric_name='accuracy', count=182, result=0.5714285714285714),\n",
       " AnnotatorMetricResult(metric_name='f1-score', count=182, result=0.428750352375672),\n",
       " AnnotatorMetricResult(metric_name='precision', count=182, result=0.5377066798941799),\n",
       " AnnotatorMetricResult(metric_name='recall', count=182, result=0.4427905213343358),\n",
       " AnnotatorMetricResult(metric_name='confusion-matrix', count=182, result={'admiration':                             suggestions_admiration_true  \\\n",
       " responses_admiration_true                           174   \n",
       " responses_admiration_false                            5   \n",
       " \n",
       "                             suggestions_admiration_false  \n",
       " responses_admiration_true                              0  \n",
       " responses_admiration_false                             3  , 'amusement':                            suggestions_amusement_true  \\\n",
       " responses_amusement_true                          176   \n",
       " responses_amusement_false                           3   \n",
       " \n",
       "                            suggestions_amusement_false  \n",
       " responses_amusement_true                             1  \n",
       " responses_amusement_false                            2  })]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggestions_metrics[\"00000000-0000-0000-0000-000000000004\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Unified Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better understanding of the general performance of the annotators, we can unify the responses before calculating the metrics. This will be simply done by using the extra `strategy` argument in the `compute_response_metrics` method. When this argument is set, Argilla will unify the responses and then compare the unified responses against the suggestions or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_metrics_unified = dataset.compute_responses_metrics(question_name=\"label\", metric_names=[\"accuracy\", \"precision\", \"recall\", \"f1-score\"], strategy=\"majority\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AnnotatorMetricResult(metric_name='accuracy', count=1000, result=0.803),\n",
       " AnnotatorMetricResult(metric_name='precision', count=1000, result=0.7865376025406677),\n",
       " AnnotatorMetricResult(metric_name='recall', count=1000, result=0.8002996311981446),\n",
       " AnnotatorMetricResult(metric_name='f1-score', count=1000, result=0.7836689489727956)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_metrics_unified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if want to select suggestions as our ground truths and calculate the metrics against them, we can do it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions_metrics_unified = dataset.compute_suggestions_metrics(question_name=\"label\", metric_names=[\"accuracy\", \"precision\", \"recall\", \"f1-score\"], strategy=\"majority\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AnnotatorMetricResult(metric_name='accuracy', count=1000, result=0.799),\n",
       " AnnotatorMetricResult(metric_name='precision', count=1000, result=0.7653116748179684),\n",
       " AnnotatorMetricResult(metric_name='recall', count=1000, result=0.7669313732138586),\n",
       " AnnotatorMetricResult(metric_name='f1-score', count=1000, result=0.7480244903078532)]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggestions_metrics_unified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we have seen how we can evaluate the annotation results of our dataset using the `metrics` module. We have first unified the response to have a more comprehensive outlook on the annotations. Then, we have calculated the metrics per annotator and for the unified responses. We have also seen how we can select the ground truths as responses or suggestions. If you feel that the annotations are not satisfactory, you can reiterate the annotation process by making changes in the structure of your project. You can refer to the [practical guides](../../../../practical_guides/practical_guides.md) to refine your structure or check out the [advanced tutorials](../../../../tutorials.md) to learn more about the advanced use cases of Argilla."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
