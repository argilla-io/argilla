{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ⓜ️ Finetuning LLMs as chat assistants: Supervised Finetuning on Mistral 7B\n",
    "In this tutorial, you will learn how to finetune a Large Language Model (LLM), Mistral 7B in particular, on a chat-style instruction dataset. We start with [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1), an LLM that only does text completion, and we end up with our own [argilla/Mistral-7B-v0.1-chat-OIG](https://huggingface.co/argilla/Mistral-7B-v0.1-chat-OIG) model that faithfully follows instructions and acts as a helpful chat assistant.\n",
    "\n",
    "This tutorial consists of the following steps:\n",
    "1. Preparing a [FeedbackDataset](https://docs.argilla.io/en/latest/conceptual_guides/data_model.html#feedback-dataset) in Argilla. <!-- using a public chat-style instruction dataset-->\n",
    "2. (Optional) Annotate instruction samples.\n",
    "3. Set up the [ArgillaTrainer](https://docs.argilla.io/en/latest/practical_guides/fine_tune.html#supervised-finetuning) for Supervised Finetuning.\n",
    "4. Perform inference using the finetuned LLM.\n",
    "5. Publish the resulting model and dataset on the Hugging Face Hub.\n",
    "\n",
    "For this tutorial, we used the `p3.8xlarge` AWS instance and trained for a total of 70 minutes. In terms of compute, training the Mistral chat model cost us less than $15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Mistral 7B\n",
    "In this tutorial we will be finetuning Mistral 7B, which is a powerful LLM developed by the Mistral AI team featuring 7.3 billion parameters. It stands out for its exceptional performance relative to its size, outperforming larger models like Llama 2 13B and Llama 1 34B on various benchmarks. \n",
    "\n",
    "Another key aspect is its ability to perform well for longer sequences, and noticeably, it is released under the permissive Apache 2.0 license. This allows it to be used in commercial use cases with no strings attached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Argilla\n",
    "\n",
    "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
    "\n",
    "**Deploy Argilla on Hugging Face Spaces:** If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:\n",
    "\n",
    "[![deploy on spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/new-space?template=argilla/argilla-template-space)\n",
    "\n",
    "For details about configuring your deployment, check the [official Hugging Face Hub guide](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla).\n",
    "\n",
    "**Launch Argilla using Argilla's quickstart Docker image**: This is the recommended option if you want [Argilla running on your local machine](../../getting_started/quickstart.ipynb). Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
    "\n",
    "For more information on deployment options, please check the Deployment section of the documentation.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Tip\n",
    "\n",
    "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
    "\n",
    "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
    "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "Let's start by installing the required dependencies to run both Argilla and the remainder of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"argilla~=1.16.0\" \"transformers~=4.34.0\" \"datasets~=2.14.5\" \"peft~=0.5.0\" \"trl~=0.7.1\" \"wandb~=0.15.12\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you must also install `torch` with `CUDA` support. The widget [here](https://pytorch.org/get-started/locally) should be helpful if `torch` is not already set up.\n",
    "\n",
    "Let's import the Argilla module for reading and writing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the `URL` and `API_KEY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "rg.init(\n",
    "    api_url=\"http://localhost:6900\",\n",
    "    api_key=\"owner.apikey\",\n",
    "    workspace=\"admin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running a private Hugging Face Space, you will also need to set the [HF_TOKEN](https://huggingface.co/settings/tokens) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the HF_TOKEN environment variable\n",
    "# import os\n",
    "# os.environ['HF_TOKEN'] = \"your-hf-token\"\n",
    "\n",
    "# # Replace api_url with the url to your HF Spaces URL\n",
    "# # Replace api_key if you configured a custom API key\n",
    "# # Replace workspace with the name of your workspace\n",
    "# rg.init(\n",
    "#     api_url=\"https://[your-owner-name]-[your_space_name].hf.space\", \n",
    "#     api_key=\"owner.apikey\",\n",
    "#     workspace=\"admin\",\n",
    "#     extra_headers={\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Telemetry\n",
    "\n",
    "We gain valuable insights from how you interact with our tutorials. To improve ourselves in offering you the most suitable content, using the following lines of code will help us understand that this tutorial is serving you effectively. Though this is entirely anonymous, you can choose to skip this step if you prefer. For more info, please check out the [Telemetry](../../reference/telemetry.md) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from argilla.utils.telemetry import tutorial_running\n",
    "    tutorial_running()\n",
    "except ImportError:\n",
    "    print(\"Telemetry is introduced in Argilla 1.20.0 and not found in the current installation. Skipping telemetry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a FeedbackDataset in Argilla\n",
    "In Argilla, the [FeedbackDataset](https://docs.argilla.io/en/latest/conceptual_guides/data_model.html#feedback-dataset) is a powerful and widely-configurable class that is in charge of defining the annotation process. In particular, we define [fields](https://docs.argilla.io/en/latest/conceptual_guides/data_model.html#field) and [questions](https://docs.argilla.io/en/latest/conceptual_guides/data_model.html#question).\n",
    "\n",
    "The former is in charge of defining the structure for the data that will be annotated, while the latter determines in what way the annotators can annotate the data. In practice, `FeedbackDataset` instances for finetuning LLMs often have \"prompt\" and \"response\" text fields, sometimes alongside a \"context\" text field or some additional metadata (e.g. sample IDs).\n",
    "\n",
    "The questions provide a lot of flexibility - text questions allow annotators to provide better responses or suggestions to improve the prompt, while label questions may be used to select if a sample is considered \"good\" or not. With multi-label questions annotators can select whether samples are biased, harmful, incorrect, etc.\n",
    "\n",
    "We recommend to set up the questions based on your goals and values for the annotation process. When preparing the training data, all of the annotations will be available, allowing you to carefully curate your training data. For example, based on your annotations you will be able to disregard samples that are (frequently) marked as harmful or low quality, or you can use the annotator-provided suggestions to responses or prompts instead of the existing ones.\n",
    "Furthermore, in some situations you may only have prompts to begin with, and you can use Argilla to request your annotators to provide responses. You can then use these in your training data.\n",
    "\n",
    "### For this tutorial\n",
    "\n",
    "For the purposes of this tutorial, we will use a \"prompt\" field with all chat history and the current prompt, as well as a \"response\" field with the response. Additionally, we introduce a \"background\" text field, which is sometimes used to provide additional background information prior to the user prompt. An example data sample is shown a few cells below, to help clarify what the background entails. For the questions we will use one simple `LabelQuestion` that asks the annotator whether the response is `\"Good\"` or `\"Bad\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.FeedbackDataset(\n",
    "    fields = [\n",
    "        rg.TextField(name=\"background\"),\n",
    "        rg.TextField(name=\"prompt\"),\n",
    "        rg.TextField(name=\"response\", title=\"Final Response\"),\n",
    "    ],\n",
    "    questions = [\n",
    "        rg.LabelQuestion(name=\"quality\", title=\"Is it a Good or Bad response?\", labels=[\"Good\", \"Bad\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, we have to create individual [records](https://docs.argilla.io/en/latest/conceptual_guides/data_model.html#record) and push those to the dataset. For this tutorial, we will use some existing Apache-2.0 data from the [Open Instruction Generalist](https://huggingface.co/datasets/laion/OIG) chat-style instruction dataset. Due to the large size of this dataset, we will load the `dataset` with `streaming=True`, and semi-randomly sample 30k samples from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"laion/OIG\", split=\"train\", streaming=True)\n",
    "# Randomly sample through shuffle + take, but note that shuffling with a streaming\n",
    "# dataset works with a buffer size, so the random 30k samples are not truly random\n",
    "# Reduce the buffer size to speed up the data selection process.\n",
    "data = data.shuffle(buffer_size=1_000_000).take(30_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded data has a \"text\" column with data that looks like so:\n",
    "```\n",
    "Background: Tubulin in molecular biology can refer either to the tubulin protein superfamily of globular proteins, or one of the member proteins of that superfamily. α- and β-tubulins polymerize into microtubules, a major component of the eukaryotic cytoskeleton. Microtubules function in many essential cellular processes, including mitosis. Tubulin-binding drugs kill cancerous cells by inhibiting microtubule dynamics, which are required for DNA segregation and therefore cell division. , there are six members of the tubulin superfamily, although not all are present in all species. Both α and β tubulins have a mass of around 50 kDa and are thus in a similar range compared to actin (with a mass of ~42 kDa). In contrast, tubulin polymers (microtubules) tend to be much bigger than actin filaments due to their cylindrical nature. Tubulin was long thought to be specific to eukaryotes. More recently, however, several prokaryotic proteins have been shown to be related to tubulin. <human>: What are some examples of drugs that bind to tubulin? <bot>: These include paclitaxel, colchicine, and the \"vinca\" alkaloids, each of which have a distinct binding site on β-tubulin. <human>: Are there any other examples? <bot>: In addition, the anti-worm drug albendazole selectively targets the colchicine site in worm, but not human or \"Drosophilia\", β-Tubulin. <human>: What is another specific example of a drug that binds to tubulin? <bot>: Class III β-tubulin is a microtubule element expressed exclusively in neurons, and is a popular identifier specific for neurons in nervous tissue. <human>: What is the difference between class III and other isotypes? <bot>: It binds colchicine much more slowly than other isotypes of β-tubulin. <human>: Are there six classes of isotypes? <bot>: β1-tubulin, sometimes called class VI β-tubulin, is the most divergent at the amino acid sequence level. <human>: What are the functions of the different isotypes? <bot>: It is expressed exclusively in megakaryocytes and platelets in humans and appears to play an important role in the formation of platelets.\n",
    "```\n",
    "Let's break this down:\n",
    "- The text uses `<human>:` and `<bot>:` to denote the start of the user-provided and generated texts, respectively.\n",
    "- The text uses `Background:` to denote some additional context that is relevant for the chat. Not all samples contain any background.\n",
    "- The dataset contains multi-step chat-style instructions.\n",
    "\n",
    "To fit this in our `FeedbackDataset`, we have to split this up into a \"background\", a \"prompt\" with all chat history and the last prompt, and the final \"response\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def extract_background_prompt_response(text: str) -> Dict[str, Any]:\n",
    "    '''Extract the anthropic prompt from a prompt and response pair.'''\n",
    "    start_prompt = text.find(\"<human>:\")\n",
    "    end_prompt = text.rfind(\"<bot>:\")\n",
    "    # Background is anything before the first <human>:\n",
    "    background = text[:start_prompt].strip()\n",
    "    # Prompt is anything between the first <human>: (inclusive) and the last <bot>: (exclusive)\n",
    "    prompt = text[start_prompt: end_prompt].strip()\n",
    "    # Response is everything after the last <bot>: (inclusive)\n",
    "    response = text[end_prompt:].strip()\n",
    "    return {\"background\": background, \"prompt\": prompt, \"response\": response}\n",
    "\n",
    "\n",
    "data = data.map(extract_background_prompt_response, input_columns=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can trivially convert this dataset into `FeedbackRecord` instances, and add them to the `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    rg.FeedbackRecord(\n",
    "        fields={\n",
    "            \"background\": sample[\"background\"],\n",
    "            \"prompt\": sample[\"prompt\"],\n",
    "            \"response\": sample[\"response\"],\n",
    "        },\n",
    "    )\n",
    "    for sample in data\n",
    "]\n",
    "dataset.add_records(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the records locally, we want to push them to the Argilla server as well. Only then will we be able to see them in the Argilla UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_argilla(\"oig-30k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once pushed, we can always load this data again using `load_from_argilla`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.FeedbackDataset.from_argilla(\"oig-30k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training-llm-mistral-sft](../../../_static/images/llms/training-llm-mistral-sft/training-llm-mistral-sft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Annotate instruction samples\n",
    "If you are using your own proprietary data, data from an unreliable source, or require on your annotators to provide responses, then you must perform data annotation. However, for the purposes of this tutorial, we will assume that all data is high quality and skip this step. See the [data collection for LLMs](https://docs.argilla.io/en/latest/conceptual_guides/llm/llm.html) documentation for additional information on this phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the ArgillaTrainer for Supervised Finetuning\n",
    "### Model & Tokenizer\n",
    "Next, we have to set up the [ArgillaTrainer](https://docs.argilla.io/en/latest/practical_guides/fine_tune.html#supervised-finetuning). First off, we will load the Mistral 7B `model` and `tokenizer`. We'll load the model using `float16` to improve the memory usage and efficiency, and `device_map=\"auto\"` automatically picks the best device to load the model on. For example, this will prioritize your GPU before your CPU.\n",
    "\n",
    "Furthermore, setting the `pad_token_id` to `eos_token_id` is required for open-end generation. If you don't define `pad_token_id`, it is often set to `eos_token_id` already, but you will be given warnings that you should do it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Task\n",
    "Next, we can set up the [TrainingTask](https://docs.argilla.io/en/latest/practical_guides/fine_tune.html#the-trainingtask) for supervised finetuning. This task requires a `formatting_func` that formats the data from Argilla in preparation for training. This formatting function first checks if the data quality is up to par. This is done by inspecting the annotations and checking if a sample was not annotated, annotated as \"Bad\" or discarded. For the purposes of the tutorial, I'll ignore this part and consider all data to be high quality.\n",
    "\n",
    "Then, we convert the data to our desired chat format, where each step consists of:\n",
    "```\n",
    "<s><human>: {prompt} <bot>: {response}</s>\n",
    "```\n",
    "or\n",
    "```\n",
    "Background: {background} <s><human>: {prompt} <bot>: {response}</s>\n",
    "```\n",
    "In this format, `<s>` and `</s>` are the BOS and EOS tokens of the LLM. If a different model is being finetuned, then these tokens must be changed to the correct BOS and EOS tokens. When dealing with multiple sequential steps, the chat is formatted like so:\n",
    "```\n",
    "<s><human>: {prompt_1} <bot>: {response_1}</s><s><human>: {prompt_2} <bot>: {response_2}</s><s><human>: {prompt_3} <bot>: {response_3}</s>\n",
    "```\n",
    "\n",
    "When generating, we can supply the LLM with the following format:\n",
    "```\n",
    "<s><human>: {prompt_1} <bot>: {response_1}</s><s><human>: {prompt_2} <bot>:\n",
    "```\n",
    "and the model will generate the assistant response given the history, followed by `</s>`. The generation will naturally stop at this EOS marker. If the user chooses to respond again, then we can again add `<s><human>: {prompt} <bot>: ` to the prior outputs and generate some more responses.\n",
    "\n",
    "Note that the OIG data itself does not contain these BOS and EOS markers, so we need to add them manually between each of the prompt-response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterator, Any\n",
    "from argilla.feedback import TrainingTask\n",
    "\n",
    "ANNOTATED_ONLY = False\n",
    "\n",
    "def formatting_func(sample: Dict[str, Any]) -> Iterator[str]:\n",
    "    if ANNOTATED_ONLY:\n",
    "        # Discard if there are no annotations...\n",
    "        if not sample[\"quality\"]:\n",
    "            return\n",
    "\n",
    "        # or if it is annotated as \"Bad\" or discarded.\n",
    "        first_annotation = sample[\"quality\"][0]\n",
    "        if first_annotation[\"value\"] == \"Bad\" or first_annotation[\"status\"] == \"discarded\":\n",
    "            return\n",
    "\n",
    "    # Filter out responses that are likely low quality\n",
    "    if len(sample[\"response\"]) <= 2:\n",
    "        return\n",
    "\n",
    "    # Add </s><s> between all prompt-response pairs\n",
    "    prompt = sample[\"prompt\"]\n",
    "    prompt = prompt.replace(\"<human>:\", f\"{tokenizer.eos_token}{tokenizer.bos_token}<human>:\")\n",
    "    prompt = prompt[prompt.find(\"<human>:\"):]\n",
    "    # Add response and optionally the background to the full text.\n",
    "    output = prompt + \" \" + sample[\"response\"]\n",
    "    if sample[\"background\"]:\n",
    "        output = sample[\"background\"] + \" \" + output\n",
    "    output = output + tokenizer.eos_token\n",
    "    # We expect one less <s> than </s>, because the Mistral tokenizer will automatically add the BOS\n",
    "    # at the start of the text when this text is tokenized. When that's done, the format will be exactly\n",
    "    # what we want\n",
    "    assert output.count(\"<s>\") + 1 == output.count(\"</s>\")\n",
    "    return output\n",
    "\n",
    "task = TrainingTask.for_supervised_fine_tuning(formatting_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times it'll be convenient to have a manual look at the data before we start the training, just to verify that the data is indeed correctly formatted. Luckily, Argilla implements a method to apply the `TrainingTask` over a `FeedbackDataset` for a specific training framework, and you'll get the training data just like how that particular framework wants to have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 23806\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset = dataset.prepare_for_training(framework=\"trl\", task=task)\n",
    "formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: The neopositivists subscribed to a notion of philosophy as the conceptual clarification of the methods, insights and discoveries of the sciences. They saw in the logical symbolism elaborated by Frege (1848–1925) and Bertrand Russell (1872–1970) a powerful instrument that could rationally reconstruct all scientific discourse into an ideal, logically perfect, language that would be free of the ambiguities and deformations of natural language. This gave rise to what they saw as metaphysical pseudoproblems and other conceptual confusions. By combining Frege's thesis that all mathematical truths are logical with the early Wittgenstein's idea that all logical truths are mere linguistic tautologies, they arrived at a twofold classification of all propositions: the analytic (a priori) and the synthetic (a posteriori). On this basis, they formulated a strong principle of demarcation between sentences that have sense and those that do not: the so-called verification principle. Any sentence that is not purely logical, or is unverifiable is devoid of meaning. As a result, most metaphysical, ethical, aesthetic and other traditional philosophical problems came to be considered pseudoproblems.\n",
      "\n",
      "When was Bertrand Russell born?\n",
      "<bot>: 1872\n",
      "</s><s><human>: When did Bertrand Russell die?\n",
      "<bot>: 1970\n",
      "</s><s><human>: What did Frege say about math?\n",
      "<bot>: all mathematical truths are logical\n",
      "</s><s><human>: What did Wittgenstein say about logic?\n",
      "<bot>: all logical truths are mere linguistic tautologies\n",
      "</s><s><human>: What are pseudoproblems?\n",
      "<bot>: most metaphysical, ethical, aesthetic and other traditional philosophical problems\n",
      "</s><s><human>: When was Frege's thesis published?\n",
      "<bot>: Sorry, I cannot find a relevant answer in the given context.\n",
      "</s><s><human>: When was Wittgenstein born?\n",
      "<bot>: Sorry, I cannot find a relevant answer in the given context.\n",
      "</s><s><human>: When did Wittgenstein die?\n",
      "<bot>: Sorry, I cannot find a relevant answer in the given context.\n",
      "</s><s><human>: Who was a neopositivist?\n",
      "<bot>: Sorry, I cannot find a relevant answer in the given context.\n",
      "</s><s><human>: What gave a sentence meaning? <bot>: Sorry, I cannot find a relevant answer in the given context.</s>\n"
     ]
    }
   ],
   "source": [
    "print(formatted_dataset[80][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to match our desired format exactly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collation\n",
    "When training LLMs, we want to verify that the text is tokenized and collated correctly. In particular, the data collation can be quite important, as some data collators will replace all special tokens (e.g. padding, BOS and EOS tokens) with a label of -100, which is the label that is ignored during training. If this is the case, then the model will not learn to produce the EOS token. Consequently, the model will always respond with endless tokens - far from ideal.\n",
    "\n",
    "By default, models trained with supervised finetuning use the [DataCollatorForLanguageModeling](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForLanguageModeling) data collator, so let's experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   523, 18529,  9670, 12628,   272,  2296,   808,   302, 11382,\n",
       "         28725,  3133,  8373, 28747,  5936, 16280,  4969,  1059,  9697,   438,\n",
       "          1830,   647,   464, 20746, 18566,  9917,  3578,  1996,   378,   533,\n",
       "          5446, 28705, 28770,  2421,   647,   464,  1733,   824,  2516,  9746,\n",
       "          7230,  5573, 10487,  3578,  1421,  2063,  4372,   272,  2996,   464,\n",
       "          5985,   272,  2078,  5944,   297,  1745,  3725,   395,   264,   464,\n",
       "          5613, 28742,   442,   464,  2501,  4135,   523, 10093,  9670,  1770,\n",
       "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[    1,   523, 18529,  9670, 12628,   272,  2296,   808,   302, 11382,\n",
       "         28725,  3133,  8373, 28747,  5936, 16280,  4969,  1059,  9697,   438,\n",
       "          1830,   647,   464, 20746, 18566,  9917,  3578,  1996,   378,   533,\n",
       "          5446, 28705, 28770,  2421,   647,   464,  1733,   824,  2516,  9746,\n",
       "          7230,  5573, 10487,  3578,  1421,  2063,  4372,   272,  2996,   464,\n",
       "          5985,   272,  2078,  5944,   297,  1745,  3725,   395,   264,   464,\n",
       "          5613, 28742,   442,   464,  2501,  4135,   523, 10093,  9670,  1770,\n",
       "          -100]])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "data_collator([tokenizer(formatted_dataset[0][\"text\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the final EOS is indeed set to `-100`, meaning that it would not be learned. Instead, we will create a custom data collator that directly copies the `input_ids` to the `labels`. This is trivial by subclassing the [`DataCollatorForSeq2Seq`](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForSeq2Seq) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, BatchEncoding\n",
    "\n",
    "class DataCollatorForSeq2SeqCopyLabels(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features, return_tensors=None) -> BatchEncoding:\n",
    "        for feature in features:\n",
    "            if \"labels\" not in feature:\n",
    "                feature[\"labels\"] = feature[\"input_ids\"].copy()\n",
    "        return super().__call__(features, return_tensors=return_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   523, 18529,  9670, 12628,   272,  2296,   808,   302, 11382,\n",
       "         28725,  3133,  8373, 28747,  5936, 16280,  4969,  1059,  9697,   438,\n",
       "          1830,   647,   464, 20746, 18566,  9917,  3578,  1996,   378,   533,\n",
       "          5446, 28705, 28770,  2421,   647,   464,  1733,   824,  2516,  9746,\n",
       "          7230,  5573, 10487,  3578,  1421,  2063,  4372,   272,  2996,   464,\n",
       "          5985,   272,  2078,  5944,   297,  1745,  3725,   395,   264,   464,\n",
       "          5613, 28742,   442,   464,  2501,  4135,   523, 10093,  9670,  1770,\n",
       "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[    1,   523, 18529,  9670, 12628,   272,  2296,   808,   302, 11382,\n",
       "         28725,  3133,  8373, 28747,  5936, 16280,  4969,  1059,  9697,   438,\n",
       "          1830,   647,   464, 20746, 18566,  9917,  3578,  1996,   378,   533,\n",
       "          5446, 28705, 28770,  2421,   647,   464,  1733,   824,  2516,  9746,\n",
       "          7230,  5573, 10487,  3578,  1421,  2063,  4372,   272,  2996,   464,\n",
       "          5985,   272,  2078,  5944,   297,  1745,  3725,   395,   264,   464,\n",
       "          5613, 28742,   442,   464,  2501,  4135,   523, 10093,  9670,  1770,\n",
       "             2]])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2SeqCopyLabels(tokenizer)\n",
    "data_collator([tokenizer(formatted_dataset[0][\"text\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see `2` at the very end of `labels`, i.e. the EOS token, just like we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Callback\n",
    "\n",
    "When training LLMs, it's always recommended to perform some form of generation during training. This is crucial as the loss alone is not a good indicator of model performance, and it is the primary method to gauge if the model is going in the right direction and learning like expected. For this purpose, we will create a callback that generates some text and prints it out on every evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "from transformers import TrainerCallback, TrainerControl, TrainerState, GenerationConfig, TrainingArguments, PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class GenerationCallback(TrainerCallback):\n",
    "    def __init__(self, prompt: str) -> None:\n",
    "        super().__init__()\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, model: Optional[PreTrainedModel] = None, tokenizer: Optional[PreTrainedTokenizer] = None, **kwargs):\n",
    "        # Tokenize the prompt and send it to the right device\n",
    "        inputs = tokenizer(self.prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                generation_config=GenerationConfig(\n",
    "                    max_new_tokens=50,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                ),\n",
    "            )\n",
    "            print(tokenizer.batch_decode(outputs, skip_special_tokens=False)[0])\n",
    "\n",
    "\n",
    "generation_callback = GenerationCallback(\"<human>: What were Nelson Mandela's relations with the ANC? <bot>:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArgillaTrainer & hyperparameters\n",
    "Next, we can initialize the [ArgillaTrainer](https://docs.argilla.io/en/latest/practical_guides/fine_tune.html#the-argillatrainer)! We have already prepared all of the components that it requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.feedback import ArgillaTrainer\n",
    "\n",
    "trainer = ArgillaTrainer(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=task,\n",
    "    framework=\"trl\",\n",
    "    train_size=0.99,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to configure the trainer with the desired arguments settings for training. We will start with configuration options for the TRL [SFTTrainer](https://huggingface.co/docs/trl/trainer#trl.SFTTrainer). This trainer accepts a [PEFT](https://github.com/huggingface/peft) config, allowing us to use the awesome [LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora). This technique accelerates the fine-tuning of large models while consuming less memory. Upon training, this will produce small `adapter_config.json` and `adapter_model.bin` files, which can be combined with the original model to produce the finetuned model. See [this documentation](https://huggingface.co/docs/peft/quicktour#easy-loading-with-auto-classes) for information on how to load these models again.\n",
    "\n",
    "We also set the maximum sequence length to 1024 as a way to keep the memory usage down, and we provide the trainer with our custom `data_collator` and `generation_callback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "trainer.update_config(\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[generation_callback],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond that, we want to configure the [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) to set the hyperparameters. Note that we only train with 3000 steps here. This already proved sufficient to finetune a reasonable model that follows our chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.update_config(\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    eval_accumulation_steps=16,\n",
    "    max_steps=3000,\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    save_strategy=\"no\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    num_train_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, all that remains is training the model. We provide an output directory for where the adapter files are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\"Mistral-7B-v0.1-chat-OIG-3k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform inference using the finetuned LLM\n",
    "After training, the model is still in memory as `model`, but often, times we will want to load the trained model anew. This is simple using [AutoPeftModelForCausalLM](https://huggingface.co/docs/peft/quicktour#easy-loading-with-auto-classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.14s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"Mistral-7B-v0.1-chat-OIG-3k\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, inference becomes simple too. See the [generate()](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationMixin.generate) method for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <human>: What were Nelson Mandela's relations with the ANC? <bot>: Nelson Mandela was a member of the ANC.</s>\n"
     ]
    }
   ],
   "source": [
    "text = \"<human>: What were Nelson Mandela's relations with the ANC? <bot>:\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=250, pad_token_id=tokenizer.pad_token_id)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=False)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <human>: What were Nelson Mandela's relations with the ANC? <bot>: Nelson Mandela was a member of the ANC.</s><s> <human>: How old was he when he joined? <bot>: 22</s>\n"
     ]
    }
   ],
   "source": [
    "text = \"<human>: What were Nelson Mandela's relations with the ANC? <bot>: Nelson Mandela was a member of the ANC.</s><s><human>: How old was he when he joined? <bot>: \"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=250, pad_token_id=tokenizer.pad_token_id)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish the resulting model and dataset on the Hugging Face Hub\n",
    "Lastly, we will want to save our trained model and the dataset to the Hugging Face Hub, even privately. Let's start with the adapter model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"argilla/Mistral-7B-v0.1-chat-OIG\"\n",
    "model.push_to_hub(model_id, private=True)\n",
    "tokenizer.push_to_hub(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resulted in the [argilla/Mistral-7B-v0.1-chat-OIG](https://huggingface.co/argilla/Mistral-7B-v0.1-chat-OIG) on the Hub. We can save the dataset to the Hub like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.FeedbackDataset.from_argilla(\"oig-30k\", workspace=\"admin\")\n",
    "dataset.push_to_huggingface(\"argilla/oig-30k\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which created the [argilla/oig-30k](https://huggingface.co/datasets/argilla/oig-30k) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing inference with our published model\n",
    "If you'd like to try out [the model that we trained for this tutorial](https://huggingface.co/argilla/Mistral-7B-v0.1-chat-OIG), then you can run the following snippets to load the Adapter files from the Hugging Face Hub and try out your own prompts. These cells below can be ran completely separate from the remainder of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"argilla/Mistral-7B-v0.1-chat-OIG\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to experiment with various different prompts here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<human>: Finish this sequence: purple, red, orange, yellow, ... <bot>: \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=250, pad_token_id=tokenizer.pad_token_id)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "To conclude, we have learned how to use the `ArgillaTrainer` to apply Supervised Finetuning via TRL to Mistral-7B to create a chat-style assistant model.\n",
    "\n",
    "If you're interested in finetuning LLMs, be sure to also check out these pages:\n",
    "\n",
    "- [🦾 Fine-tune LLMs and other language models](https://docs.argilla.io/en/latest/practical_guides/fine_tune.html)\n",
    "- [🪄 Fine-tuning and evaluating GPT-3.5 with human feedback for RAG](https://docs.argilla.io/en/latest/tutorials_and_integrations/tutorials/feedback/fine-tuning-openai-rag-feedback.html)\n",
    "- [🏆 Train a reward model for RLHF](https://docs.argilla.io/en/latest/tutorials_and_integrations/tutorials/feedback/train-reward-model-rlhf.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
