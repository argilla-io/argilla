{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¸ Using LLMs for Text Classification and Summarization Suggestions with `spacy-llm`\n",
    "\n",
    "In this tutorial, we'll implement a `spacy-llm` pipeline to obtain model suggestions with GPT3.5 and add them to our `FeedbackDataset` as `suggestions`. The flow of the tutorial will be:\n",
    "\n",
    "- Run Argilla and load `spacy-llm` along with other libraries\n",
    "- Define config for your pipeline and initialize it\n",
    "- Create your `FeedbackDataset` instance\n",
    "- Generate predictions on data and add them to `records`\n",
    "- Push to Argilla\n",
    "\n",
    "## Introduction\n",
    "\n",
    "[spacy-llm](https://spacy.io/usage/large-language-models) is a package that integrates the strength of LLMs into regular spaCy pipelines, thus allowing quick and practical prompting for various tasks. Besides, since it requires no training data, the models are ready to use in your pipeline. If you want to train your own model or create your custom task, `spacy-llm` also helps to implement any custom pipeline.\n",
    "\n",
    "It is quite easy to use this powerful package with Argilla Feedback datasets. We can make inferences with the pipeline we will create and add them to our `FeedbackDataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Argilla\n",
    "\n",
    "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
    "\n",
    "\n",
    "**Deploy Argilla on Hugging Face Spaces**: If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:\n",
    "\n",
    "[![deploy on spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/new-space?template=argilla/argilla-template-space)\n",
    "\n",
    "For details about configuring your deployment, check the [official Hugging Face Hub guide](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla).\n",
    "\n",
    "\n",
    "**Launch Argilla using Argilla's quickstart Docker image**: This is the recommended option if you want [Argilla running on your local machine](../../../getting_started/quickstart.md). Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
    "\n",
    "For more information on deployment options, please check the Deployment section of the documentation.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Tip\n",
    "    \n",
    "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
    "\n",
    "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
    "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "Let us first install the required libraries for our task,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"spacy-llm[transformers]\" \"transformers[sentencepiece]\" argilla datasets -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and import them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy_llm.util import assemble\n",
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "import configparser\n",
    "from collections import Counter\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to initialize the Argilla client with `API_URL` and `API_KEY`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "rg.init(\n",
    "    api_url=\"http://localhost:6900\",\n",
    "    api_key=\"admin.apikey\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running a private Hugging Face Space, you will also need to set the [HF_TOKEN](https://huggingface.co/settings/tokens) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the HF_TOKEN environment variable\n",
    "# import os\n",
    "# os.environ['HF_TOKEN'] = \"your-hf-token\"\n",
    "\n",
    "# # Replace api_url with the url to your HF Spaces URL\n",
    "# # Replace api_key if you configured a custom API key\n",
    "# rg.init(\n",
    "#     api_url=\"https://[your-owner-name]-[your_space_name].hf.space\", \n",
    "#     api_key=\"admin.apikey\",\n",
    "#     extra_headers={\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Telemetry\n",
    "\n",
    "We gain valuable insights from how you interact with our tutorials. To improve ourselves in offering you the most suitable content, using the following lines of code will help us understand that this tutorial is serving you effectively. Though this is entirely anonymous, you can choose to skip this step if you prefer. For more info, please check out the [Telemetry](../../reference/telemetry.md) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from argilla.utils.telemetry import tutorial_running\n",
    "    tutorial_running()\n",
    "except ImportError:\n",
    "    print(\"Telemetry is introduced in Argilla 1.20.0 and not found in the current installation. Skipping telemetry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `spacy-llm` pipeline\n",
    "\n",
    "To be able to use GPT3.5 and other models from OpenAI with spacy-llm, we'll need an API key from [openai.com](https://openai.com) and set it as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPEN_AI_API_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to implement a `spacy-llm` pipeline for your LLM task: running the pipeline in the source code or using a `config.cfg` file to define all settings and hyperparameters of your pipeline. In this tutorial, we'll work with a config file and you can have more info about running directly in Python [here](https://spacy.io/usage/large-language-models#example-3).\n",
    "\n",
    "Let us first define the settings of our pipeline as parameters in our config file. We'll implement two tasks: text classification and summarization, which we define them in the `pipeline` command. Then, we add our components to our pipeline to specify each task with their respective models and hypermeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_string = \"\"\"\n",
    "  [nlp]\n",
    "  lang = \"en\"\n",
    "  pipeline = [\"llm_textcat\",\"llm_summarization\",\"sentencizer\"]\n",
    "\n",
    "  [components]\n",
    "\n",
    "  [components.llm_textcat]\n",
    "  factory = \"llm\"\n",
    "\n",
    "  [components.llm_textcat.task]\n",
    "  @llm_tasks = \"spacy.TextCat.v2\"\n",
    "  labels = [\"HISTORY\",\"MUSIC\",\"TECHNOLOGY\",\"SCIENCE\",\"SPORTS\",\"POLITICS\"]\n",
    "  \n",
    "  [components.llm_textcat.model]\n",
    "  @llm_models = \"spacy.GPT-3-5.v1\"\n",
    "\n",
    "  [components.llm_summarization]\n",
    "  factory = \"llm\"\n",
    "\n",
    "  [components.llm_summarization.task]\n",
    "  @llm_tasks = \"spacy.Summarization.v1\"\n",
    "\n",
    "  [components.llm_summarization.model]\n",
    "  @llm_models = \"spacy.GPT-3-5.v1\"\n",
    "\n",
    "  [components.sentencizer]\n",
    "  factory = \"sentencizer\"\n",
    "  punct_chars = null\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these settings, we create an LLM pipeline for text classification and summarization in English with GPT3.5.\n",
    "\n",
    "`spacy-llm` offers various models to implement in your pipeline. You can have a look at the available [OpenAI models](https://spacy.io/api/large-language-models#models-rest) as well as check the [HuggingFace models](https://spacy.io/api/large-language-models#models-hf) offered if you want to work with open-source models.\n",
    "\n",
    "Now, with `ConfigParser`, we can create the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_string(config_string)\n",
    "\n",
    "with open(\"config.cfg\", 'w') as configfile:\n",
    "   config.write(configfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assemble the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = assemble(\"config.cfg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to make inferences with the pipeline we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"No matter how hard they tried, Barcelona lost the match.\")\n",
    "doc.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We need two functions that will ease the inferencing process and give us the text category and summary that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the category with the highest score\n",
    "def get_textcat_suggestion(doc):\n",
    "  model_prediction = doc.cats\n",
    "  return max(model_prediction, key=model_prediction.get)\n",
    "\n",
    "#selects the top N sentences with the highest scores and return combined string\n",
    "def get_summarization_suggestion(doc):\n",
    "  sentence_scores = Counter()\n",
    "  for sentence in doc.sents:\n",
    "      for word in sentence:\n",
    "          sentence_scores[sentence] += 1\n",
    "  summary_sentences = nlargest(2, sentence_scores, key=sentence_scores.get)\n",
    "  summary = ' '.join(str(sentence) for sentence in summary_sentences)\n",
    "  return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `squad_v2` from HuggingFace library in this tutorial. `squad_v2` is a dataset consisting of questions and their answers along with the context to search for the answer within. We'll use only the `context` column for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hf = load_dataset(\"squad_v2\", split=\"train\").shard(num_shards=10000, index=235)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedbackDataset\n",
    "\n",
    "Now that we have our pipeline for inference and the data, we can create our Argilla `FeedbackDataset` to make and store model suggestions. For this tutorial, we will create both a text classification task and a summarization task. Argilla Feedback lets us implement both tasks with `LabelQuestion` and `TextQuestion`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.FeedbackDataset(\n",
    "    fields=[\n",
    "        rg.TextField(name=\"text\")\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.LabelQuestion(\n",
    "            name=\"label-question\",\n",
    "            title=\"Classify the text category.\",\n",
    "            #make sure that the labels are in line with the labels we have defined in config.cfg\n",
    "            labels=[\"HISTORY\",\"MUSIC\",\"TECHNOLOGY\",\"SCIENCE\",\"SPORTS\",\"POLITICS\"]\n",
    "        ),\n",
    "        rg.TextQuestion(\n",
    "            name=\"text-question\",\n",
    "            title=\"Provide a summary for the text.\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create the records for our dataset by iterating over the dataset we loaded. While doing this, we will make inferences and save them in the `suggestions` with `get_textcat_suggestion()` and `get_summarization_suggestion()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    rg.FeedbackRecord(\n",
    "        fields={\n",
    "            \"text\": doc.text\n",
    "        },\n",
    "        suggestions=[\n",
    "            {\"question_name\": \"label-question\",\n",
    "            \"value\": get_textcat_suggestion(doc)},\n",
    "            {\"question_name\":\"text-question\",\n",
    "             \"value\": get_summarization_suggestion(doc)}\n",
    "        ]\n",
    "    ) for doc in [nlp(item) for item in dataset_hf[\"context\"]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the records, let us add them to the `FeedbackDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.add_records(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push to Argilla\n",
    "\n",
    "We are now ready to push our dataset to Argilla and can start to collect annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = dataset.push_to_argilla(name=\"squad_spacy-llm\", workspace=\"admin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the Argilla page ready to annotate as below.\n",
    "\n",
    "![Screenshot of Argilla UI](../../../_static/tutorials/labelling-spacy-llm/feedback-annotation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we have implemented a spacy-llm pipeline for text classification and summarization tasks. By Argilla Feedback datasets, we have been able to add the model predictions as suggestions to our dataset so that our annotators can utilize them. For more info on spacy-llm, you can go to their LLM [page](https://spacy.io/usage/large-language-models), and for other uses of Argilla Feedback datasets, you can refer to our [guides](../../../practical_guides/practical_guides.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Dec 17 2020, 03:56:09) \n[Clang 11.0.0 (clang-1100.0.33.17)]"
  },
  "metadata": {
   "interpreter": {
    "hash": "0f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "8874e298d2bce9702a08b32d5709c9f02d53b2045f1d246836c6e4c8123e6782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
