{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec4a7ec-ebda-453b-a5a4-960711982245",
   "metadata": {},
   "source": [
    "# üßπ Find and clean label errors with cleanlab\n",
    "\n",
    "In this tutorial, we will leverage *Argilla* and [cleanlab](https://github.com/cgnorthcutt/cleanlab) to find, uncover and correct potential label errors.\n",
    "You can do this by following 4 basic MLOps Steps:\n",
    "\n",
    "- üíæ load a dataset with potential label errors, here we use the [ag_news](https://huggingface.co/datasets/ag_news) dataset;\n",
    "- üíª train a model to make predictions for a test set, here we use a lightweight [sklearn](https://scikit-learn.org/stable//) model;\n",
    "- üßê use *cleanlab* via *Argilla* and get potential label error candidates in the test set;\n",
    "- üñç uncover and correct label errors quickly and comfortably with the *Argilla* web app;\n",
    "\n",
    "![monitoring-textclassification-cleanlab-explainability](/_static/tutorials/monitoring-textclassification-cleanlab-explainability/monitoring-textclassification-cleanlab-explainability.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5804acc4-8eaf-406c-a45d-64f069aeb5f7",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "As shown recently by [Curtis G. Northcutt et al.](https://arxiv.org/abs/2103.14749) label errors are pervasive even in the most-cited test sets used to benchmark the progress of the field of machine learning.\n",
    "They introduce a new principled framework to ‚Äúidentify label errors, characterize label noise, and learn with noisy labels‚Äù called **confident learning**. \n",
    "It is open-sourced as the [cleanlab Python package](https://github.com/cgnorthcutt/cleanlab) that supports finding, quantifying, and learning with label errors in data sets.\n",
    "\n",
    "*Argilla* provides built-in support for *cleanlab* and makes it a breeze to find potential label errors in your dataset.\n",
    "In this tutorial, we will try to uncover and correct label errors in the well-known [ag_news](https://huggingface.co/datasets/ag_news) dataset that is often used to benchmark classification models in NLP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3288feb9",
   "metadata": {},
   "source": [
    "## Running Argilla\n",
    "\n",
    "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
    "\n",
    "\n",
    "**Deploy Argilla on Hugging Face Spaces**: If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:\n",
    "\n",
    "[![deploy on spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/new-space?template=argilla/argilla-template-space)\n",
    "\n",
    "For details about configuring your deployment, check the [official Hugging Face Hub guide](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla).\n",
    "\n",
    "\n",
    "**Launch Argilla using Argilla's quickstart Docker image**: This is the recommended option if you want [Argilla running on your local machine](/getting_started/quickstart_installation.html). Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
    "\n",
    "For more information on deployment options, please check the Deployment section of the documentation.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Tip\n",
    "    \n",
    "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
    "\n",
    "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
    "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter Notebook tool of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0a89e-2f3a-4acc-9d5a-c5c83afb4958",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install argilla datasets scikit-learn cleanlab -qqq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3564ae1",
   "metadata": {},
   "source": [
    "Let's import the Argilla module for reading and writing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a558fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b89cf218",
   "metadata": {},
   "source": [
    "If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the `URL` and `API_KEY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0197dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "# Replace workspace with the name of your workspace\n",
    "rg.init(\n",
    "    api_url=\"http://localhost:6900\", \n",
    "    api_key=\"owner.apikey\",\n",
    "    workspace=\"admin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running a private Hugging Face Space, you will also need to set the [HF_TOKEN](https://huggingface.co/settings/tokens) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the HF_TOKEN environment variable\n",
    "# import os\n",
    "# os.environ['HF_TOKEN'] = \"your-hf-token\"\n",
    "\n",
    "# # Replace api_url with the url to your HF Spaces URL\n",
    "# # Replace api_key if you configured a custom API key\n",
    "# # Replace workspace with the name of your workspace\n",
    "# rg.init(\n",
    "#     api_url=\"https://[your-owner-name]-[your_space_name].hf.space\", \n",
    "#     api_key=\"owner.apikey\",\n",
    "#     workspace=\"admin\",\n",
    "#     extra_headers={\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"},\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e60d789b",
   "metadata": {},
   "source": [
    "Finally, let's include the imports we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from argilla.labeling.text_classification import find_label_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Telemetry\n",
    "\n",
    "We gain valuable insights from how you interact with our tutorials. To improve ourselves in offering you the most suitable content, using the following lines of code will help us understand that this tutorial is serving you effectively. Though this is entirely anonymous, you can choose to skip this step if you prefer. For more info, please check out the [Telemetry](../../reference/telemetry.md) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from argilla.utils.telemetry import tutorial_running\n",
    "    tutorial_running()\n",
    "except ImportError:\n",
    "    print(\"Telemetry is introduced in Argilla 1.20.0 and not found in the current installation. Skipping telemetry.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ab67518",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "If you want to skip the first three sections of this tutorial, and only uncover and correct the label errors in the Argilla web app, you can load the records directly from the [Hugging Face Hub](https://huggingface.co/datasets):\n",
    "\n",
    "```python\n",
    "records_with_label_errors = rg.read_datasets(\n",
    "    load_dataset(\"argilla/cleanlab-label_errors\", split=\"train\"),\n",
    "    task=\"TextClassification\",\n",
    ")\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd7aab0e-a6ea-4364-b021-1994bb57d931",
   "metadata": {},
   "source": [
    "## 1. Load datasets\n",
    "\n",
    "We start by downloading the [ag_news](https://huggingface.co/datasets/ag_news) dataset via the very convenient [datasets](https://github.com/huggingface/datasets) library. We then extract the train and test set, as well as the labels of this classification task.\n",
    "We also shuffle the train set, since by default it is ordered by the classification label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308e4ca-62e0-4ab0-8372-e4099acf502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# Get the train set and shuffle\n",
    "ds_train = dataset[\"train\"].shuffle(seed=43)\n",
    "\n",
    "# Get the test set\n",
    "ds_test = dataset[\"test\"]\n",
    "\n",
    "# Get the classification labels\n",
    "labels = ds_train.features[\"label\"].names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "001645aa-bbd9-4d15-b48a-fba494b35758",
   "metadata": {},
   "source": [
    "## 2. Train model\n",
    "\n",
    "For this tutorial, we will use a multinomial **Naive Bayes classifier**, a lightweight and easy-to-train [sklearn](https://scikit-learn.org/stable//) model. \n",
    "However, you can use any model of your choice as long as it includes the probabilities for all labels in its predictions.\n",
    "\n",
    "The features for our classifier will be simply the token counts of our input text.\n",
    "\n",
    "After defining our classifier, we can fit it with our train set. Since we are using a rather lightweight model, this should not take too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34917f-3ba4-4350-b58e-077cde4f6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our classifier as a pipeline of token counts + naive bayes model\n",
    "classifier = Pipeline([(\"vect\", CountVectorizer()), (\"clf\", MultinomialNB())])\n",
    "\n",
    "# Fit the classifier\n",
    "classifier.fit(X=ds_train[\"text\"], y=ds_train[\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa361dc-735e-40d5-85e8-0084a1de0c83",
   "metadata": {},
   "source": [
    "Let us check how our model performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee90653-3302-4dd7-9429-3d85e429ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test accuracy\n",
    "classifier.score(\n",
    "    X=ds_test[\"text\"],\n",
    "    y=ds_test[\"label\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2bfcb1-63ef-4f73-a76a-bb4eaf6a2bf9",
   "metadata": {},
   "source": [
    "We should obtain a decent accuracy of 0.90, especially because we only used the token counts as an input feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070744ab-2ad6-4afe-b340-3c7e40c3d51f",
   "metadata": {},
   "source": [
    "## 3. Get label error candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1851cc55-0812-47e6-9cd4-573ef375f6ca",
   "metadata": {},
   "source": [
    "As a first step to get label error candidates in our test set, we have to predict the probabilities for all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261094e-1dad-4591-bfd5-e052060ba1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for all labels\n",
    "probabilities = classifier.predict_proba(ds_test[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf504efa-2b5e-476e-aa58-9b95d54ee359",
   "metadata": {},
   "source": [
    "With the predictions at hand, we create Argilla records that contain the text input, the prediction of the model, the potential erroneous annotation, and some metadata of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35924760-dfd1-4ab3-abcd-63d23c7e5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create records for the test set\n",
    "records = [\n",
    "    rg.TextClassificationRecord(\n",
    "        text=data[\"text\"],\n",
    "        prediction=list(zip(labels, prediction)),\n",
    "        annotation=labels[data[\"label\"]],\n",
    "        metadata={\"split\": \"test\"},\n",
    "    )\n",
    "    for data, prediction in zip(ds_test, probabilities)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10247ff-177d-4f74-831a-33b7bcb4723a",
   "metadata": {},
   "source": [
    "We could log these records directly to Argilla and conveniently inspect them by eye, checking the annotation of each text input.\n",
    "But here we will use a quicker way by leveraging Argilla's built-in support for [cleanlab](https://github.com/cgnorthcutt/cleanlab).\n",
    "You simply import the [find_label_errors](/../reference/python/python_labeling.html) function from *Argilla* and pass in the list of records. That's it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31446f1-de26-46cd-9f76-2ddf0fd5e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get records with potential label errors\n",
    "records_with_label_error = find_label_errors(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad378c8-ef54-43c9-a436-53d258791773",
   "metadata": {},
   "source": [
    "The `records_with_label_error` list contains around 600 candidates for potential label errors, which is more than 8% of our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07884d9d-3621-4fde-a679-122ea4fe66aa",
   "metadata": {},
   "source": [
    "## 4. Uncover and correct label errors\n",
    "\n",
    "Now let us log those records to the *Argilla* web app to conveniently check them by eye, and to quickly correct potential label errors at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e730c5-b606-48e1-bd41-e6c7b2f12592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncover label errors in the Argilla web app\n",
    "rg.log(records_with_label_error, \"label_errors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676a4ab-24b0-46d5-884c-439c22bb733c",
   "metadata": {},
   "source": [
    "By default, the records in the `records_with_label_error` list are ordered by their likelihood of containing a label error.\n",
    "They will also contain a metadata called *\"label_error_candidate\"* by default, which reflects the order in the list.\n",
    "You can use this field in the *Argilla* web app to sort the records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca82154-d290-4b9b-a237-351e36988dd9",
   "metadata": {},
   "source": [
    "We can confirm that the most likely candidates are indeed clear label errors.\n",
    "Towards the end of the candidate list, the examples get more ambiguous, and it is not immediately obvious if the gold annotations are erroneous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb28a5-a585-4296-9aee-0d2af7d06f62",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "With *Argilla* you can quickly and conveniently find label errors in your data.\n",
    "The built-in support for [cleanlab](https://github.com/cgnorthcutt/cleanlab), together with the optimized user experience of the Argilla web app, makes the process a breeze and allows you to efficiently correct label errors on the fly.\n",
    "\n",
    "In just a few steps you can quickly check if your test data set is seriously affected by label errors and if your benchmarks are really meaningful in practice. \n",
    "Maybe your less complex models turn out to beat your resource-hungry supermodel, and the deployment process just got a little bit easier üòÄ.\n",
    "\n",
    "Although we only used a sklearn model in this tutorial, Argilla does not care about the model architecture or the framework you are working with. \n",
    "It just cares about the underlying data and allows you to put more humans in the loop of your AI Lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0b8c4-5c53-4231-aedb-1d48c0cdc6bc",
   "metadata": {},
   "source": [
    "## Appendix I: Find label errors in your train data using cross-validation\n",
    "\n",
    "In order to check your training data for label errors, you can fall back to the cross-validation technique to get out-of-sample predictions.\n",
    "With a classifier from sklearn, cross-validation is really easy and you can do it conveniently in one line of code.\n",
    "Afterward, the steps of creating *Argilla* records, finding label error candidates, and uncovering them are the same as shown in the tutorial above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9e496-9546-432f-9ca1-40d6d0853e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Get predicted probabilities for the whole dataset via cross-validation\n",
    "cv_probs = cross_val_predict(\n",
    "    classifier,\n",
    "    X=ds_train[\"text\"] + ds_test[\"text\"],\n",
    "    y=ds_train[\"label\"] + ds_test[\"label\"],\n",
    "    cv=int(len(ds_train) / len(ds_test)),\n",
    "    method=\"predict_proba\",\n",
    "    n_jobs=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096197d-c19c-40f2-9c3e-4614ebc95023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create records for the training set\n",
    "records = [\n",
    "    rg.TextClassificationRecord(\n",
    "        text=data[\"text\"],\n",
    "        prediction=list(zip(labels, prediction)),\n",
    "        annotation=labels[data[\"label\"]],\n",
    "        metadata={\"split\": \"train\"},\n",
    "    )\n",
    "    for data, prediction in zip(ds_train, cv_probs)\n",
    "]\n",
    "\n",
    "# Uncover label errors for the train set in the Argilla web app\n",
    "rg.log(find_label_errors(records), \"label_errors_in_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ae179-261b-4b91-a209-31ce141864f7",
   "metadata": {},
   "source": [
    "Here we find around 9400 records with potential label errors, which is also around 8% with respect to the train data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409a420",
   "metadata": {},
   "source": [
    "## Appendix II: Log datasets to the Hugging Face Hub\n",
    "\n",
    "Here we will show you an example of how you can push an Argilla dataset (records) to the [Hugging Face Hub](https://huggingface.co/datasets).\n",
    "In this way, you can effectively version any of your Argilla datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab556ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = rg.load(\"label_errors\")\n",
    "records.to_datasets().push_to_hub(\"<name of the dataset on the HF Hub>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Dec 17 2020, 03:56:09) \n[Clang 11.0.0 (clang-1100.0.33.17)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8874e298d2bce9702a08b32d5709c9f02d53b2045f1d246836c6e4c8123e6782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
