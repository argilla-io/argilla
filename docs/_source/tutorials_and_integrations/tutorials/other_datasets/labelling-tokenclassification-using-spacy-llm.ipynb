{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”— Using LLMs for Few-Shot Token Classification Suggestions with `spacy-llm`\n",
    "\n",
    "This tutorial will guide you through a Named Entity Recognition (NER) pipeline using spacy-llm and few-shot learning. The steps will be as follows:\n",
    "\n",
    "- Run Argilla and load `spacy-llm` along with other libraries\n",
    "- Define few-shot examples for the model\n",
    "- Define config for your pipeline and initialize it\n",
    "- Create `TokenClassificationRecord` and store the inferences in it\n",
    "- Push to Argilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "NER is a Natural Language Processing task where tokens are categorized into pre-defined categories such as person, organization or place as `named entities`. Thus, NER is one of the primary steps to turn unstructured data into structured data. With Argilla, you will have the opportunity to validate and annotate the named entities that you obtain from your NER pipeline.\n",
    "\n",
    "[spacy-llm](https://spacy.io/usage/large-language-models) is a package that integrates the strength of LLMs into regular spaCy pipelines, thus allowing quick and practical prompting for various tasks. Besides, since it requires no training data, the models are ready to use in your pipeline. If you want to train your own model or create your custom task, `spacy-llm` also helps to implement any custom pipeline.\n",
    "\n",
    "In this tutorial, we will create a `spacy-llm` pipeline to perform NER on `banking77` dataset. It is a dataset consisting of online banking queries by customers and their annotations with the corresponding intents. We will show our model a few positive and negative examples to improve its performance, where we will actually have defined a category for a specific named entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Argilla\n",
    "\n",
    "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
    "\n",
    "\n",
    "**Deploy Argilla on Hugging Face Spaces**: If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:\n",
    "\n",
    "[![deploy on spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/new-space?template=argilla/argilla-template-space)\n",
    "\n",
    "For details about configuring your deployment, check the [official Hugging Face Hub guide](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla).\n",
    "\n",
    "\n",
    "**Launch Argilla using Argilla's quickstart Docker image**: This is the recommended option if you want [Argilla running on your local machine](/getting_started/quickstart_installation.html). Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
    "\n",
    "For more information on deployment options, please check the Deployment section of the documentation.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Tip\n",
    "    \n",
    "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
    "\n",
    "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
    "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter Notebook tool of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You will need to install some libraries along with Argilla for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"spacy-llm[transformers]\" argilla datasets -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let us import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import configparser\n",
    "import argilla as rg\n",
    "from spacy_llm.util import assemble\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to initialize the Argilla client with `API_URL` and `API_KEY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "# Replace workspace with the name of your workspace\n",
    "rg.init(\n",
    "    api_url=\"http://localhost:6900\", \n",
    "    api_key=\"owner.apikey\",\n",
    "    workspace=\"admin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running a private Hugging Face Space, you will also need to set the [HF_TOKEN](https://huggingface.co/settings/tokens) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the HF_TOKEN environment variable\n",
    "# import os\n",
    "# os.environ['HF_TOKEN'] = \"your-hf-token\"\n",
    "\n",
    "# # Replace api_url with the url to your HF Spaces URL\n",
    "# # Replace api_key if you configured a custom API key\n",
    "# # Replace workspace with the name of your workspace\n",
    "# rg.init(\n",
    "#     api_url=\"https://[your-owner-name]-[your_space_name].hf.space\", \n",
    "#     api_key=\"owner.apikey\",\n",
    "#     workspace=\"admin\",\n",
    "#     extra_headers={\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use GPT3.5 and other models from OpenAI with spacy-llm, we'll need an API key from [openai.com](https://openai.com) and set it as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = <YOUR_OPENAI_API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Telemetry\n",
    "\n",
    "We gain valuable insights from how you interact with our tutorials. To improve ourselves in offering you the most suitable content, using the following lines of code will help us understand that this tutorial is serving you effectively. Though this is entirely anonymous, you can choose to skip this step if you prefer. For more info, please check out the [Telemetry](../../reference/telemetry.md) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from argilla.utils.telemetry import tutorial_running\n",
    "    tutorial_running()\n",
    "except ImportError:\n",
    "    print(\"Telemetry is introduced in Argilla 1.20.0 and not found in the current installation. Skipping telemetry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Let us import our dataset from HuggingFace hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hf = load_dataset(\"banking77\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a sample from the dataset to view what it consists of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hf.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `spacy-llm` pipeline\n",
    "\n",
    "There are two ways to implement a `spacy-llm` pipeline for your LLM task: running the pipeline in the source code or using a `config.cfg` file to define all settings and hyperparameters of your pipeline. In this tutorial, we'll work with a config file and you can have more info about running directly in Python [here](https://spacy.io/usage/large-language-models#example-3).\n",
    "\n",
    "Before defining the pipeline, let us first create the examples that will be used for few-shot learning by our model.\n",
    "\n",
    "### Few-shot examples\n",
    "\n",
    "In `spacy-llm`, you have the opportunity to inject your examples into the prompt to the LLM. As it will enhance the performance for your specific task, `spacy-llm` makes it very easy to have few-shot examples in your pipeline. You can have detailed info [here](https://spacy.io/api/large-language-models#ner).\n",
    "\n",
    "Let us define some very basic example sentences in `json` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = [\n",
    "  {\n",
    "    \"text\": \"I was charged with an exchange rate for my purchase and it was not right.\",\n",
    "    \"spans\": [\n",
    "      {\n",
    "        \"text\": \"charged\",\n",
    "        \"is_entity\": 'false',\n",
    "        \"label\": \"==NONE==\",\n",
    "        \"reason\": \"is an action done to the customer by the bank, not by them\"\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"purchase\",\n",
    "        \"is_entity\": \"true\",\n",
    "        \"label\": \"ACTIVITY\",\n",
    "        \"reason\": \"is an action that the customer did, not the bank\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"The exchange rate for the last item I bought seems to be correct.\",\n",
    "    \"spans\": [\n",
    "      {\n",
    "        \"text\": \"exchange rate\",\n",
    "        \"is_entity\": \"false\",\n",
    "        \"label\": \"==NONE==\",\n",
    "        \"reason\": \"is a name for currency, not an action or performance\"\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"item\",\n",
    "        \"is_entity\": \"false\",\n",
    "        \"label\": \"==NONE==\",\n",
    "        \"reason\": \"is a generic name for the object bought, not a performance\"\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"bought\",\n",
    "        \"is_entity\": \"true\",\n",
    "        \"label\": \"ACTIVITY\",\n",
    "        \"reason\": \"is the verb for the action that the customer performed\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have introduced both positive and negative examples. Besides positive ones, telling the model that certain token(s) are not named entities in certain contexts will improve the performance even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the json file in the base directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fewshot.json\", \"w\") as outfile:\n",
    "    json.dump(json_file, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the pipeline\n",
    "\n",
    "Let us create the `spacy-llm` pipeline with settings and parameters via `config.cfg`. We will implement the NER task, which we define in the `pipeline` command. Then, we add our components to our pipeline to specify the task with its model and hypermeters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_string = \"\"\"\n",
    "  [paths]\n",
    "  examples = \"fewshot.json\"\n",
    "\n",
    "  [nlp]\n",
    "  lang = \"en\"\n",
    "  pipeline = [\"llm\",\"sentencizer\"]\n",
    "\n",
    "  [components]\n",
    "\n",
    "  [components.llm]\n",
    "  factory = \"llm\"\n",
    "\n",
    "  [components.llm.task]\n",
    "  @llm_tasks = \"spacy.NER.v3\"\n",
    "  labels = [\"PERSON\", \"ORGANIZATION\", \"CARDINAL\", \"PERCENT\", \"ACTIVITY\"]\n",
    "\n",
    "  [components.llm.task.examples]\n",
    "  @misc = \"spacy.FewShotReader.v1\"\n",
    "  path = \"${paths.examples}\"\n",
    "\n",
    "  [components.llm.model]\n",
    "  @llm_models = \"spacy.GPT-3-5.v1\"\n",
    "  config = {\"temperature\": 0.5}\n",
    "\n",
    "  [components.sentencizer]\n",
    "  factory = \"sentencizer\"\n",
    "  punct_chars = null\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the labels should include any custom-named entity we have defined in few-shot examples. Otherwise, the model will come up with its own definition of the custom-named entity and run accordingly, which might be a problem for specific named entities.\n",
    "\n",
    "`spacy-llm` offers various models to implement in your pipeline. You can have a look at the available [OpenAI models](https://spacy.io/api/large-language-models#models-rest) as well as check the [HuggingFace models](https://spacy.io/api/large-language-models#models-hf) offered if you want to work with open-source models.\n",
    "\n",
    "\n",
    "Now, with `ConfigParser`, we can create the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_string(config_string)\n",
    "\n",
    "with open(\"config.cfg\", 'w') as configfile:\n",
    "   config.write(configfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assemble the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = assemble(\"config.cfg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the model to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The same item is 50% more expensive now, so they did not purchase it.\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Now that we have a functioning pipeline for our NER task, we can now create a `TokenClassificationRecord` to store the inferences in it. We will use the pipeline to generate predictions for our dataset and store them in the record.\n",
    "\n",
    "### Create records\n",
    "\n",
    "We will store the tokens and predictions in a `TokenClassificationRecord`. Let us create a tokenizer function to tokenize the sentences in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc):\n",
    "  return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a list of records from the dataset items. See the [documentation](https://docs.argilla.io/en/latest/reference/python/python_client.html#argilla.client.models.TokenClassificationRecord) for more information on what other fields you can add to your record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    rg.TokenClassificationRecord(\n",
    "        text=doc.text,\n",
    "        tokens=tokenizer(doc),\n",
    "        prediction=[(ent.label_, ent.start_char, ent.end_char) for ent in doc.ents],\n",
    "        prediction_agent=\"gpt-3.5-turbo\"\n",
    "    ) for doc in [nlp(item) for item in dataset_hf[\"text\"]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create `DatasetForTokenClassification` from the list of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.DatasetForTokenClassification(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push to Argilla\n",
    "\n",
    "Push the dataset to Argilla using `rg.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.log(dataset, \"banking77_ner\", workspace=\"admin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is now ready to be annotated with predictions from GPT3.5! You can validate or discard the predictions made by the model.\n",
    "\n",
    "<img src=\"/_static/tutorials/labelling-tokenclassification-using-spacy-llm/argilla-ner-annotation.png\" alt=\"NER Annotation\" style=\"width: 1100px;\">\n",
    "\n",
    "In this tutorial, we have implemented a `spacy-llm` pipeline for the NER task by using model predictions from GPT3.5. In addition, we employed a few-shot learning approach to improve the performance of our model, which is facilitated by `spacy-llm`. You can see more tutorials on the use of `spaCy` with Argilla [here](https://docs.argilla.io/en/latest/tutorials/libraries/spacy.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d98cb9bf90a932b5bf8e86e91214497eb0e38eb318595fbd6fbd5460fe92036"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
