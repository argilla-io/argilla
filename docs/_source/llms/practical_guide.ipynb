{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Guide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the task\n",
    "In the Feedback Task Datasets it is possible to combine multiple questions of different kinds, so the first step will be to define the aim of your project and the kind of feedback you will need to collect to get there.\n",
    "[Add a snapshot of the task]\n",
    "In this section, you will find all the elements you can configure to set up your dataset and start collecting feedback."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record fields\n",
    "A record is a data item to be annotated that can have one or multiple fields e.g., your records can have a `prompt` and `output` pair. Currently, we only support text fields, but we expect to add images in the future.\n",
    "\n",
    "You can define the title and content these fields using the Python SDK and they will follow the order in which these are added to the dataset settings. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "Formulate the feedback you want to collect for your dataset as questions. These can be required or optional. The annotators will need to answer all required questions to submit a response, but they may or may not answer optional questions. You may add a description to your questions so that annotators can have more context or information about the question right from the UI.\n",
    "\n",
    "These are the types of questions that are currently supported in the Feedback Task:\n",
    "    - Rating: The values of rating questions are integers. These are useful when you need to collect a numerical score.\n",
    "    - Text: This one gives the annotator a free text area, where they can enter any text. These are useful when you need to collect natural language data, for example corrections or explanations.\n",
    "\n",
    "    ```{note}\n",
    "    We are planning to expand the kinds of questions supported in the Feedback Task in the coming releases\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Write guidelines\n",
    "Once you know the data you want to display and the questions you need to ask, we highly encourage you to write some annotation guidelines to make sure that annotators understand the task and will be as consistent as possible when answering the questions. You may pass these guidelines as a text for them to see in the dataset settings and/or as descriptions to the questions of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up your team of annotators\n",
    "The Feedback Task supports having multiple annotations for your records. This means that as long as your annotators have access to the dataset, they can all give responses to every record in the dataset. You can set up different roles and workspaces to control who will have access to the dataset.\n",
    " \n",
    "```{hint}\n",
    "if you don't want all annotators working on all examples you may divide the data into different datasets and assign those.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Push your dataset to Argilla \n",
    "Here is a code snippet that illustrates how to set up and create a dataset in Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset config from Python client (including guidelines, question descriptions, etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add records, \n",
    "- Add records:\n",
    "    - Get a dataset from HF hub\n",
    "    - Data from json/csv/pandas…\n",
    "    - Add records using Python client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating in the Feedback Task\n",
    "Once you open the dataset, you will see by default the `Pending`, i.e. records that still don't have a response, in a single-record view. On the left, you can find the record to annotate and on the right the response form with all the questions to answer. \n",
    "\n",
    "We highly recommend that you read the annotation guidelines before starting the annotation. If there are any, you can find them in the dataset settings page. [describe how to get there] You can also click on the info icon next to the questions to get more information on the specific question.\n",
    "\n",
    "In the annotation view, you will be able to provide responses. Once all required questions have responses the `Submit` button will be enabled and you will be able to submit your response. If you prefer not to give a response for a record, you can move to the next record or discard it using the `Discard` button. \n",
    "\n",
    "If you need to review your submitted or discarded responses, you can select the view/queue? you need. From there, you can modify, submit or discard responses. You can also use the `Clear` button to remove the response and send the record back to the `Pending` queue.\n",
    "\n",
    "You can speed up the process by using shortcuts:\n",
    "|Action|Keys|\n",
    "|------|----|\n",
    "|Next page|&rarr; `Right arrow`|\n",
    "|Previous page|&larr; `Left arrow`|\n",
    "|Clear|&#8679; `Shift` + &blank; `Space`|\n",
    "|Submit|&crarr; `Enter`|\n",
    "|Discard|&#x232B; `Backspace`|\n",
    "\n",
    "If your cursor is in the text area, these are the action shortcuts:\n",
    "|Action|Keys|\n",
    "|------|----|\n",
    "|Clear|&#8679; `Shift` + &blank; `Space`|\n",
    "|Submit|&#8679; `Shift` + &crarr; `Enter`|\n",
    "|Discard|&#8679; `Shift` + &#x232B; `Backspace`|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Collect responses\n",
    "- Using the Python client to collect the responses to the dataset\n",
    "- Unifying responses (?) -> Técnicas de IAA.\n",
    "    - Majority vote, average... \n",
    "    - How to calculate IAA for text fields? bleu rouge? Rating of the proposed texts. \n",
    "    Make a dataset to collect human text, then rate the human text and use it for a rating exercise to get an annotator score or clean the dataset.\n",
    "- Export / publish the dataset.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
