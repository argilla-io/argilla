{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to Guide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will help you with all the practical aspects of setting up an annotation project for training and fine-tuning LLMs using Argilla's Feedback Task Datasets. It covers everything from defining your task to collecting, organizing, and using the feedback effectively.\n",
    "\n",
    "![Feedback dataset snapshot](../_static/llms/snapshot-feedback-demo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "import os\n",
    "\n",
    "rg.init(\n",
    "    api_url=os.environ.get(\"ARGILLA_API_URL_DEV\"),\n",
    "    api_key=os.environ.get(\"ARGILLA_API_KEY\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the task\n",
    "The Feedback Task Datasets allow to combine multiple questions of different kinds, so the first step will be to define the aim of your project and the kind of data and feedback you will need to get there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format records\n",
    "A record in Argilla refers to a data item that requires annotation and can consist of one or multiple fields. For example, your records can include a pair of a prompt and an output. Currently, we only support plain text fields, but we plan to introduce support for markdown and images in the future.\n",
    "\n",
    "Take some time to explore and find data that fits the purpose of your project. If you are planning to use public data, the [Datasets page](https://huggingface.co/datasets) of the Hugging Face Hub is a good place to start.\n",
    "\n",
    "´´´{hint}\n",
    "Always check the licenses of the datasets to make sure you can legally use the dataset for your specfic use case.\n",
    "´´´\n",
    "\n",
    "Once you have a dataset, load it and inspect it to find the fields that you want to use in your Feedback dataset. A quick overview of the data will also help you formulate the right questions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalia/opt/anaconda3/envs/argilla/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset json (/Users/natalia/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response', 'category'],\n",
       "    num_rows: 15011\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('databricks/databricks-dolly-15k', split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which is a species of fish? Tope or Rope</td>\n",
       "      <td></td>\n",
       "      <td>Tope</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why can camels survive for long without water?</td>\n",
       "      <td></td>\n",
       "      <td>Camels use the fat in their humps to keep them...</td>\n",
       "      <td>open_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice's parents have three daughters: Amy, Jes...</td>\n",
       "      <td></td>\n",
       "      <td>The name of the third daughter is Alice</td>\n",
       "      <td>open_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was Tomoaki Komorida born?</td>\n",
       "      <td>Komorida was born in Kumamoto Prefecture on Ju...</td>\n",
       "      <td>Tomoaki Komorida was born on July 10,1981.</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15006</th>\n",
       "      <td>How do i accept the change</td>\n",
       "      <td></td>\n",
       "      <td>Embrace the change and see the difference</td>\n",
       "      <td>brainstorming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15007</th>\n",
       "      <td>What is a laser and who created it?</td>\n",
       "      <td>A laser is a device that emits light through a...</td>\n",
       "      <td>A laser is a device that emits light from an e...</td>\n",
       "      <td>summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15008</th>\n",
       "      <td>What is the difference between a road bike and...</td>\n",
       "      <td></td>\n",
       "      <td>Road bikes are built to be ridden on asphalt a...</td>\n",
       "      <td>open_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15009</th>\n",
       "      <td>How does GIS help in the real estate investmen...</td>\n",
       "      <td></td>\n",
       "      <td>Real estate investors depend on precise, accur...</td>\n",
       "      <td>general_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15010</th>\n",
       "      <td>What is the Masters?</td>\n",
       "      <td></td>\n",
       "      <td>The Masters Tournament is a golf tournament he...</td>\n",
       "      <td>general_qa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15011 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction  \\\n",
       "0             When did Virgin Australia start operating?   \n",
       "1               Which is a species of fish? Tope or Rope   \n",
       "2         Why can camels survive for long without water?   \n",
       "3      Alice's parents have three daughters: Amy, Jes...   \n",
       "4                        When was Tomoaki Komorida born?   \n",
       "...                                                  ...   \n",
       "15006                         How do i accept the change   \n",
       "15007                What is a laser and who created it?   \n",
       "15008  What is the difference between a road bike and...   \n",
       "15009  How does GIS help in the real estate investmen...   \n",
       "15010                               What is the Masters?   \n",
       "\n",
       "                                                 context  \\\n",
       "0      Virgin Australia, the trading name of Virgin A...   \n",
       "1                                                          \n",
       "2                                                          \n",
       "3                                                          \n",
       "4      Komorida was born in Kumamoto Prefecture on Ju...   \n",
       "...                                                  ...   \n",
       "15006                                                      \n",
       "15007  A laser is a device that emits light through a...   \n",
       "15008                                                      \n",
       "15009                                                      \n",
       "15010                                                      \n",
       "\n",
       "                                                response        category  \n",
       "0      Virgin Australia commenced services on 31 Augu...       closed_qa  \n",
       "1                                                   Tope  classification  \n",
       "2      Camels use the fat in their humps to keep them...         open_qa  \n",
       "3                The name of the third daughter is Alice         open_qa  \n",
       "4             Tomoaki Komorida was born on July 10,1981.       closed_qa  \n",
       "...                                                  ...             ...  \n",
       "15006          Embrace the change and see the difference   brainstorming  \n",
       "15007  A laser is a device that emits light from an e...   summarization  \n",
       "15008  Road bikes are built to be ridden on asphalt a...         open_qa  \n",
       "15009  Real estate investors depend on precise, accur...      general_qa  \n",
       "15010  The Masters Tournament is a golf tournament he...      general_qa  \n",
       "\n",
       "[15011 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# turn it into a pandas dataframe to get a quick overview of a few examples\n",
    "df = pd.DataFrame(dataset)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create records following Argilla's Feedback Record format [link to Python reference].\n",
    "\n",
    "The name of the fields will need to match the fields set up in the dataset configuration (see [below](#create-your-dataset))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we create the records, we can rename the fields and optionally filter the original dataset\n",
    "records = [rg.FeedbackRecord(fields={\"question\": record[\"instruction\"], \"answer\": record[\"response\"]}) for record in dataset if record[\"category\"]==\"open_qa\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define questions\n",
    "To collect feedback for your dataset, you need to formulate questions. The Feedback Task currently supports the following types of questions:\n",
    "\n",
    "- Rating: These questions require annotators to select one option from a list of integer values. This type is useful for collecting numerical scores.\n",
    "- Text: These questions offer annotators a free-text area where they can enter any text. This type is useful for collecting natural language data, such as corrections or explanations.\n",
    "\n",
    "```{note}\n",
    "We have plans to expand the range of supported question types in future releases of the Feedback Task.\n",
    "```\n",
    "\n",
    "You can define your questions using the Python SDK and set up the following configurations:\n",
    "- `name`: A shortname for the question.\n",
    "- `title`: The text displayed in the UI.\n",
    "- `description` (optional): The text to be displayed in the question tooltip in the UI. You can use it to give more context or information to annotators.\n",
    "- `required`: Set your question as required or optional. Annotators must answer all required questions to submit a response, but they have the choice to answer optional questions or not.\n",
    "- `values`: In a RatingQuestion, these are the rating options represented as a list of integer values.\n",
    "\n",
    "```{note}\n",
    "The order of the questions in the UI follows the order in which these are added to the dataset in the Python SDK.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of questions to display in the feedback form\n",
    "questions =[\n",
    "    rg.RatingQuestion(\n",
    "        name=\"rating\", \n",
    "        title=\"Rate the quality of the response:\", \n",
    "        description=\"1 = very bad - 5= very good\",\n",
    "        required=True,\n",
    "        values=[1,2,3,4,5]\n",
    "    ),\n",
    "    rg.TextQuestion(\n",
    "        name=\"corrected-text\",\n",
    "        title=\"Provide a correction to the response:\",\n",
    "        required=False\n",
    "    )\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Write guidelines\n",
    "Once you have decided on the data to show and the questions to ask, it's important to provide clear guidelines to the annotators. These guidelines help them understand the task and answer the questions consistently. You can provide guidelines in two ways:\n",
    "- In the dataset guidelines: this is added as an argument when you create your dataset in the Python SDK (see below). It will appear in the dataset settings in the UI.\n",
    "- As question descriptions: these are added as an argument when you create questions in the Python SDK (see above). This text will appear in a tooltip next to the question in the UI.\n",
    "\n",
    "It is good practice to use at least the dataset guidelines, if not both methods. In the guidelines, you can include a description of the project, details on how to answer each question with examples, instructions on when to discard a record, etc. Question descriptions should be short and provide context to a specific question. They can be a summary of the guidelines to that question, but often times that is not sufficient to align the whole annotation team."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your annotation team\n",
    "Depending on the nature of your project and the size of your annotation team, you may want to have control over annotation overlap i.e., having multiple annotations for a single record. You will need to decide on this before pushing your dataset to Argilla, as this has implications on how your dataset is set up. Let's explore a few overlapping options.\n",
    "\n",
    "### Full overlap\n",
    "The Feedback Task supports having multiple annotations for your records. This means that all users with access to the dataset can give responses to all the records in the dataset. To have this full overlap just push the dataset (as detailed in [Create your dataset](#create-your-dataset)) in a workspace where all team members have access. Learn more about managing user access to workspaces [here](../getting_started/installation/configurations/user_management.md#creating-an-annotator-user-assigned-to-a-workspace).\n",
    "\n",
    "### Zero overlap\n",
    "If you only want one annotation per record, we recommend that you split your records into chunks and assign these to a single annotator. Then, you can create several datasets, one in each annotator's personal workspace, and add the records assigned to each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import httpx\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# make a request using your Argilla Client\n",
    "rg_client= rg.active_client().client\n",
    "auth_headers = {\"X-Argilla-API-Key\": rg_client.token}\n",
    "http=httpx.Client(base_url=rg_client.base_url, headers=auth_headers)\n",
    "users = http.get(\"/api/users\").json()\n",
    "\n",
    "# optional: filter users to get only those with annotator role\n",
    "users = [u for u in users if u['role']=='annotator']\n",
    "\n",
    "# optional: shuffle the records to get a random assignment\n",
    "random.shuffle(records)\n",
    "\n",
    "# build a dictionary where the key is the username and the value is the list of records assigned to them\n",
    "assignments = defaultdict(list)\n",
    "\n",
    "# divide your records in chunks of the same length as the users list and make the assignments\n",
    "# you will need to follow the instructions to create and push a dataset for each of the key-value pairs in this dictionary\n",
    "n = len(users)\n",
    "chunked_records = [records[i:i + n] for i in range(0, len(records), n)]\n",
    "for chunk in chunked_records:\n",
    "    for idx, record in enumerate(chunk):\n",
    "        assignments[users[idx]['username']].append(record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlled overlap\n",
    "This option is optimal when you want to have annotation overlap, but up to a certain number and not with the whole team. This can be because you want your team to be more efficient or perhaps to calculate the agreement between pairs of annotators. In this case, you also need to create several datasets and push them to the annotators' personal workspaces with the difference that each record will appear in multiple datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to assign with overlap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and import a dataset\n",
    "Now we are ready to create our dataset. To do that, first you'll need to define the following configurations:\n",
    "- `name`: The name of the dataset.\n",
    "- `workspace`: The workspace where the dataset will be created. If you don't provide one, it will be placed in the default workspace attached to the API key used in `rg.init()`.\n",
    "- `guidelines` (optional): A set of guidelines for the annotators. These will appear in the dataset settings in the UI.\n",
    "- `fields`: The list of fields to show in the record card. The order in which the fields will appear matches the order of this list.\n",
    "- `questions`: The list of questions to show in the form.\n",
    "\n",
    "Once the dataset is created locally, add the records and, when you're happy with the result, push the dataset to Argilla. At that point, you will be able to see the dataset from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset locally\n",
    "dataset = rg.FeedbackDataset(\n",
    "    guidelines=\"You will see a collection of records with a question and an answer.\\nYou will be asked to rate the answer from 1 (very bad) to 5 (very good).\\nIf your rating is below 5, please provide a correction to the output.\",\n",
    "    fields = [\n",
    "        rg.TextField(name=\"question\"),\n",
    "        rg.TextField(name=\"answer\")\n",
    "    ],\n",
    "    questions=questions\n",
    ")\n",
    "\n",
    "# add the records to the dataset\n",
    "dataset.add_records(records)\n",
    "\n",
    "# push the dataset and records to Argilla\n",
    "dataset.push_to_argilla(name='my_dataset', workspace='my_workspace')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to load and import datasets saved in the Hugging Face Hub that have Argilla's Feedback Dataset format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a public dataset\n",
    "dataset = rg.FeedbackDataset.from_huggingface(\"argilla/feedback-dataset\")\n",
    "\n",
    "# load a private dataset\n",
    "dataset = rg.FeedbackDataset.from_huggingface(\"argilla/feedback-dataset\", use_auth_token=True)\n",
    "\n",
    "# push to Argilla\n",
    "dataset.push_to_argilla(name=\"my_hub_dataset\", workspace=\"my_workspace\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or copy an existing dataset in your Argilla instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = rg.FeedbackDataset.from_argilla(\"demo_feedback\", workspace=\"recognai\")\n",
    "\n",
    "# push the dataset with a different name / workspace\n",
    "dataset.push_to_argilla(name=\"my_dataset\", workspace=\"my_workspace\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating a Feedback Dataset\n",
    "Once you open the dataset, you will see by default the records with `Pending` responses, i.e. records that still don't have a response, in a single-record view. On the left, you can find the record to annotate and on the right the form with all the questions to answer. \n",
    "\n",
    "We highly recommend that you read the annotation guidelines before starting the annotation. If there are any, you can find them in the dataset settings page. [describe how to get there] If any of the questions have a description, you will find an info icon next to them. Click it to read the description.\n",
    "\n",
    "In the annotation view, you will be able to provide responses. Once all required questions have responses the `Submit` button will be enabled and you will be able to submit your response. If you prefer not to give a response for a record, you can move to the next record or discard it using the `Discard` button. \n",
    "\n",
    "If you need to review your submitted or discarded responses, you can select the queue you need. From there, you can modify, submit or discard responses. You can also use the `Clear` button to remove the response and send the record back to the `Pending` queue.\n",
    "\n",
    "You can speed up the annotation process by using shortcuts:\n",
    "|Action|Keys|\n",
    "|------|----|\n",
    "|Clear|&#8679; `Shift` + &blank; `Space`|\n",
    "|Discard|&#x232B; `Backspace`|\n",
    "|Discard (from text area)|&#8679; `Shift` + &#x232B; `Backspace`|\n",
    "|Submit|&crarr; `Enter`|\n",
    "|Submit (from text area)|&#8679; `Shift` + &crarr; `Enter`|\n",
    "|Go to previous page|&larr; `Left arrow`|\n",
    "|Go to next page|&rarr; `Right arrow`|\n",
    "\n",
    "![Spanshot of the Submitted queue and the progress bar in a Feedback dataset](../_static/llms/snapshot-feedback-submitted.png)\n",
    "\n",
    "You can track your progress and the number of `Pending`, `Submitted` and `Discarded` responses by clicking the `Progress` icon in the sidebar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect responses\n",
    "\n",
    "To collect the responses given by annotators, you can simply load the dataset in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching records from Argilla: 15it [00:01,  7.15it/s]\n"
     ]
    }
   ],
   "source": [
    "feedback = rg.FeedbackDataset.from_argilla(\"demo_feedback\", workspace=\"recognai\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dataset doesn't have any annotation overlap i.e., all records have only one response, the post-processing stage will be quite simple. You just need to transform the dataset into whatever format you need for training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Remember to only take into account responses with the `submitted` status.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure and solve disagreements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dataset does have records with more than one `submitted` response, you will need to unify the responses before using the data for training. \n",
    "\n",
    "Let's explore the differences in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which episodes of season four of Game of Thrones did Michelle MacLaren direct?\n",
      "Answer: She directed \"Oathkeeper\" and \"First of His Name\" the fourth and fifth episodes of season four, respectively.\n",
      "--------------------------------------------------\n",
      "Annotator 1\n",
      "Status: submitted\n",
      "Response: {'rating': {'value': 5}}\n",
      "--------------------------------------------------\n",
      "Annotator 2\n",
      "Status: submitted\n",
      "Response: {'rating': {'value': 3}, 'corrected-text': {'value': 'In Season 3 she directed the episodes \"The Bear and the Maiden Fair\" and \"Second Sons\". In Season 4 , she directed another two episodes: \"Oathkeeper\" and \"First of His Name\".'}}\n"
     ]
    }
   ],
   "source": [
    "# example of annotation results for a single record\n",
    "print(f\"Question: {feedback[4]['fields']['question']}\")\n",
    "print(f\"Answer: {feedback[4]['fields']['answer']}\")\n",
    "for ix,response in enumerate(feedback[4]['responses']):\n",
    "    print('-'*50)\n",
    "    print(f\"Annotator {ix+1}\")\n",
    "    print(f\"Status: {response['status']}\")\n",
    "    print(f\"Response: {response['values']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1: submitted responses per record (use only records that have at least 1 response, no matter if it's discarded) - horizontal = no of submitted responses, vertical = number of records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 2: distance between responses in rating question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratings often represent a subjective value, meaning that the is no wrong or right answer for these questions. However, since a `RatingQuestion` has a closed set of options, their results can help with visualizing the disagreement between annotators. On the other hand, texts are unique and subjective, making it almost impossible that two annotators will give the same answer for a `TextQuestion`. For this reason, we don't recommend using these responses to measure disagreements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "If you feel like the disagreement between annotators is too high, especially for questions that aren't as subjective, this is a good sign that you should review your annotation guidelines and/or the questions and options.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will explore some techniques you can use to solve disagreements in the responses. These are not the only possible techniques and you should choose them carefully according to the needs of your project and annotation team."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unifying ratings\n",
    "##### Majority vote\n",
    "If a record has more than 2 submitted responses, you can take the most popular option as the final score. In the case of a tie, you can break it choosing a random option or the lowest / highest score.\n",
    "\n",
    "##### Mean score\n",
    "For this technique you can take all responses and calculate the mean score. That will be final rating.\n",
    "\n",
    "##### Lowest / highest score\n",
    "Depending on how the question is formulated, you can take the `max` or `min` value. That will be the final rating.\n",
    "\n",
    "#### Unifying texts\n",
    "##### Rate / rank the responses\n",
    "Make a new dataset that includes the texts you have collected in the record fields and ask your annotation team to rate or rank the responses. Then choose the response with the highest score. If there is a tie, choose one of the options randomly or consider duplicating the record as explained [below](#duplicate-the-record).\n",
    "\n",
    "##### Choose based on the annotator\n",
    "Take a subset of the records (enough to get a good representation of responses from each annotator), and rate / rank them as explained in the section above. Then, give each annotator a score based on the preferences of the team. You can use this score to choose text responses over the whole dataset.\n",
    "\n",
    "##### Choose based on answers to other questions\n",
    "You can use the answers to other questions as quality markers. For example, you can assume that whoever gave the lowest score will make a more extensive correction and you may want to choose that as the final text. However, this method does not guarantee that the text will be of good quality.\n",
    "\n",
    "##### Duplicate the record\n",
    "You may consider that the different answers given by your annotation team are all valid options. In this case, you can duplicate the record to keep each answer. Again, this method does not guarantee the quality of the text, so it is recommended to "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export or publish a dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to export a dataset, load the dataset using the `from_argilla` method as demonstrated [above](#collect-responses) and turn it into the desired format. Then, you can save it, for example, as a json file or even push it to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# save as json\n",
    "with open(\"my_file.json\", \"w\") as file:\n",
    "    json.dump(feedback, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to a public dataset in the Hugging Face Hub\n",
    "feedback.push_to_huggingface(\"argilla/feedback-dataset\", generate_card=True)\n",
    "\n",
    "# push to a private dataset in the Hugging Face Hub\n",
    "feedback.push_to_huggingface(\"argilla/feedback-dataset\", generate_card=True, private=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hugging Face Hub uses git for version control, meaning that the data will be updated every time that you push using the same dataset name, but previous versions won't be lost. Feedback Datasets saved in the Hub keeping Argilla's format can be loaded ready to be pushed to Argilla as explained [above](#create-and-import-a-dataset)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d98cb9bf90a932b5bf8e86e91214497eb0e38eb318595fbd6fbd5460fe92036"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
