{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b1cd645",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "Getting started with Argilla is easy! Let`s see some examples for different NLP tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304cbaf2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594529d",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "pip install \"argilla[server]\" datasets\n",
    "```\n",
    "\n",
    "If you donâ€™t have Elasticsearch (ES) running, make sure you have docker installed and run:\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "Check the [setup and installation section](setup-and-installation) for further options and configurations regarding Elasticsearch.\n",
    "\n",
    "</div>\n",
    "```bash\n",
    "docker run -d --name elasticsearch-for-argilla -p 9200:9200 -p 9300:9300 -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch-oss:7.10.2\n",
    "```\n",
    "\n",
    "Then simply run:\n",
    "```bash\n",
    "python -m argilla\n",
    "```\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "The most common error message after this step is related to the Elasticsearch instance not running. Make sure your Elasticsearch instance is running on http://localhost:9200/. If you already have an Elasticsearch instance or cluster, you point the server to its URL by using ENV variables.\n",
    "\n",
    "</div>\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset from the hub\n",
    "dataset = load_dataset(\"argilla/gutenberg_spacy-ner\", split=\"train\")\n",
    "\n",
    "# read in dataset, assuming its a dataset for token classification\n",
    "dataset = rg.read_datasets(dataset, task=\"TokenClassification\")\n",
    "\n",
    "# log the dataset to the Rubrix web app\n",
    "rg.log(dataset_rb, \"gutenberg_spacy-ner\")\n",
    "\n",
    "# load dataset from json\n",
    "my_dataframe = pd.read_json(\n",
    "    \"https://raw.githubusercontent.com/argilla-io/datasets/main/sst-sentimentclassification.json\")\n",
    "\n",
    "# convert pandas dataframe to DatasetForTextClassification\n",
    "dataset = rg.DatasetForTextClassification.from_pandas(my_dataframe)\n",
    "\n",
    "# log the dataset to the Rubrix web app\n",
    "rg.log(dataset, name=\"sst-sentimentclassification\")\n",
    "```\n",
    "\n",
    "\n",
    "ðŸŽ‰ You can now access Argilla UI pointing your browser at [http://localhost:6900/](http://localhost:6900/). **The default username and password are** `argilla` **and** `1234`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0038d1-86b1-4eb9-ada4-ae561ad25aa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fbba86-4f60-46a1-95a1-36c8a003731d",
   "metadata": {},
   "source": [
    "The main component of the Argilla data model is called a record. A dataset in Argilla is a collection of these records. \n",
    "Records can be of different types depending on the currently supported tasks:\n",
    "\n",
    " 1. `TextClassificationRecord`\n",
    " 2. `TokenClassificationRecord`\n",
    " 3. `TextGenerationRecord`\n",
    " \n",
    "The most critical attributes of a record that are common to all types are:\n",
    "\n",
    " - `text`: The input text of the record (Required);\n",
    " - `annotation`: Annotate your record in a task-specific manner (Optional);\n",
    " - `prediction`: Add task-specific model predictions to the record (Optional);\n",
    " - `metadata`: Add some arbitrary metadata to the record (Optional);\n",
    " \n",
    "In Argilla, records are created programmatically using the [client library](../reference/python/python_client.rst) within a Python script, a [Jupyter notebook](https://jupyter.org/), or another IDE.\n",
    "\n",
    "\n",
    "Let's see how to create and upload a basic record to the Argilla web app  (make sure Argilla is already installed on your machine as described in the [setup guide](installation/installation.md)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866426c8-b3af-4307-a3eb-3d50171e4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# Create a basic text classification record\n",
    "record = rg.TextClassificationRecord(text=\"Hello world, this is me!\")\n",
    "\n",
    "# Upload (log) the record to the Argilla web app\n",
    "rg.log(record, \"my_first_record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e23a98-ec42-4f20-8087-c5eda1918455",
   "metadata": {},
   "source": [
    "Now you can access the *\"my_first_record\"* dataset in the Argilla web app and look at your first record. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84db0f9-a9ca-4799-9a26-635d2f3b94d4",
   "metadata": {},
   "source": [
    "However, most of the time, you will have your data in some file format, like TXT, CSV, or JSON. \n",
    "Argilla relies on two well-known Python libraries to read these files: [pandas](https://pandas.pydata.org/) and [datasets](https://huggingface.co/docs/datasets/index). \n",
    "After reading the files with one of those libraries, Argilla provides shortcuts to create your records automatically.\n",
    "\n",
    "Let us look at a few examples for each of the record types.\n",
    "**As mentioned earlier, you choose the record type depending on the task you want to tackle.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4137fd2-cc98-4f59-a14e-31cd7489d59b",
   "metadata": {},
   "source": [
    "### 1. Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1004cfb-6fed-4281-950f-1f19495cd114",
   "metadata": {},
   "source": [
    "In this example, we will read a [CSV file](https://www.kaggle.com/datasets/databar/10k-snapchat-reviews) from a Kaggle competition that contains reviews for the Snapchat app. \n",
    "The underlying task here could be to classify the reviews by their sentiment. \n",
    "\n",
    "Let us read the file with [pandas](https://pandas.pydata.org/)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "    \n",
    "If the file is too big to fit in memory, try using the [datasets library](https://huggingface.co/docs/datasets/index) with no memory constraints, as shown in the next section.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae148b-4d91-49ef-a7d1-6073ce8f2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "dataframe = pd.read_csv(\"Snapchat_app_store_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae293cb-4349-4d04-926f-e3abf0c3afad",
   "metadata": {},
   "source": [
    "and have a quick look at the first three rows of the resulting [pandas DataFrame](https://pandas.pydata.org/docs/getting_started/intro_tutorials/01_table_oriented.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3eb22d64-b15c-42d6-a43c-8a6f11c2bf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>userName</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>isEdited</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Savvanananahhh</td>\n",
       "      <td>4</td>\n",
       "      <td>For the most part I quite enjoy Snapchat itâ€™s ...</td>\n",
       "      <td>False</td>\n",
       "      <td>10/4/20 6:01</td>\n",
       "      <td>Performance issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Idek 9-101112</td>\n",
       "      <td>3</td>\n",
       "      <td>Iâ€™m sorry to say it, but something is definite...</td>\n",
       "      <td>False</td>\n",
       "      <td>10/14/20 2:13</td>\n",
       "      <td>What happened?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>William Quintana</td>\n",
       "      <td>3</td>\n",
       "      <td>Snapchat update ruined my story organization! ...</td>\n",
       "      <td>False</td>\n",
       "      <td>7/31/20 19:54</td>\n",
       "      <td>STORY ORGANIZATION RUINED!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          userName  rating  \\\n",
       "0           0    Savvanananahhh       4   \n",
       "1           1     Idek 9-101112       3   \n",
       "2           2  William Quintana       3   \n",
       "\n",
       "                                              review  isEdited           date  \\\n",
       "0  For the most part I quite enjoy Snapchat itâ€™s ...     False   10/4/20 6:01   \n",
       "1  Iâ€™m sorry to say it, but something is definite...     False  10/14/20 2:13   \n",
       "2  Snapchat update ruined my story organization! ...     False  7/31/20 19:54   \n",
       "\n",
       "                        title  \n",
       "0          Performance issues  \n",
       "1              What happened?  \n",
       "2  STORY ORGANIZATION RUINED!  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f7560f-0929-478a-ae4e-62274a1f04c5",
   "metadata": {},
   "source": [
    "We will choose the _review_ column as input text for our records.\n",
    "For Argilla to know, we have to rename the corresponding column to _text_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe9946-9e0f-4be9-ade2-9884d9bda998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'review' column to 'text', \n",
    "dataframe = dataframe.rename(columns={\"review\": \"text\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a191e8c9-e55a-41b2-ad24-b0860fd31445",
   "metadata": {},
   "source": [
    "We can now read this `DataFrame` with Argilla, which will automatically create the records and put them in a [Argilla Dataset](../guides/features/datasets.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f63eda-d041-4849-8828-f3de0b25cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# Read DataFrame into a Argilla Dataset\n",
    "dataset_rg = rg.read_pandas(dataframe, task=\"TextClassification\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf825a9-7aaf-4b43-970d-6b4c2d493bb6",
   "metadata": {},
   "source": [
    "We will upload this dataset to the web app and give it the name *snapchat_reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c0cb7-f129-45e7-8784-88908d882104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload (log) the Dataset to the web app\n",
    "rg.log(dataset_rg, \"snapchat_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cb8c3-5dfc-4e5a-bdf4-3c19f3d5fb00",
   "metadata": {},
   "source": [
    "![Screenshot of the uploaded snapchat reviews](../_static/reference/webapp/explore-text-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341abb81-2acd-411e-a1d3-7c54cfc257f8",
   "metadata": {},
   "source": [
    "### 2. Token classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59944e44-4202-4890-9a45-f99fc3fb2dd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will use German reviews of organic coffees in a [CSV file](https://www.kaggle.com/datasets/mldado/german-online-reviewsratings-of-organic-coffee) for this example. \n",
    "The underlying task here could be to extract all attributes of an organic coffee.\n",
    "\n",
    "This time, let's read the file with [datasets](https://huggingface.co/docs/datasets/index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502febcb-26f1-4832-8218-4f029ebed697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Read the csv file\n",
    "dataset = Dataset.from_csv(\"kaffee_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c2276-7cae-41e5-ba3b-157a8c1a6c6e",
   "metadata": {},
   "source": [
    "and have a quick look at the first three rows of the resulting [dataset Dataset](https://huggingface.co/docs/datasets/access):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1b77947c-ed89-4dce-ba75-158264c8d384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>brand</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Wenn ich Bohnenkaffee trinke (auf Arbeit trink...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>FÃ¼r mich ist dieser Kaffee ideal. Die Grundvor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Ich persÃ¶nlich bin insbesondere von dem Geschm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        brand  rating  \\\n",
       "0           0  GEPA Kaffee       5   \n",
       "1           1  GEPA Kaffee       5   \n",
       "2           2  GEPA Kaffee       5   \n",
       "\n",
       "                                              review  \n",
       "0  Wenn ich Bohnenkaffee trinke (auf Arbeit trink...  \n",
       "1  FÃ¼r mich ist dieser Kaffee ideal. Die Grundvor...  \n",
       "2  Ich persÃ¶nlich bin insbesondere von dem Geschm...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best way to visualize a Dataset is actually via pandas\n",
    "dataset.select(range(3)).to_pandas() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d75b4-c9d7-40c0-b66c-c52e9de7ef1a",
   "metadata": {},
   "source": [
    "We will choose the _review_ column as input text for our records.\n",
    "For Argilla to know, we have to rename the corresponding column to _text_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "32194771-66a3-4ecd-960e-59a8f8be8c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column(\"review\", \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d2f14-37a0-4a5a-8ae7-86f8e9304fa9",
   "metadata": {},
   "source": [
    "In contrast to the other types, token classification records need the input text **and** the corresponding tokens. \n",
    "So let us tokenize our input text in a small helper function and add the tokens to a new column called _tokens_. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "We will use [spaCy](https://spacy.io/) to tokenize the text, but you can use whatever library you prefer.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4664fe-0840-4768-b856-79bdbd1dc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a german spaCy model to tokenize our text\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Define our tokenize function\n",
    "def tokenize(row):\n",
    "    tokens = [token.text for token in nlp(row[\"text\"])]\n",
    "    return {\"tokens\": tokens}\n",
    "\n",
    "# Map the tokenize function to our dataset\n",
    "dataset = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5ee02-3f26-4db2-9729-e664e9740e18",
   "metadata": {},
   "source": [
    "Let us have a quick look at our extended `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "efcd39d2-0cb3-4ce1-b9ec-ca341d4bdb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>brand</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Wenn ich Bohnenkaffee trinke (auf Arbeit trink...</td>\n",
       "      <td>[Wenn, ich, Bohnenkaffee, trinke, (, auf, Arbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>FÃ¼r mich ist dieser Kaffee ideal. Die Grundvor...</td>\n",
       "      <td>[FÃ¼r, mich, ist, dieser, Kaffee, ideal, ., Die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Ich persÃ¶nlich bin insbesondere von dem Geschm...</td>\n",
       "      <td>[Ich, persÃ¶nlich, bin, insbesondere, von, dem,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        brand  rating  \\\n",
       "0           0  GEPA Kaffee       5   \n",
       "1           1  GEPA Kaffee       5   \n",
       "2           2  GEPA Kaffee       5   \n",
       "\n",
       "                                                text  \\\n",
       "0  Wenn ich Bohnenkaffee trinke (auf Arbeit trink...   \n",
       "1  FÃ¼r mich ist dieser Kaffee ideal. Die Grundvor...   \n",
       "2  Ich persÃ¶nlich bin insbesondere von dem Geschm...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [Wenn, ich, Bohnenkaffee, trinke, (, auf, Arbe...  \n",
       "1  [FÃ¼r, mich, ist, dieser, Kaffee, ideal, ., Die...  \n",
       "2  [Ich, persÃ¶nlich, bin, insbesondere, von, dem,...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.select(range(3)).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e40cc-51f4-4321-b42a-2301775c0e9f",
   "metadata": {},
   "source": [
    "We can now read this `Dataset` with Argilla, which will automatically create the records and put them in a [Argilla Dataset](../guides/features/datasets.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ff337-7b12-48fc-b8a1-a829a7800d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# Read Dataset into a Argilla Dataset\n",
    "dataset_rg = rg.read_datasets(dataset, task=\"TokenClassification\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb220a7-cb2c-4e38-bd41-4cf96ed478c2",
   "metadata": {},
   "source": [
    "We will upload this dataset to the web app and give it the name `coffee_reviews`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee85f1-1a37-4850-9bda-c4e54aa5db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the dataset to the Argilla web app\n",
    "rg.log(dataset_rg, \"coffee-reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c2d53-37c6-438a-b447-6dcae3369f55",
   "metadata": {},
   "source": [
    "![Screenshot of the uploaded coffee reviews](../_static/reference/webapp/features-annotate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66683c8-9ed5-4ab9-9937-39eeac9ccab0",
   "metadata": {},
   "source": [
    "### 3. Text2Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c17d862-5e64-4c34-8aa5-3941506913c6",
   "metadata": {},
   "source": [
    "In this example, we will use English sentences from the European Center for Disease Prevention and Control available at the [Hugging Face Hub](https://huggingface.co/datasets/europa_ecdc_tm). \n",
    "The underlying task here could be to translate the sentences into other European languages.\n",
    "\n",
    "Let us load the data with [datasets](https://huggingface.co/docs/datasets/index) from the [Hub](https://huggingface.co/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbce85f-200e-4b54-9650-308395b81770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Dataset from the Hugging Face Hub and extract the train split\n",
    "dataset = load_dataset(\"europa_ecdc_tm\", \"en2fr\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de908f2-ff08-4a0e-87d9-a278f9ccd452",
   "metadata": {},
   "source": [
    "and have a quick look at the first row of the resulting [dataset Dataset](https://huggingface.co/docs/datasets/access):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "16d386a5-14cb-46cd-afb2-e8405ddd5232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'Vaccination against hepatitis C is not yet available.',\n",
       "  'fr': 'Aucune vaccination contre lâ€™hÃ©patite C nâ€™est encore disponible.'}}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e41a33-11c2-422f-a5f4-411dc464f451",
   "metadata": {},
   "source": [
    "We can see that the English sentences are nested in a dictionary inside the _translation_ column. \n",
    "To extract the phrases into a new _text_ column, we will write a quick helper function and [map](https://huggingface.co/docs/datasets/process#map) the whole `Dataset` with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d93ad-86f0-4d6c-a31f-5cc1d55235a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our helper extract function\n",
    "def extract(row):\n",
    "    return {\"text\": row[\"translation\"][\"en\"]}\n",
    "\n",
    "# Map the extract function to our dataset\n",
    "dataset = dataset.map(extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aa9293-64bb-4061-b686-b3115a7942fc",
   "metadata": {},
   "source": [
    "Let us have a quick look at our extended `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c7f51520-5d8c-45ef-aae5-f3f32cad8654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'Vaccination against hepatitis C is not yet available.',\n",
       "  'fr': 'Aucune vaccination contre lâ€™hÃ©patite C nâ€™est encore disponible.'},\n",
       " 'text': 'Vaccination against hepatitis C is not yet available.'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e0dac-7772-49ae-b57e-52619ebf5899",
   "metadata": {},
   "source": [
    "We can now read this `Dataset` with Argilla, which will automatically create the records and put them in a [Argilla Dataset](../guides/features/datasets.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f98a37-5fda-4e79-aff1-f3fb770498ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# Read Dataset into a Argilla Dataset\n",
    "dataset_rg = rg.read_datasets(dataset, task=\"Text2Text\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5236bf1-f98c-411f-81d0-e31740f0fc10",
   "metadata": {},
   "source": [
    "We will upload this dataset to the web app and give it the name `ecdc_en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da01d6f-2728-4ed0-b6aa-cd2e2531202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the dataset to the Argilla web app\n",
    "rg.log(dataset_rg, \"ecdc_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4969a-9132-459d-9f8d-e0006a1d52a0",
   "metadata": {},
   "source": [
    "![Screenshot of the uploaded English phrases.](../_static/reference/webapp/explore-text2text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3139dd1-a939-4baa-8d26-bd1214c8cbcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Label datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfeaf75-143e-4543-85f3-2a6e995dcf06",
   "metadata": {},
   "source": [
    "Argilla provides several ways to label your data. Using Argilla's UI, you can mix and match the following options:\n",
    "\n",
    "1. Manually labeling each record using the specialized interface for each task type;\n",
    "2. Leveraging a user-provided model and validating its predictions;\n",
    "3. Defining heuristic rules to produce \"noisy labels\" which can then be combined with weak supervision;\n",
    "\n",
    "Each way has its pros and cons, and the best match largely depends on your individual use case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bccc4-05c7-4c27-920b-5b76eb4acd22",
   "metadata": {},
   "source": [
    "### 1. Manual labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b085744-7c61-4614-b542-c4de2cea9181",
   "metadata": {},
   "source": [
    "![Manual annotations of a sentiment classification task](../_static/reference/webapp/features-metrics.png)\n",
    "\n",
    "The straightforward approach of manual annotations might be necessary if you do not have a suitable model for your use case or cannot come up with good heuristic rules for your dataset. \n",
    "It can also be a good approach if you dispose of a large annotation workforce or require few but unbiased and high-quality labels.\n",
    "\n",
    "Argilla tries to make this relatively cumbersome approach as painless as possible. \n",
    "Via an intuitive and adaptive UI, its exhaustive search and filter functionalities, and bulk annotation capabilities, Argilla turns the manual annotation process into an efficient option.  \n",
    "\n",
    "Look at our dedicated [feature reference](../reference/webapp/features.md) for a detailed and illustrative guide on manually annotating your dataset with Argilla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631840b-9cf7-45e6-9dc4-5f6b24cf0e8b",
   "metadata": {},
   "source": [
    "### 2. Validating predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c46840-0991-4fc0-bbb0-15df33ee242b",
   "metadata": {},
   "source": [
    "![Validate predictions for a token classification dataset](../_static/reference/webapp/features-validation.png)\n",
    "\n",
    "Nowadays, many pre-trained or zero-shot models are available online via model repositories like the Hugging Face Hub. \n",
    "Most of the time, you probably will find a model that already suits your specific dataset task to some degree. \n",
    "In Argilla, you can pre-annotate your data by including predictions from these models in your records.\n",
    "Assuming that the model works reasonably well on your dataset, you can filter for records with high prediction scores and validate the predictions.\n",
    "In this way, you will rapidly annotate part of your data and alleviate the annotation process.\n",
    "\n",
    "One downside of this approach is that your annotations will be subject to the possible biases and mistakes of the pre-trained model.\n",
    "When guided by pre-trained models, it is common to see human annotators get influenced by them.\n",
    "Therefore, it is advisable to avoid pre-annotations when building a rigorous test set for the final model evaluation.\n",
    "\n",
    "Check the [introduction tutorial](../tutorials//notebooks/labelling-tokenclassification-spacy-pretrained.ipynb) to learn to add predictions to the records. \n",
    "And our [feature reference](../reference/webapp/features.md) includes a detailed guide on validating predictions in the Argilla web app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cbd593-e241-4f27-9a58-6932912ea9f1",
   "metadata": {},
   "source": [
    "### 3. Defining rules (weak labeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7ea92-d40f-4d09-a0e8-16b7d7867e6e",
   "metadata": {},
   "source": [
    "![Defining a rule for a multi-label text classification task.](../_static/reference/webapp/features-weak-labelling.png)\n",
    "\n",
    "Another approach to annotating your data is to define heuristic rules tailored to your dataset. \n",
    "For example, let us assume you want to classify news articles into the categories of *Finance*, *Sports*, and *Culture*. \n",
    "In this case, a reasonable rule would be to label all articles that include the word \"stock\" as *Finance*. \n",
    "\n",
    "Rules can get arbitrarily complex and can also include the record's metadata. \n",
    "The downsides of this approach are that it might be challenging to come up with working heuristic rules for some datasets. \n",
    "Furthermore, rules are rarely 100% precise and often conflict with each other.These noisy labels can be cleaned up using weak supervision and label models, or something as simple as majority voting. It is usually a trade-off between the amount of annotated data and the quality of the labels.\n",
    "\n",
    "Check [our guide](../guides/techniques/weak_supervision.ipynb) for an extensive introduction to weak supervision with Argilla. \n",
    "Also, check the [feature reference](../reference/webapp/features.md) for the Define rules mode of the web app and our [various tutorials](../tutorials/techniques/weak_supervision.md) to see practical examples of weak supervision workflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1144ce2-0fe2-48c0-8116-d57dc4429640",
   "metadata": {},
   "source": [
    "## How to prepare your data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5437bcd-b42c-4f6b-a9f5-d4b45572c648",
   "metadata": {},
   "source": [
    "Once you have uploaded and annotated your dataset in Argilla, you are ready to prepare it for training a model. Most NLP models today are trained via [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) and need input-output pairs to serve as training examples for the model. The input part of such pairs is usually the text itself, while the output is the corresponding annotation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62573a8-54c8-4002-9686-3450ad90c7a3",
   "metadata": {},
   "source": [
    "### Manual extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1923f-b100-4755-aba9-68d7de48d247",
   "metadata": {},
   "source": [
    "The exact data format for training a model depends on your [training framework](#how-to-train-a-model) and the task you are tackling (text classification, token classification, etc.). Argilla is framework agnostic; you can always manually extract from the records what you need for the training. \n",
    "\n",
    "The extraction happens using the [client library](../reference/python/python_client.rst) within a Python script, a Jupyter notebook, or another IDE. First, we have to load the annotated dataset from the Argilla UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ed86e-881d-4196-adc2-ebe01dacb306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "dataset = rg.load(\"my_annotated_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d061ca1a-98db-4c31-9362-f816e401c2b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "    \n",
    "If you follow a weak supervision approach, the steps are slightly different. \n",
    "We refer you to our [weak supervision guide](../guides/techniques/weak_supervision.ipynb) for a complete workflow.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d94095-7cac-4810-97fe-c257b8f34a2c",
   "metadata": {},
   "source": [
    "Then we can iterate over the records and extract our training examples. For example, let's assume you want to train a text classifier with a [sklearn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that takes as input a text and outputs a label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34397c3-5b0d-4151-9945-135f54520f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the inputs and labels in Python lists\n",
    "inputs, labels = [], []\n",
    "\n",
    "# Iterate over the records in the dataset\n",
    "for record in dataset:\n",
    "    \n",
    "    # We only want records with annotations\n",
    "    if record.annotation:\n",
    "        inputs.append(record.text)\n",
    "        labels.append(record.annotation)\n",
    "\n",
    "# Train the model\n",
    "sklearn_pipeline.fit(inputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd8be8e-dd83-4506-8ed8-2f32cf6b5835",
   "metadata": {},
   "source": [
    "### Automatic extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf82fc5-d3a5-4a56-b308-caf31a4d763b",
   "metadata": {},
   "source": [
    "For a few frameworks and tasks, Argilla provides a convenient method to automatically extract training examples in a suitable format from a dataset. \n",
    "\n",
    "For example: If you want to train a [transformers](https://huggingface.co/docs/transformers/index) model for text classification, you can load an annotated dataset for text classification and call the `prepare_for_training()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c2aa8-603b-48a5-8da4-11c7e93f9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.load(\"my_annotated_dataset\")\n",
    "\n",
    "dataset_for_training = dataset.prepare_for_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ee9dc-93b8-4802-83bc-e63048a83073",
   "metadata": {},
   "source": [
    "With the returned `dataset_for_training`, you can continue following the steps to [fine-tune a pre-trained model](https://huggingface.co/docs/transformers/training#finetune-a-pretrained-model) with the [transformers library](https://huggingface.co/docs/transformers/index). \n",
    "\n",
    "Check the dedicated [dataset guide](../guides/features/datasets.ipynb#prepare-dataset-for-training) for more examples of the `prepare_for_training()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90307acf-ba85-4f8c-86d3-ca398be7a496",
   "metadata": {},
   "source": [
    "## How to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb1351-6324-4faa-9067-fd50785844f5",
   "metadata": {},
   "source": [
    "Argilla helps you to create and curate training data. **It is not a framework for training a model.** You can use Argilla complementary with other excellent open-source frameworks that focus on developing and training NLP models.\n",
    "\n",
    "Here we list three of the most commonly used open-source libraries, but many more are available and may be more suited for your specific use case:\n",
    "\n",
    " - [transformers](https://huggingface.co/docs/transformers/index): This library provides thousands of pre-trained models for various NLP tasks and modalities. Its excellent documentation focuses on fine-tuning those models to your specific use case;\n",
    " - [spaCy](https://spacy.io/): This library also comes with pre-trained models built into a pipeline tackling multiple tasks simultaneously. Since its a purely NLP library, it comes with much more NLP features than just model training;\n",
    " - [scikit-learn](https://scikit-learn.org/stable/): This de facto standard library is a powerful swiss army knife for machine learning with some NLP support. Usually, their NLP models lack the performance when compared to transformers or spacy, but give it a try if you want to train a lightweight model quickly; \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb28fb5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf5ecc3fecf17575e278cfc1ec050b1bf24f64b662a1b5421daed12bdd8255ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
