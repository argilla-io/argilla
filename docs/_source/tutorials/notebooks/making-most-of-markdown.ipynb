{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Most of Markdown within Argilla TextFields\n",
    "\n",
    "As you may have noticed, Argilla supports Markdown within its text fields. This means you can easily add formatting like **bold** and *italic* text, [links](https://www.google.com), and even insert HTML elements like images, audios, videos, and iframes. It's a powerful tool to have at your disposal. Let's dive in!\n",
    "\n",
    "In this notebook, we will go over the basics of Markdown, and how to use it within Argilla.\n",
    "\n",
    "* Exploiting the power of `displaCy` for NER and relationship extraction.\n",
    "* Exploring multi-modality: videos, audio, and image.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Argilla\n",
    "\n",
    "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
    "\n",
    "\n",
    "**Deploy Argilla on Hugging Face Spaces**: If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:\n",
    "\n",
    "[![deploy on spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/new-space?template=Argilla)\n",
    "\n",
    "For details about configuring your deployment, check the [official Hugging Face Hub guide](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla).\n",
    "\n",
    "\n",
    "**Launch Argilla using Argilla's quickstart Docker image**: This is the recommended option if you want [Argilla running on your local machine](../../getting_started/quickstart.html). Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
    "\n",
    "For more information on deployment options, please check the Deployment section of the documentation.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Tip\n",
    "    \n",
    "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
    "\n",
    "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
    "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter Notebook tool of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Environment\n",
    "\n",
    "To complete this tutorial, you will need to install the Argilla client and a few third-party libraries using `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "%pip install argilla==1.18\n",
    "%pip install datasets\n",
    "%pip install spacy spacy-transformers\n",
    "%pip install Pillow\n",
    "%pip install span_maker\n",
    "%pip install soundfile librosa\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the needed imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from argilla.client.feedback.utils import audio_to_html, image_to_html, video_to_html\n",
    "\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import span_marker\n",
    "import tarfile\n",
    "import glob\n",
    "import subprocess\n",
    "import random\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the `URL` and `API_KEY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "# Replace workspace with the name of your workspace\n",
    "rg.init(\n",
    "    api_url=\"http://localhost:6900\", \n",
    "    api_key=\"owner.apikey\",\n",
    "    workspace=\"admin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running a private Hugging Face Space, you will also need to set the [`HF_TOKEN`](https://huggingface.co/settings/tokens) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the HF_TOKEN environment variable\n",
    "# import os\n",
    "# os.environ['HF_TOKEN'] = \"your-hf-token\"\n",
    "\n",
    "# # Replace api_url with the url to your HF Spaces URL\n",
    "# # Replace api_key if you configured a custom API key\n",
    "# rg.init(\n",
    "#     api_url=\"https://hf-space.hf.space\", \n",
    "#     api_key=\"admin.apikey\",\n",
    "#     extra_headers={\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploiting `displaCy`\n",
    "\n",
    "[SpaCy](https://spacy.io/) is a well-know open-source library for Natural Language Processing (NLP). It provides a wide range of models for different languages, and it is very easy to use. One of the provided options is [`displaCy`](https://spacy.io/usage/visualizers) a visualizer for the output of the NLP models. In this tutorial, we will be using it to visualize the output of the NER model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `displaCy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will explain how `displaCy` works by importing the English SpaCy pipeline (`en_core_web_sm`) while excluding the default NER component. Later, we replace this component by using the `add_pipe` method to introduce the new `span_marker` component at the pipeline's conclusion. This new component is responsible for conducting NER training with the specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the custom pipeline\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\", \n",
    "    exclude=[\"ner\"]\n",
    ")\n",
    "nlp.add_pipe(\"span_marker\", config={\"model\": \"tomaarsen/span-marker-bert-tiny-fewnerd-coarse-super\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can check how using the `displacy.render` function, which takes in the text and the output of the model, returns a HTML string. BBelow, two examples are provided: the first illustrates the sentence's dependency tree, while the second showcases the NER findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the dependency parse\n",
    "doc = nlp(\"Rats are various medium-sized, long-tailed rodents.\")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">When \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sebastian Thrun\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">person</span>\n",
       "</mark>\n",
       " started working on self-driving cars at \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">organization</span>\n",
       "</mark>\n",
       " in 2007, few people outside of the company took him seriously.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the entity recognition\n",
    "text = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n",
    "doc2 = nlp(text)\n",
    "displacy.render(doc2, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: creating a `FeedbackDataset` with the displaCy output\n",
    "\n",
    "In the example, we show how to create an Argilla `FeedbackDataset` adding the displaCy output to it. This way, we can check the accuracy of the model and asses whether the user want to apply a correction to the dependencies and/or entities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we [configure the `FeedbackDataset`](/practical_guides/create_dataset.html#configure-the-dataset). In fields, we use three TextField to show the default text, dependencies and entities. While, in questions, we add a LabelQuestion, a  MultiLabelQuestion and two TextQuestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FeedbackDataset configuration\n",
    "dataset_spacy = rg.FeedbackDataset(\n",
    "    fields=[\n",
    "        rg.TextField(name=\"text\", required= True, use_markdown=True),\n",
    "        rg.TextField(name=\"dependency-tree\", required= True, use_markdown=True),\n",
    "        rg.TextField(name=\"entities\", required= True, use_markdown=True)\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.LabelQuestion(name=\"relevant\", title=\"Is the text relevant?\", labels=[\"Yes\", \"No\"], required=True),\n",
    "        rg.MultiLabelQuestion(name=\"question-multi\", title=\"Mark which is correct\", labels=[\"flag-pos\", \"flag-ner\"], required=True),\n",
    "        rg.TextQuestion(name=\"dependency-correction\", title=\"Write the correct answer if needed\", use_markdown=True),\n",
    "        rg.TextQuestion(name=\"ner-correction\", title=\"Write the correct answer if needed\", use_markdown=True)\n",
    "    ]\n",
    ")\n",
    "dataset_spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the basic [few-nerd dataset from Hugging Face](https://huggingface.co/datasets/DFKI-SLT/few-nerd). This dataset contains a few sentences, and the output of the NER model. We will be using this dataset to show how to use displaCy within Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the HF dataset\n",
    "dataset_fewnerd = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\", split=\"train[:20]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use this dataset to populate our Argilla `FeedbackDataset`. We will be using the `displacy.render` function to render the displacy output as html setting `jupyter=False`, and add it to the `FeedbackDataset`. We will also add the text, and the output of the NER model to the `FeedbackDataset`. Finally, we will also add markdown formatted tables to support basic support for NER and dependency annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the custom pipeline\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\", \n",
    "    exclude=[\"ner\"]\n",
    ")\n",
    "nlp.add_pipe(\"span_marker\", config={\"model\": \"tomaarsen/span-marker-bert-tiny-fewnerd-coarse-super\"})\n",
    "\n",
    "# Read the dataset and run the pipeline\n",
    "texts = [\" \".join(x[\"tokens\"]) for x in dataset_fewnerd]\n",
    "docs = nlp.pipe(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to set the correct width and height of the SVG element\n",
    "def wrap_in_max_width(html):\n",
    "    html = html.replace(\"max-width: none;\", \"\")\n",
    "    \n",
    "    # Remove existing width and height setting based on regex width=\"/d\"\n",
    "    html = re.sub(r\"width=\\\"\\d+\\\"\", \"overflow-x: auto;\", html)\n",
    "    html = re.sub(r\"height=\\\"\\d+\\\"\", \"\", html)\n",
    "    \n",
    "    # Find the SVG element in the HTML output\n",
    "    svg_start = html.find(\"<svg\")\n",
    "    svg_end = html.find(\"</svg>\") + len(\"</svg>\")\n",
    "    svg = html[svg_start:svg_end]\n",
    "\n",
    "    # Set the width and height attributes of the SVG element to 100%\n",
    "    svg = svg.replace(\"<svg\", \"<svg width='100%' height='100%'\")\n",
    "\n",
    "    # Wrap the SVG element in a div with max-width and horizontal scrolling\n",
    "    return f\"<div style='max-width: 100%; overflow-x: auto;'>{svg}</div>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the records to the FeedbackDataset\n",
    "records = []\n",
    "for doc in docs:\n",
    "    record = rg.FeedbackRecord(\n",
    "        fields={\n",
    "            \"text\": doc.text, \n",
    "            \"dependency-tree\": displacy.render(doc, style=\"dep\", jupyter=False), \n",
    "            \"entities\": displacy.render(doc, style=\"ent\", jupyter=False)\n",
    "        },\n",
    "        suggestions=[{\n",
    "                \"question_name\": \"dependency-correction\", \n",
    "                \"value\": pd.DataFrame([{\"Label\": token.dep_, \"Text\": token.text} for token in doc]).to_markdown(index=False)\n",
    "\n",
    "            },\n",
    "            {\n",
    "                \"question_name\": \"ner-correction\", \n",
    "                \"value\": pd.DataFrame([{\"Label\": ent.label_, \"Text\": ent.text} for ent in doc.ents]).to_markdown(index=False),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    records.append(record)\n",
    "    \n",
    "dataset_spacy.add_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the dataset to Argilla\n",
    "dataset_spacy = dataset_spacy.push_to_argilla(name=\"exploiting_displacy\", workspace=\"admin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Multi-Modality: Video, Audio and Image\n",
    "\n",
    "As we already mentioned, Argilla supports handling video, audio, and images within markdown fields, provided they are formatted in HTML. To facilitate this, we offer three functions: `video_to_html`, `audio_to_html`, and `image_to_html`. These functions accept either the file path or the file's byte data and return the corresponding HTMurl to render the media file within the Argilla user interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define our `FeedbackDataset` with a TextField to add the media content. We will also add a question to ask the user to describe the video, audio, or image file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the FeedbackDataset\n",
    "ds_multi_modal = rg.FeedbackDataset(\n",
    "    fields=[rg.TextField(name=\"content\", use_markdown=True, required=True)],\n",
    "    questions=[rg.TextQuestion(name=\"description\", title=\"Describe the content of the media:\", use_markdown=True, required=True)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the corresponding functions to the `add_records`-method and we add the media content to the `FeedbackDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the records\n",
    "records = [\n",
    "    rg.FeedbackRecord(fields={\"content\": video_to_html(\"data/snapshot.mp4\")}),\n",
    "    rg.FeedbackRecord(fields={\"content\": audio_to_html(\"data/mixkit-close-sea-waves-loop-1195.wav\")}),\n",
    "    rg.FeedbackRecord(fields={\"content\": image_to_html(\"data/peacock-1169961.jpg\")}),\n",
    "]\n",
    "ds_multi_modal.add_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the dataset to Argilla\n",
    "ds_multi_modal = ds_multi_modal.push_to_argilla(\"multi-modal-basic\", workspace=\"admin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Creating a multi-modal-video-audio-image `FeedbackDataset`\n",
    "\n",
    "A multi-modal dataset can be used to create a dataset with text and different types of media content. It can be useful for different tasks, such as image captioning, video captioning, audio captioning, and so on. Now, we will create our own  as examples that will combine videos, audio and images that can be filtered using the metadata properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this purpose, we will be using three different datasets from Hugging Face:\n",
    "\n",
    "* **Video**: We use an action recognition dataset, the [ucf101-subset](https://huggingface.co/datasets/sayakpaul/ucf101-subset) from the [UCF101](https://www.crcv.ucf.edu/data/UCF101.php). This dataset contains realistic action videos from YouTube, classified in 101 actions.\n",
    "\n",
    "* **Audio**: We use an audio classification dataset, the [ccmusic-database/bel_folk](https://huggingface.co/datasets/ccmusic-database/bel_folk). This dataset contains 1 minute audio clips of Chinese folk music, and the genre of the music.\n",
    "\n",
    "* **Image**: We use an image classification dataset, the [zishuod/pokemon-icons](https://huggingface.co/datasets/zishuod/pokemon-icons). This dataset contains images of Pokemon that need to be classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will create the configuration of our Feedback dataset. This will involve a few key components:\n",
    "\n",
    "- TextField: This will be used for adding the media content.\n",
    "- TextQuestion: A feature to describe the content in detail.\n",
    "- RatingQuestion: This will allow us to rate the content's quality effectively.\n",
    "- LabelQuestion: An essential part for tagging the content with the most suitable age group.\n",
    "\n",
    "Additionally, we will enhance the configuration with three metadata properties to streamline content management:\n",
    "\n",
    "- The first property will identify the assigned annotator group.\n",
    "- The second will specify the media source.\n",
    "- The third will highlight the source dataset of the content in each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We configure the FeedbackDataset\n",
    "multi_modal_dataset = rg.FeedbackDataset(\n",
    "    guidelines=\"Please, read the question carefully and try to answer it as accurately as possible.\",\n",
    "    fields=[\n",
    "        rg.TextField(name=\"content\", use_markdown=True, required=True),\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.TextQuestion(\n",
    "            name=\"description\",\n",
    "            title=\"Describe the content of the media.\", \n",
    "            use_markdown=True, \n",
    "            required=True\n",
    "        ),\n",
    "        rg.RatingQuestion(\n",
    "            name=\"quality\",\n",
    "            description=\"Rate the overall quality of the content on a scale from 1 to 5.\",\n",
    "            required=True,\n",
    "            values=[1, 2, 3, 4, 5]\n",
    "        ),\n",
    "        rg.LabelQuestion(\n",
    "            name=\"age_group\",\n",
    "            description=\"Select the most appropriate age group for this content.\",\n",
    "            required=True,\n",
    "            labels = [\"Children\", \"Teens\", \"Adults\", \"All Ages\"]\n",
    "        ),\n",
    "    ],\n",
    "    metadata_properties = [\n",
    "        rg.TermsMetadataProperty(\n",
    "            name=\"groups\",\n",
    "            title=\"Annotation groups\",\n",
    "            values=[\"group-a\", \"group-b\", \"group-c\"],\n",
    "        ),\n",
    "        rg.TermsMetadataProperty(\n",
    "            name=\"media\",\n",
    "            title=\"Media source\",\n",
    "            values=[\"video\", \"audio\", \"image\"],\n",
    "        ),\n",
    "        rg.TermsMetadataProperty(\n",
    "            name=\"source-dataset\",\n",
    "            title=\"Original dataset source\",\n",
    "        ),\n",
    "    ],\n",
    "    allow_extra_metadata = False\n",
    ")\n",
    "\n",
    "try:\n",
    "    multi_modal_dataset = rg.FeedbackDataset.from_argilla(\"multi-modal\")\n",
    "except:\n",
    "    multi_modal_dataset = multi_modal_dataset.push_to_argilla(\"multi-modal\")\n",
    "\n",
    "multi_modal_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to load and preprocess the datasets to be able to add the media content to the `FeedbackDataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Video content: We download the dataset and unzip it. Then, from the train, we randomly select two .avi files from each class and use ffmpeg to convert them to .mp4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n",
    "filename = \"UCF101_subset.tar.gz\"\n",
    "file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the files\n",
    "with tarfile.open(file_path) as t:\n",
    "     t.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths to the input and output directories.\n",
    "base_directory = \"/content/UCF101_subset/train\"\n",
    "output_directory = \"/content/UCF101_subset/random\"\n",
    "\n",
    "# Check if the output directory exists, if not, create it.\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Iterate over the directories (each corresponding to a class) in the 'train' folder.\n",
    "for folder in os.listdir(base_directory):\n",
    "    folder_path = os.path.join(base_directory, folder)\n",
    "    \n",
    "    # Check if it's a directory.\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Get all .avi files in the directory.\n",
    "        avi_files = glob.glob(os.path.join(folder_path, \"*.avi\"))\n",
    "        \n",
    "        # Randomly select 2 .avi files.\n",
    "        selected_files = random.sample(avi_files, 2)\n",
    "        \n",
    "        for avi_file in selected_files:\n",
    "            # Define the output .mp4 file path.\n",
    "            mp4_file = os.path.join(output_directory, os.path.basename(avi_file).replace(\".avi\", \".mp4\"))\n",
    "            \n",
    "            # Command to convert .avi to .mp4 using ffmpeg.\n",
    "            command = f\"ffmpeg -i \\\"{avi_file}\\\" -c:v libx264 -c:a aac \\\"{mp4_file}\\\"\"\n",
    "            \n",
    "            try:\n",
    "                subprocess.run(command, check=True, shell=True)\n",
    "                print(f\"Converted {avi_file} to {mp4_file}\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(e)\n",
    "\n",
    "print(\"Conversion process is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Audio content: We load the first 20 samples from HF and shuffle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_audio_dataset = load_dataset(\"ccmusic-database/bel_folk\", split=\"train[:20]\")\n",
    "my_audio_dataset = my_audio_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_audio_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Image dataset: We load the first 20 samples from HF and shuffle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image_dataset  = load_dataset(\"zishuod/pokemon-icons\", split=\"train[:20]\")\n",
    "my_image_dataset = my_image_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will add and push the data to our FeedbackDataset. We will use the `video_to_html`, `audio_to_html`, and `image_to_html` functions to add the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding the video records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory where we saved our samples\n",
    "random_dir = output_directory\n",
    "\n",
    "# Get a list of all entries in the directory\n",
    "entries = os.listdir(random_dir)\n",
    "\n",
    "# Shuffle the list of entries to avoid any bias\n",
    "random.shuffle(entries)\n",
    "\n",
    "# Iterate over all the entries in random order\n",
    "for entry in entries:\n",
    "    \n",
    "    # Define the full path\n",
    "    fullPath = os.path.join(random_dir, entry)\n",
    "\n",
    "    # Add the records to the FeedbackDataset\n",
    "    record = rg.FeedbackRecord(\n",
    "        fields={\"content\": video_to_html(fullPath)},\n",
    "        metadata={\"groups\":\"group-a\", \"media\":\"video\", \"source-dataset\":\"https://huggingface.co/datasets/sayakpaul/ucf101-subset\"}\n",
    "    )\n",
    "    try:\n",
    "        multi_modal_dataset.add_records(record, show_progress=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding the audio records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the samples in the dataset\n",
    "for entry in my_audio_dataset:\n",
    "    \n",
    "    # Add the records to the FeedbackDataset\n",
    "    record = rg.FeedbackRecord(\n",
    "        fields={\"content\": audio_to_html(entry[\"audio\"][\"path\"])},\n",
    "        metadata={\"groups\":\"group-b\", \"media\":\"audio\", \"source-dataset\":\"https://huggingface.co/datasets/ccmusic-database/bel_folk\"}\n",
    "    )\n",
    "    try:\n",
    "        multi_modal_dataset.add_records(record, show_progress=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding the image records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a temporary path to save the image\n",
    "temp_img_path = \"temp_img.png\"\n",
    "\n",
    "# Iterate over the samples in the dataset\n",
    "records = []\n",
    "for entry in my_image_dataset:\n",
    "    \n",
    "    # Save the image to the temporary path\n",
    "    entry[\"image\"].save(temp_img_path, format=\"png\")\n",
    "    \n",
    "    # Add the records to the FeedbackDataset\n",
    "    record = rg.FeedbackRecord(\n",
    "        fields={\"content\": image_to_html(temp_img_path, file_type=\"png\")},\n",
    "        metadata={\"groups\":\"group-c\", \"media\":\"image\", \"source-dataset\":\"https://huggingface.co/datasets/zishuod/pokemon-icons\"}\n",
    "    )\n",
    "    try:\n",
    "        multi_modal_dataset.add_records(record, show_progress=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish, we can push the FeedbackDataset to HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to HuggingFace Hub\n",
    "multi_modal_dataset.push_to_huggingface(\"argilla/multi-modal-dataset\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
