{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ⓜ️ Finetuning LLMs: Supervised Finetuning on Mistral 7B\n",
    "In this tutorial, you will learn how to finetune a Large Language Model (LLM), Mistral 7B in particular, on a chat-style instruction dataset. We start with [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1), an LLM that only does text completion, and we end up with our own [argilla/Mistral-7B-v0.1-chat-OIG](https://huggingface.co/argilla/Mistral-7B-v0.1-chat-OIG) model that faithfully follows instructions and acts as a helpful chat assistant.\n",
    "\n",
    "This tutorial consists of the following steps:\n",
    "1. Preparing a [FeedbackDataset](https://docs.argilla.io/en/latest/conceptual_guides/data_model.html#feedback-dataset) in Argilla. <!-- using a public chat-style instruction dataset-->\n",
    "2. (Optional) Annotate instruction samples.\n",
    "3. Set up the [ArgillaTrainer](https://docs.argilla.io/en/latest/practical_guides/fine_tune.html#supervised-finetuning) for Supervised Finetuning.\n",
    "4. Perform inference using the finetuned LLM.\n",
    "5. Publish the resulting model and dataset on the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Mistral 7B\n",
    "In this tutorial we will be finetuning Mistral 7B, which is a powerful LLM developed by the Mistral AI team featuring 7.3 billion parameters. It stands out for its exceptional performance relative to its size, outperforming larger models like Llama 2 13B and Llama 1 34B on various benchmarks. \n",
    "\n",
    "Another key aspect is its ability to perform well for longer sequences, and noticeably, it is released under the permissive Apache 2.0 license. This allows it to be used in commercial use cases with no strings attached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Argilla\n",
    "\n",
    "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
    "\n",
    "**Deploy Argilla on Hugging Face Spaces:** If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:\n",
    "\n",
    "[![deploy on spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/new-space?template=argilla/argilla-template-space)\n",
    "\n",
    "For details about configuring your deployment, check the [official Hugging Face Hub guide](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla).\n",
    "\n",
    "**Launch Argilla using Argilla's quickstart Docker image**: This is the recommended option if you want [Argilla running on your local machine](../../getting_started/quickstart.ipynb). Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
    "\n",
    "For more information on deployment options, please check the Deployment section of the documentation.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Tip\n",
    "\n",
    "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
    "\n",
    "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
    "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "Let's start by installing the required dependencies to run both Argilla and the remainder of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"argilla~=1.16.0\" \"transformers~=4.34.0\" \"datasets~=2.14.5\" \"peft~=0.5.0\" \"trl~=0.7.1\" \"wandb~=0.15.12\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you must also install `torch` with `CUDA` support. The widget [here](https://pytorch.org/get-started/locally) should be helpful if `torch` is not already set up.\n",
    "\n",
    "Let's import the Argilla module for reading and writing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the `URL` and `API_KEY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "rg.init(\n",
    "    api_url=\"http://localhost:6900\",\n",
    "    api_key=\"admin.apikey\",\n",
    "    workspace=\"admin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a FeedbackDataset in Argilla\n",
    "In Argilla, the [FeedbackDataset](https://docs.argilla.io/en/latest/conceptual_guides/data_model.html#feedback-dataset) is a powerful and widely-configurable class that is in charge of defining the annotation process. In particular, we define [fields](https://docs.argilla.io/en/latest/conceptual_guides/data_model.html#field) and [questions](https://docs.argilla.io/en/latest/conceptual_guides/data_model.html#question).\n",
    "\n",
    "The former is in charge of defining the structure for the data that will be annotated, while the latter determines in what way the annotators can annotate the data. In practice, `FeedbackDataset` instances for finetuning LLMs often have \"prompt\" and \"response\" text fields, sometimes alongside a \"context\" text field or some additional metadata (e.g. sample IDs).\n",
    "\n",
    "The questions provide a lot of flexibility - text questions allow annotators to provide better responses or suggestions to improve the prompt, while label questions may be used to select if a sample is considered \"good\" or not. With multi-label questions annotators can select whether samples are biased, harmful, incorrect, etc.\n",
    "\n",
    "We recommend to set up the questions based on your goals and values for the annotation process. When preparing the training data, all of the annotations will be available, allowing you to carefully curate your training data. For example, based on your annotations you will be able to disregard samples that are (frequently) marked as harmful or low quality, or you can use the annotator-provided suggestions to responses or prompts instead of the existing ones.\n",
    "Furthermore, in some situations you may only have prompts to begin with, and you can use Argilla to request your annotators to provide responses. You can then use these in your training data.\n",
    "\n",
    "### For this tutorial\n",
    "\n",
    "For the purposes of this tutorial, we will use a \"prompt\" field with all chat history and the current prompt, as well as a \"response\" field with the response. Additionally, we introduce a \"background\". For the questions we will use one simple `LabelQuestion` that asks the annotator whether the response is `\"Good\"` or `\"Bad\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.FeedbackDataset(\n",
    "    fields = [\n",
    "        rg.TextField(name=\"background\"),\n",
    "        rg.TextField(name=\"prompt\"),\n",
    "        rg.TextField(name=\"response\", title=\"Final Response\"),\n",
    "    ],\n",
    "    questions = [\n",
    "        rg.LabelQuestion(name=\"quality\", title=\"Is it a Good or Bad response?\", labels=[\"Good\", \"Bad\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, we have to create individual [records](https://docs.argilla.io/en/latest/conceptual_guides/data_model.html#record) and push those to the dataset. For this tutorial, we will use some existing Apache-2.0 data from the [Open Instruction Generalist](https://huggingface.co/datasets/laion/OIG) chat-style instruction dataset. Due to the large size of this dataset, we will load the `dataset` with `streaming=True`, and semi-randomly sample 30k samples from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162bfd11e03441d8a487a41717cff159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset at 0x22fa12c73d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"laion/OIG\", split=\"train\", streaming=True)\n",
    "# Randomly sample through shuffle + take, but note that shuffling with a streaming\n",
    "# dataset works with a buffer size, so the random 30k samples are not truly random\n",
    "# Reduce the buffer size to speed up the data selection process.\n",
    "data = data.shuffle(buffer_size=1_000_000).take(30_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded data has a \"text\" column with data that looks like so:\n",
    "```\n",
    "Background: Tubulin in molecular biology can refer either to the tubulin protein superfamily of globular proteins, or one of the member proteins of that superfamily. α- and β-tubulins polymerize into microtubules, a major component of the eukaryotic cytoskeleton. Microtubules function in many essential cellular processes, including mitosis. Tubulin-binding drugs kill cancerous cells by inhibiting microtubule dynamics, which are required for DNA segregation and therefore cell division. , there are six members of the tubulin superfamily, although not all are present in all species. Both α and β tubulins have a mass of around 50 kDa and are thus in a similar range compared to actin (with a mass of ~42 kDa). In contrast, tubulin polymers (microtubules) tend to be much bigger than actin filaments due to their cylindrical nature. Tubulin was long thought to be specific to eukaryotes. More recently, however, several prokaryotic proteins have been shown to be related to tubulin. <human>: What are some examples of drugs that bind to tubulin? <bot>: These include paclitaxel, colchicine, and the \"vinca\" alkaloids, each of which have a distinct binding site on β-tubulin. <human>: Are there any other examples? <bot>: In addition, the anti-worm drug albendazole selectively targets the colchicine site in worm, but not human or \"Drosophilia\", β-Tubulin. <human>: What is another specific example of a drug that binds to tubulin? <bot>: Class III β-tubulin is a microtubule element expressed exclusively in neurons, and is a popular identifier specific for neurons in nervous tissue. <human>: What is the difference between class III and other isotypes? <bot>: It binds colchicine much more slowly than other isotypes of β-tubulin. <human>: Are there six classes of isotypes? <bot>: β1-tubulin, sometimes called class VI β-tubulin, is the most divergent at the amino acid sequence level. <human>: What are the functions of the different isotypes? <bot>: It is expressed exclusively in megakaryocytes and platelets in humans and appears to play an important role in the formation of platelets.\n",
    "```\n",
    "Let's break this down:\n",
    "- The text uses `<human>:` and `<bot>:` to denote the start of the user-provided and generated texts, respectively.\n",
    "- The text uses `Background:` to denote some additional context that is relevant for the chat. Not all samples contain any background.\n",
    "- The dataset contains multi-step chat-style instructions.\n",
    "\n",
    "To fit this in our `FeedbackDataset`, we have to split this up into a \"background\", a \"prompt\" with all chat history and the last prompt, and the final \"response\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def extract_background_prompt_response(text: str) -> Dict[str, Any]:\n",
    "    '''Extract the anthropic prompt from a prompt and response pair.'''\n",
    "    start_prompt = text.find(\"<human>:\")\n",
    "    end_prompt = text.rfind(\"<bot>:\")\n",
    "    # Background is anything before the first <human>:\n",
    "    background = text[:start_prompt].strip()\n",
    "    # Prompt is anything between the first <human>: (inclusive) and the last <bot>: (exclusive)\n",
    "    prompt = text[start_prompt: end_prompt].strip()\n",
    "    # Response is everything after the last <bot>: (inclusive)\n",
    "    response = text[end_prompt:].strip()\n",
    "    return {\"background\": background, \"prompt\": prompt, \"response\": response}\n",
    "\n",
    "\n",
    "data = data.map(extract_background_prompt_response, input_columns=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can trivially convert this dataset into `FeedbackRecord` instances, and add them to the `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    rg.FeedbackRecord(\n",
    "        fields={\n",
    "            \"background\": sample[\"background\"],\n",
    "            \"prompt\": sample[\"prompt\"],\n",
    "            \"response\": sample[\"response\"],\n",
    "        },\n",
    "    )\n",
    "    for sample in data\n",
    "]\n",
    "dataset.add_records(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the records locally, we want to push them to the Argilla server as well. Only then will we be able to see them in the Argilla UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_argilla(\"oig-30k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training-llm-mistral-sft](../../_static/tutorials/training-llm-mistral-sft/training-llm-mistral-sft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Annotate instruction samples\n",
    "If you are using your own proprietary data, data from an unreliable source, or require on your annotators to provide responses, then you must perform data annotation. However, for the purposes of this tutorial, we will assume that all data is high quality and skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the ArgillaTrainer for Supervised Finetuning\n",
    "Next, we have to set up the [ArgillaTrainer](https://docs.argilla.io/en/latest/practical_guides/fine_tune.html#supervised-finetuning). First off, we will load the Mistral 7B `model` and `tokenizer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "class DataCollatorForSeq2SeqCopyLabels(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        for feature in features:\n",
    "            if \"labels\" not in feature:\n",
    "                feature[\"labels\"] = feature[\"input_ids\"].copy()\n",
    "        return super().__call__(features, return_tensors=return_tensors)\n",
    "\n",
    "# model_id = \"gpt2-medium\"\n",
    "# model_id = \"EleutherAI/pythia-1b-deduped\"\n",
    "# model_id = \"PY007/TinyLlama-1.1B-intermediate-step-240k-503b\"\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left', add_eos_token=True)\n",
    "# tokenizer_with_eos = AutoTokenizer.from_pretrained(model_id)#, padding_side='right')\n",
    "# tokenizer_with_eos.pad_token_id = tokenizer_with_eos.eos_token_id\n",
    "# tokenizer_with_eos._tokenizer.post_processor = TemplateProcessing(\n",
    "#     single=f\"$A {tokenizer_with_eos.eos_token}\",\n",
    "#     special_tokens=[(tokenizer_with_eos.eos_token, tokenizer_with_eos.eos_token_id)],\n",
    "# )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "data_collator = DataCollatorForSeq2SeqCopyLabels(tokenizer=tokenizer, model=model)\n",
    "\n",
    "\"\"\"\n",
    "tokenizer_without_eos = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer_without_eos.pad_token_id = tokenizer_without_eos.eos_token_id\n",
    "\"\"\"\n",
    "\n",
    "def formatting_func(sample: Dict[str, Any]) -> Iterator[str]:\n",
    "    if len(sample[\"response\"]) <= 2:\n",
    "        return\n",
    "    prompt = sample[\"prompt\"]\n",
    "    prompt = prompt.replace(\"<human>:\", f\"{tokenizer.eos_token}{tokenizer.bos_token}<human>:\")\n",
    "    prompt = prompt[prompt.find(\"<human>:\"):]\n",
    "    output = prompt + \" \" + sample[\"response\"]\n",
    "    if sample[\"background\"]:\n",
    "        output = sample[\"background\"] + \" \" + output\n",
    "    output = output + \"</s>\"\n",
    "    assert output.count(\"<s>\") + 1 == output.count(\"</s>\")\n",
    "    return output\n",
    "\n",
    "task = TrainingTask.for_supervised_fine_tuning(formatting_func)\n",
    "formatted_dataset = dataset.prepare_for_training(framework=\"trl\", task=task)\n",
    "\n",
    "collated = data_collator([tokenizer(formatted_dataset[3][\"text\"])])\n",
    "print(collated)\n",
    "print(tokenizer.decode(collated[\"input_ids\"][0]))\n",
    "# breakpoint()\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import StoppingCriteria\n",
    "\n",
    "class StoppingCriteriaTokens(StoppingCriteria):\n",
    "    def __init__(self, stopping_input_ids = None):\n",
    "        super().__init__()\n",
    "        self.stopping_input_ids = stopping_input_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> bool:\n",
    "        for stopping_input_ids in self.stopping_input_ids:\n",
    "            length = len(stopping_input_ids)\n",
    "            if input_ids[0, -length:].tolist() == stopping_input_ids:\n",
    "                return True\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaTokens(tokenizer_without_eos([\"<bot>:\", \"<human>:\"]).input_ids)])\n",
    "stopping_criteria\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import TrainerCallback, TrainerControl, TrainerState, GenerationConfig\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "class GenerationCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        model = kwargs.pop(\"model\")\n",
    "        # tokenizer = kwargs.pop(\"tokenizer\")\n",
    "        # template = \"Human: {question}\\n\\nAssistant: \"\n",
    "        # question = \"How do I make a bomb?\"\n",
    "        # prompt = \"<human>: Write me an article about Santiago el Pinar, the 119 Municipalities, Chiapas, discussing in detail Municipalities of Chiapas, Mexico in one paragraph. End with a sentence about Santiago el Pinar. <bot>: Santiago el Pinar is a town and one of the 119 Municipalities of Chiapas, in southern Mexico. As of 2010, the municipality had a total population of 3,245, up from 2,174 as of 2005. It covers an area of 17.76 km². As of 2010, the town of Santiago el Pinar had a population of 1,072. Other than the town of Santiago el Pinar, the municipality had 12 localities, none of which had a population over 1,000. <human>: What is Santiago el Pinar? <bot>: Santiago el Pinar Santiago el Pinar is a town and one of the 119 Municipalities of Chiapas, in southern Mexico. <human>: Is it a growing city? <bot>: As of 2010, the municipality had a total population of 3,245, up from 2,174 as of 2005. It covers an area of 17.76 km². <human>: Is the city a good place to live? <bot>: As of 2010, the town of Santiago el Pinar had a population of 1,072. <human>: continue <bot>:\"\n",
    "        # prompt = \"<human>: Write me an article about Czech immigration, the World War I, discussing in detail around 40,000, Gran La Plata in one paragraph. End with a sentence about Czechs, Buenos Aires. <bot>: Czech immigration to Argentina began during the World War I and was divided in four periods. It is estimated that around 40,000 Czechs arrived in Argentina between then and 1970. Argentina has the largest Czech community. Czechs settled mainly in Buenos Aires, Gran La Plata, Rosario and Chaco. <human>: When did the Czechs settle in Argentina? <bot>: Czechs in Argentina The Czech immigration in Argentina began during the World War 1 and it was split in four periods. <human>: What led to the split? <bot>: It is estimated that around 40,000 Czechs arrived to Argentina until 1970. <human>: continue <bot>: Argentina has the largest Czech community in Latin America. <human>: Where did the majority settle? <bot>: Czechs settled mainly in Buenos Aires, Gran La Plata, Rosario and Chaco. <human>: What period did they settle? <bot>: There are four Czech immigration periods in Argentina. <human>: What led to this? <bot>:\"\n",
    "        # The first one is slightly before to the World War 1, the second one from 1920 to 1930, the third one during the World War 2 and the fourth one, the smallest in migration proportion, during 1990. During the first two periods, the immigration group was mainly integrated by workers and farmers motivated by economic reasons.\n",
    "        # del tokenizer._tokenizer.post_processor\n",
    "        prompt = \"<human>: What are Albert Muwalo's relations with the UDI? <bot>:\"\n",
    "        # For much of the first half of the 1970s, Muwalo was able to use his post as Minister of State in the President's Office and his MCP positions to control access to Banda and manipulate the information that Banda received, particularly on the repression that Muwalo and Gwede controlled. \n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
    "        # print(\"Inputs:\", inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                generation_config=GenerationConfig(\n",
    "                    max_new_tokens=50,\n",
    "                    # early_stopping=True,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    # temperature=0.7,\n",
    "                    # repetition_penalty=1.2,\n",
    "                    # do_sample=True,\n",
    "                ),\n",
    "                # stopping_criteria=stopping_criteria,\n",
    "            )\n",
    "            print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0])\n",
    "        return super().on_evaluate(args, state, control, **kwargs)\n",
    "\n",
    "gen_callback = GenerationCallback()\n",
    "trainer = ArgillaTrainer(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=task,\n",
    "    framework=\"trl\",\n",
    "    train_size=0.99,\n",
    ")\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16, # 32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "trainer.update_config(\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    max_seq_length=1024,\n",
    "    eval_accumulation_steps=16,\n",
    "    max_steps=3000, # <- Just do 3k samples\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    # learning_rate=1e-4,\n",
    "    save_strategy=\"no\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    # optim=\"paged_adamw_32bit\",\n",
    "    remove_unused_columns=False,\n",
    "    # run_name=\"dwight_schrute\",\n",
    "    fp16=True,\n",
    "    num_train_epochs=1,\n",
    "    data_collator=data_collator,\n",
    "    peft_config=peft_config,\n",
    "    callbacks=[gen_callback],\n",
    "    # deepspeed=\"deepspeed_zero_3.json\",\n",
    ")\n",
    "trainer.train(\"Mistral-7B-v0.1-chat-OIG-extra-bos-eos-3k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "\n",
    "peft_model_id = \"Mistral-7B-v0.1-chat-OIG-extra-bos-eos-3k\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    #load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model.eval()\n",
    "\n",
    "\"\"\"\n",
    "tokenizer_without_eos = AutoTokenizer.from_pretrained(config.base_model_name_or_path, add_space_prefix=True)\n",
    "\n",
    "\n",
    "class StoppingCriteriaTokens(StoppingCriteria):\n",
    "    def __init__(self, stopping_input_ids = None):\n",
    "        super().__init__()\n",
    "        self.stopping_input_ids = stopping_input_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> bool:\n",
    "        for stopping_input_ids in self.stopping_input_ids:\n",
    "            length = len(stopping_input_ids)\n",
    "            # print(input_ids)\n",
    "            # breakpoint()\n",
    "            if input_ids[0, -length:].tolist() == stopping_input_ids:\n",
    "                return True\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaTokens(tokenizer_without_eos([\"bot>:\", \"human>:\"], add_special_tokens=False, return_tensors=None).input_ids)])\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaTokens([[10093, 9670], [2930, 9670]])])\n",
    "\"\"\"\n",
    "# stopping_criteria\n",
    "\n",
    "# inputs = tokenizer(\"<human>: Provide an article about This Earth, Pramoedya Ananta Toer's epic quartet, Hasta Mitra, discussing in detail Pramoedya Ananta Toer's, Tirto Adhi Soerjo in three paragraphs. <bot>:\", return_tensors=\"pt\")\n",
    "# inputs = tokenizer(\"<human>: What theatre director Rick Lombardo directed? <bot>:\", return_tensors=\"pt\")\n",
    "# text = \"<human>: Please continue this list of colors: red, orange, yellow, ... <bot>:\"\n",
    "# text = \"<human>: Please continue this list of colors: red, orange, yellow, ... <bot>: green, blue, indigo, violet</s><s><human>: Now write them all in reverse order. <bot>:\"\n",
    "# text = \"<human>: Please continue this list of colors: red, orange, yellow, ... <bot>: green, blue, indigo, violet</s><s><human>: Now write them all in reverse order. <bot>: violet, indigo, blue, green</s><s><human>: Include the original colors please.\"\n",
    "# text = \"<human>: Please continue this list of colors: red, orange, yellow, ... <bot>: green, blue, indigo, violet</s><s> <human>: Now write them all in reverse order. <bot>: violet, indigo, blue, green</s><s> <human>: Include the original colors please. <bot>: red, orange, yellow, green, blue, indigo, violet</s><s><human> But then reverse those <bot>:\"\n",
    "# <s> <human>: Please continue this list of colors: red, orange, yellow, ... <bot>: green, blue, indigo, violet</s><s>  <human>: Now write them all in reverse order. <bot>: violet, indigo, blue, green</s><s>  <human>: Include the original colors please. <bot>: red, orange, yellow, green, blue, indigo, violet</s><s> <human> But then reverse those <bot>: violet, indigo, blue, green, yellow, orange, red</s>\n",
    "\n",
    "text = \"<human>: Hello! <bot>: Hi!</s><s><human>: How are you? <bot>:\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=1000)#, stopping_criteria=stopping_criteria)\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=False)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import argilla as rg\n",
    "\n",
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "rg.init(\n",
    "    api_url=\"https://tomaarsen-argilla.hf.space\",\n",
    "    api_key=\"tomtomtom\",\n",
    "    workspace=\"admin\"\n",
    ")\n",
    "\n",
    "peft_model_id = \"Mistral-7B-v0.1-chat-OIG-extra-bos-eos-3k\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    #load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "# model.eval()\n",
    "\n",
    "model_id = \"argilla/Mistral-7B-v0.1-chat-OIG\"\n",
    "model.push_to_hub(model_id, private=True)\n",
    "tokenizer.push_to_hub(model_id)\n",
    "\n",
    "dataset = rg.FeedbackDataset.from_argilla(\"oig-30k\", workspace=\"admin\")\n",
    "dataset.push_to_huggingface(\"argilla/oig-30k\", private=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
