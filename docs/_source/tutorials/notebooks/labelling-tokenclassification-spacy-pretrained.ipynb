{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí´  Explore and analyze `spaCy` NER predictions\n",
    "\n",
    "In this tutorial, we will learn to log [spaCy](https://spacy.io/) Name Entity Recognition (NER) predictions. \n",
    "\n",
    "This is useful for: \n",
    "\n",
    "- üßêEvaluating pre-trained models.\n",
    "- üîéSpotting frequent errors both during development and production. \n",
    "- üìàImproving your pipelines over time using Argilla annotation mode.\n",
    "- üéÆMonitoring your model predictions using Argilla integration with Kibana\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "\n",
    "![labelling-tokenclassification-spacy-pretrained](../../_static/tutorials/labelling-tokenclassification-spacy-pretrained/labelling-tokenclassification-spacy-pretrained.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will learn how to explore and analyze spaCy NER pipelines in an easy way.\n",
    "\n",
    "We will load the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub and use a transformer-based spaCy model for detecting entities in this dataset and log the detected entities into an Argilla dataset. This dataset can be used for exploring the quality of predictions and for creating a new training set, by correcting, adding and validating entities.\n",
    "\n",
    "Then, we will use a smaller spaCy model for detecting entities and log the detected entities into the same Argilla dataset to compare its predictions with the previous model. And, as a bonus, we will use Argilla and spaCy on a more challenging dataset: IMDB."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Argilla\n",
    "\n",
    "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
    "\n",
    "\n",
    "**Deploy Argilla on Hugging Face Spaces**: If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:\n",
    "\n",
    "[![deploy on spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/new-space?template=argilla/argilla-template-space)\n",
    "\n",
    "For details about configuring your deployment, check the [official Hugging Face Hub guide](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla).\n",
    "\n",
    "\n",
    "**Launch Argilla using Argilla's quickstart Docker image**: This is the recommended option if you want [Argilla running on your local machine](../../getting_started/quickstart.ipynb). Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
    "\n",
    "For more information on deployment options, please check the Deployment section of the documentation.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Tip\n",
    "    \n",
    "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
    "\n",
    "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
    "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter Notebook tool of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install argilla -qqq\n",
    "%pip install torch -qqq\n",
    "%pip install datasets \"spacy[transformers]~=3.0\" protobuf -qqq \n",
    "!python -m spacy download en_core_web_trf\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the Argilla module for reading and writing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the `URL` and `API_KEY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "# Replace workspace with the name of your workspace\n",
    "rg.init(\n",
    "    api_url=\"http://localhost:6900\", \n",
    "    api_key=\"owner.apikey\",\n",
    "    workspace=\"admin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running a private Hugging Face Space, you will also need to set the [HF_TOKEN](https://huggingface.co/settings/tokens) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the HF_TOKEN environment variable\n",
    "# import os\n",
    "# os.environ['HF_TOKEN'] = \"your-hf-token\"\n",
    "\n",
    "# # Replace api_url with the url to your HF Spaces URL\n",
    "# # Replace api_key if you configured a custom API key\n",
    "# # Replace workspace with the name of your workspace\n",
    "# rg.init(\n",
    "#     api_url=\"https://[your-owner-name]-[your_space_name].hf.space\", \n",
    "#     api_key=\"owner.apikey\",\n",
    "#     workspace=\"admin\",\n",
    "#     extra_headers={\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"},\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's include the imports we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Telemetry\n",
    "\n",
    "We gain valuable insights from how you interact with our tutorials. To improve ourselves in offering you the most suitable content, using the following lines of code will help us understand that this tutorial is serving you effectively. Though this is entirely anonymous, you can choose to skip this step if you prefer. For more info, please check out the [Telemetry](../../reference/telemetry.md) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from argilla.utils.telemetry import tutorial_running\n",
    "    tutorial_running()\n",
    "except ImportError:\n",
    "    print(\"Telemetry is introduced in Argilla 1.20.0 and not found in the current installation. Skipping telemetry.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "If you want to skip running the spaCy pipelines, you can also load the resulting Argilla records directly from the [Hugging Face Hub](https://huggingface.co/datasets), and continue the tutorial logging them to the Argilla web app.\n",
    "For example:\n",
    "    \n",
    "```python\n",
    "records = rg.read_datasets(\n",
    "    load_dataset(\"argilla/gutenberg_spacy_ner\", split=\"train\"),\n",
    "    task=\"TokenClassification\",\n",
    ")\n",
    "```\n",
    "\n",
    "The Argilla records of this tutorial are available under the names _\"argilla/gutenberg_spacy_ner\"_ and _\"argilla/imdb_spacy_ner\"_.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dataset\n",
    "For this tutorial, we're going to use the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub. It contains all explicit time references in a dataset of 52,183 novels whose full text is available via Project Gutenberg. From extracts of novels, we are surely going to find some NER entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guten_id</th>\n",
       "      <th>hour_reference</th>\n",
       "      <th>time_phrase</th>\n",
       "      <th>is_ambiguous</th>\n",
       "      <th>time_pos_start</th>\n",
       "      <th>time_pos_end</th>\n",
       "      <th>tok_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4447</td>\n",
       "      <td>5</td>\n",
       "      <td>five o'clock</td>\n",
       "      <td>True</td>\n",
       "      <td>145</td>\n",
       "      <td>147</td>\n",
       "      <td>I crossed the ground she had traversed , notin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4447</td>\n",
       "      <td>12</td>\n",
       "      <td>the fall of the winter noon</td>\n",
       "      <td>True</td>\n",
       "      <td>68</td>\n",
       "      <td>74</td>\n",
       "      <td>So profoundly penetrated with thoughtfulness w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28999</td>\n",
       "      <td>12</td>\n",
       "      <td>midday</td>\n",
       "      <td>True</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>And here is Hendon , and it is time for us to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28999</td>\n",
       "      <td>12</td>\n",
       "      <td>midday</td>\n",
       "      <td>True</td>\n",
       "      <td>133</td>\n",
       "      <td>134</td>\n",
       "      <td>Sorrows and trials she had had in plenty in he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28999</td>\n",
       "      <td>0</td>\n",
       "      <td>midnight</td>\n",
       "      <td>True</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>Jeannie joined her friend in the window-seat ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  guten_id hour_reference                  time_phrase  is_ambiguous  \\\n",
       "0     4447              5                 five o'clock          True   \n",
       "1     4447             12  the fall of the winter noon          True   \n",
       "2    28999             12                       midday          True   \n",
       "3    28999             12                       midday          True   \n",
       "4    28999              0                     midnight          True   \n",
       "\n",
       "   time_pos_start  time_pos_end  \\\n",
       "0             145           147   \n",
       "1              68            74   \n",
       "2              46            47   \n",
       "3             133           134   \n",
       "4              43            44   \n",
       "\n",
       "                                         tok_context  \n",
       "0  I crossed the ground she had traversed , notin...  \n",
       "1  So profoundly penetrated with thoughtfulness w...  \n",
       "2  And here is Hendon , and it is time for us to ...  \n",
       "3  Sorrows and trials she had had in plenty in he...  \n",
       "4  Jeannie joined her friend in the window-seat ....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"gutenberg_time\", split=\"train\", streaming=True)\n",
    "\n",
    "# Let's have a look at the first 5 examples of the train set. \n",
    "pd.DataFrame(dataset.take(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging spaCy NER entities into Argilla\n",
    "\n",
    "\n",
    "Let's instantiate a spaCy transformer `nlp` pipeline and apply it to the first 50 examples in our dataset, collecting the *tokens* and *NER entities*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Creating an empty record list to save all the records\n",
    "records = []\n",
    "\n",
    "# Iterate over the first 50 examples of the Gutenberg dataset\n",
    "for record in tqdm(list(dataset.take(50))):\n",
    "    # We only need the text of each instance\n",
    "    text = record[\"tok_context\"]\n",
    "\n",
    "    # spaCy Doc creation\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Entity annotations\n",
    "    entities = [(ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "\n",
    "    # Pre-tokenized input text\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    # Argilla TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=entities,\n",
    "            prediction_agent=\"en_core_web_trf\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "rg.log(records=records, name=\"gutenberg_spacy_ner\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to the `gutenberg_spacy_ner` dataset in Argilla you can explore the predictions of this model.\n",
    "\n",
    "You can:\n",
    "\n",
    "- Filter records containing specific entity types,\n",
    "- See the most frequent \"mentions\" or surface forms for each entity. Mentions are the string values of specific entity types, for example, \"1 month\" can be the mention of a duration entity. This is useful for error analysis, to quickly see potential issues and problematic entity types,\n",
    "- Use the free-text search to find records containing specific words, \n",
    "- And validate, include or reject specific entity annotations to build a new training set.\n",
    "\n",
    "Now let's compare with a smaller, but more efficient pre-trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Creating an empty record list to save all the records\n",
    "records = []\n",
    "\n",
    "# Iterate over 10000 examples of the Gutenberg dataset\n",
    "for record in tqdm(list(dataset.take(10000))):\n",
    "    # We only need the text of each instance\n",
    "    text = record[\"tok_context\"]\n",
    "\n",
    "    # spaCy Doc creation\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Entity annotations\n",
    "    entities = [(ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "\n",
    "    # Pre-tokenized input text\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    # Argilla TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=entities,\n",
    "            prediction_agent=\"en_core_web_sm\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "rg.log(records=records, name=\"gutenberg_spacy_ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring and comparing `en_core_web_sm` and `en_core_web_trf` models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to your `gutenberg_spacy_ner` dataset, you can explore and compare the results of both models. \n",
    "\n",
    "To only see predictions of a specific model, you can use the `predicted by` filter, which comes from the `prediction_agent` parameter of your `TextClassificationRecord`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, both **spaCy pre-trained models** seem to work pretty well. Let's try with a more challenging dataset, which is more dissimilar to the original training data these models have been trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "records = []\n",
    "for record in tqdm(imdb.select(range(5000))):\n",
    "    # We only need the text of each instance\n",
    "    text = record[\"text\"]\n",
    "\n",
    "    # spaCy Doc creation\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Entity annotations\n",
    "    entities = [(ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "\n",
    "    # Pre-tokenized input text\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    # Argilla TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=entities,\n",
    "            prediction_agent=\"en_core_web_sm\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "rg.log(records=records, name=\"imdb_spacy_ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring this dataset highlights **the need of fine-tuning for specific domains**.\n",
    "\n",
    "For example, if we check the most frequent mentions for Person, we find two highly frequent misclassified entities: **gore** (the film genre) and **Oscar** (the prize). \n",
    "\n",
    "You can easily check every example by using the filters and search box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this tutorial, you learned how to log and explore different `spaCy` NER models with Argilla. Now you can:\n",
    "\n",
    "- Build custom dashboards using Kibana to monitor and visualize spaCy models.\n",
    "- Build training sets using pre-trained spaCy models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Log datasets to the Hugging Face Hub\n",
    "\n",
    "Here we will show you an example of how you can push an Argilla dataset (records) to the [Hugging Face Hub](https://huggingface.co/datasets).\n",
    "In this way, you can effectively version any of your Argilla datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = rg.load(\"gutenberg_spacy_ner\")\n",
    "records.to_datasets().push_to_hub(\"<name of the dataset on the HF Hub>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Dec 17 2020, 03:56:09) \n[Clang 11.0.0 (clang-1100.0.33.17)]"
  },
  "metadata": {
   "interpreter": {
    "hash": "0f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "8874e298d2bce9702a08b32d5709c9f02d53b2045f1d246836c6e4c8123e6782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
