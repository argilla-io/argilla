{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nn8y7M13ulli",
   "metadata": {
    "id": "nn8y7M13ulli"
   },
   "source": [
    "# üöÄ Run Argilla with a Transformer in an active learning loop and a free GPU in your browser\n",
    "In this tutorial, you will learn how to set up a complete active learning loop with Google Colab with a GPU in the backend. This tutorial is based on the [small-text active learning tutorial](https://docs.v1.argilla.io/en/latest/tutorials/notebooks/training-textclassification-smalltext-activelearning.html). The main difference is that this tutorial is designed to be run in a Google Colab notebook with a GPU as the backend for a more efficient active learning loop with Transformer models. It is recommended to follow this tutorial directly on Google Colab. You can [open the Colab notebook via this hyperlink](https://colab.research.google.com/drive/11oTWno3hzgJnip11EcgqEhdpbW1IX-lP?usp=sharing), create your own copy and modify it for your own use-cases. \n",
    "\n",
    "‚ö†Ô∏è Note that this notebook requires manual input to start Argilla in a terminal and to input an ngrok token. Please read the instructions for each cell. If you do not follow the instructions and execute everything in the correct order, the code will bug. If you face an error, restarting your runtime can solve several issues. ‚ö†Ô∏è\n",
    "\n",
    "üôãüèº‚Äç‚ôÇÔ∏è The notebook was contributed by [Moritz Laurer](https://www.linkedin.com/in/moritz-laurer/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6X__c767GiyM",
   "metadata": {
    "id": "6X__c767GiyM"
   },
   "source": [
    "## Initial setup on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qeQzO3H1GlkS",
   "metadata": {
    "id": "qeQzO3H1GlkS"
   },
   "source": [
    "In the Colab interface, you can choose a CPU (for initial testing) or a GPU (for an efficient active learning loop) by clicking Runtime > Change runtime type > Hardware accelerator in the menu in the top left. Once you have chosen your hardware, install the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837556c3",
   "metadata": {
    "id": "837556c3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install \"argilla[server, listeners]==1.16.0\"\n",
    "%pip install \"transformers[sentencepiece]~=4.25.1\"\n",
    "%pip install \"datasets~=2.7.1\"\n",
    "%pip install \"small-text[transformers]~=1.3.2\"\n",
    "%pip install \"colab-xterm~=0.1.2\"\n",
    "%pip install \"pyngrok~=5.2.1\"\n",
    "%pip install \"colab-xterm~=0.1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7doWoB1HrM2",
   "metadata": {
    "id": "s7doWoB1HrM2"
   },
   "outputs": [],
   "source": [
    "# info on the hardware you are using - either a CPU or GPU\n",
    "!nvidia-smi\n",
    "# info on available ram\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('\\n\\nYour runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Telemetry\n",
    "\n",
    "We gain valuable insights from how you interact with our tutorials. To improve ourselves in offering you the most suitable content, using the following lines of code will help us understand that this tutorial is serving you effectively. Though this is entirely anonymous, you can choose to skip this step if you prefer. For more info, please check out the [Telemetry](../../reference/telemetry.md) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from argilla.utils.telemetry import tutorial_running\n",
    "    tutorial_running()\n",
    "except ImportError:\n",
    "    print(\"Telemetry is introduced in Argilla 1.20.0 and not found in the current installation. Skipping telemetry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qq_uSvy_BUf-",
   "metadata": {
    "id": "Qq_uSvy_BUf-"
   },
   "source": [
    "## Install Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_u7gFMiTBYTs",
   "metadata": {
    "id": "_u7gFMiTBYTs"
   },
   "source": [
    "Elastic Search is a requirement for using Argilla. The [docker installation](https://docs.v1.argilla.io/en/latest/getting_started/quickstart.html) of Elastic Search recommended by Argilla does not work in Google Colab as [Colab does not support docker](https://github.com/googlecolab/colabtools/issues/299). Elastic Search therefore needs to be installed 'manually' with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.2-linux-x86_64.tar.gz -q\n",
    "tar -xzf elasticsearch-7.10.2-linux-x86_64.tar.gz\n",
    "chown -R daemon:daemon elasticsearch-7.10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aed8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "\n",
    "sudo -u daemon -- elasticsearch-7.10.2/bin/elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(30)  # sleeping to give ES time to set up. Otherwise downstream code will bug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S1VThY6kCcu7",
   "metadata": {
    "id": "S1VThY6kCcu7"
   },
   "source": [
    "## Start the Argilla localhost in a terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "axtfY-QyLnCW",
   "metadata": {
    "id": "axtfY-QyLnCW"
   },
   "source": [
    "You now need to start Argilla localhost in a separate terminal. We cannot just run `!argilla server start` in a code cell on Colab, because the cell will run indefinitely and block us from running other cells. We therefore need to open a separate terminal to run Argilla.\n",
    "\n",
    "1. Option with Colab Pro: Open the Colab Pro terminal (button to the bottom left) and type in the terminal: `argilla server start`\n",
    "2. Option without Colab Pro: Run the following code cell to get a free terminal window in the code cell with [xterm](https://github.com/InfuseAI/colab-xterm). Then type `argilla server start` in the terminal window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnNg9Nh4DbHZ",
   "metadata": {
    "id": "qnNg9Nh4DbHZ"
   },
   "outputs": [],
   "source": [
    "# create a terminal to run Argilla with, in case you don't have Colab Pro.\n",
    "# type \"argilla server start\" into the terminal that appears below this code cell.\n",
    "%load_ext colabxterm\n",
    "%xterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "us9JDMaVJCYs",
   "metadata": {
    "id": "us9JDMaVJCYs"
   },
   "source": [
    "The terminal window above should now display something like: \n",
    "\n",
    "\"...\n",
    "INFO:     Application startup complete.\n",
    "\n",
    "INFO:     Uvicorn running on http://0.0.0.0:6900 (Press CTRL+C to quit)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VJtKSI3ymGWX",
   "metadata": {
    "id": "VJtKSI3ymGWX"
   },
   "source": [
    "## Create a public link to Argilla localhost with ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xHPwq1q9ffDN",
   "metadata": {
    "id": "xHPwq1q9ffDN"
   },
   "source": [
    "We now have some virtual machine from Google running an Argilla localhost, but we cannot access it yet. [ngrok](https://ngrok.com/) is a service designed to create public links to a localhost. We can therefore use ngrok to create a public link to access the Argilla localhost running on the Google machine. Note that anyone with this (temporary) public link can access the (temporary) localhost. In order to use ngrok, you need to create a free account. Creating a free account only takes a minute following the [instructions here](https://ngrok.com/). With the free account, you receive an access token. Once you have your access token, you can run the following cell and copy the token into the input prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64NUr5xl_g42",
   "metadata": {
    "id": "64NUr5xl_g42"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\n",
    "print(\"You need to create a free ngrok account to get an authtoken. The token looks something like this: ASDO1283YZaDu95vysXYIUXZXYRR_54YfASDIb8cpNfVoz349587\")\n",
    "conf.get_default().auth_token = getpass.getpass()\n",
    "# if the above does not work, you can try: \n",
    "#ngrok.set_auth_token(\"<INSER_YOUR_NGROK_AUTHTOKEN>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26-V4pI16m8x",
   "metadata": {
    "id": "26-V4pI16m8x"
   },
   "outputs": [],
   "source": [
    "# disconnect all existing tunnels to avoid issues when rerunning cells\n",
    "[ngrok.disconnect(tunnel.public_url) for tunnel in ngrok.get_tunnels()]\n",
    "\n",
    "# create the public link\n",
    "# ! check whether this is actually the localhost port Argilla is running on via the terminal above \n",
    "ngrok_tunnel = ngrok.connect(6900)  # insert the port number Argilla is running on. e.g. 6900 if the terminal displays something like \"Uvicorn running on http://0.0.0.0:6900\"\n",
    "print(\"You can now access the Argilla localhost with the public link below. (It should look something like 'http://X03b-34-XXX-237-25.ngrok.io')\\n\")\n",
    "print(f\"Your ngrok public link: {ngrok_tunnel}\\n\")\n",
    "print(\"After clicking on the link, there will be a warning, which you can ignore\")\n",
    "print(\"You can then login with the default argilla username 'argilla' and password '1234'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NSN0aJOFKNhe",
   "metadata": {
    "id": "NSN0aJOFKNhe"
   },
   "source": [
    "## Log data to argilla and start your active learning loop with small-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EObyEHqxhes8",
   "metadata": {
    "id": "EObyEHqxhes8"
   },
   "source": [
    "If you click on your public link above, you should be able to access Argilla, but there is no data logged to Argilla yet. The following code downloads an example dataset and logs it to Argilla. You can change the following code to download any other dataset you want to annotate. The following code follows the [active learning with small-text](https://docs.v1.argilla.io/en/latest/tutorials/notebooks/training-textclassification-smalltext-activelearning.html) tutorial and therefore contains fewer explanations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eSHX6xa0V14u",
   "metadata": {
    "id": "eSHX6xa0V14u"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import datasets\n",
    "dataset_name = \"trec\"\n",
    "dataset_hf = datasets.load_dataset(dataset_name, version=datasets.Version(\"2.0.0\"))\n",
    "# we work with only a sixth of the texts of the dataset for faster testing\n",
    "dataset_hf[\"train\"] = dataset_hf[\"train\"].shard(num_shards=6, index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "myAyMzbQWFSN",
   "metadata": {
    "id": "myAyMzbQWFSN"
   },
   "outputs": [],
   "source": [
    "## choose the transformer and load tokenizer\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Choose transformer model: In non-gpu environments we use a tiny model to increase efficiency\n",
    "if not torch.cuda.is_available():\n",
    "    transformer_model = \"prajjwal1/bert-tiny\"\n",
    "    print(f\"No GPU is available, we therefore use the small model '{transformer_model}' for the active learning loop.\\n\")\n",
    "else:\n",
    "    transformer_model = \"microsoft/deberta-v3-xsmall\"  #\"bert-base-uncased\"\n",
    "    print(f\"A GPU is available, we can therefore use '{transformer_model}' for the active learning loop.\\n\")\n",
    "\n",
    "# Init tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dU5MnQ5MWvn8",
   "metadata": {
    "id": "dU5MnQ5MWvn8"
   },
   "outputs": [],
   "source": [
    "## create small_text transformersdataset object\n",
    "import numpy as np\n",
    "from small_text import TransformersDataset\n",
    "\n",
    "num_classes = dataset_hf[\"train\"].features[\"coarse_label\"].num_classes\n",
    "target_labels = np.arange(num_classes)\n",
    "\n",
    "train_text = [row[\"text\"] for row in dataset_hf[\"train\"]]\n",
    "train_labels = np.array([row[\"coarse_label\"] for row in dataset_hf[\"train\"]])\n",
    "\n",
    "# Create the dataset for small-text\n",
    "dataset_st = TransformersDataset.from_arrays(\n",
    "    train_text, train_labels, tokenizer, target_labels=target_labels\n",
    ")\n",
    "\n",
    "# Create test dataset\n",
    "test_text = [row[\"text\"] for row in dataset_hf[\"test\"]]\n",
    "test_labels = np.array([row[\"coarse_label\"] for row in dataset_hf[\"test\"]])\n",
    "\n",
    "dataset_test = TransformersDataset.from_arrays(\n",
    "    test_text, test_labels, tokenizer, target_labels=np.arange(num_classes)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k-oMMHmuXZUN",
   "metadata": {
    "id": "k-oMMHmuXZUN"
   },
   "outputs": [],
   "source": [
    "## setting up the active learner\n",
    "from small_text import (\n",
    "    BreakingTies,\n",
    "    PoolBasedActiveLearner,\n",
    "    TransformerBasedClassificationFactory,\n",
    "    TransformerModelArguments,\n",
    ")\n",
    "\n",
    "# Define our classifier\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "num_epochs = 5  # higher values of around 40 will probably improve performance on small datasets, but the active learning loop will take longer\n",
    "clf_factory = TransformerBasedClassificationFactory(\n",
    "    TransformerModelArguments(transformer_model),\n",
    "    num_classes=num_classes,\n",
    "    kwargs={\"device\": device, \"num_epochs\": num_epochs, \"lr\": 2e-05, \"mini_batch_size\": 8,\n",
    "            \"early_stopping_no_improvement\": 5}\n",
    ")\n",
    "\n",
    "\n",
    "# Define our query strategy\n",
    "query_strategy = BreakingTies()\n",
    "\n",
    "# Use the active learner with a pool containing all unlabeled data\n",
    "active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, dataset_st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P24VwkAYX4Aj",
   "metadata": {
    "id": "P24VwkAYX4Aj"
   },
   "outputs": [],
   "source": [
    "## draw an initial sample for the first annotation round\n",
    "# https://small-text.readthedocs.io/en/v1.1.1/components/initialization.html\n",
    "from small_text import random_initialization, random_initialization_stratified, random_initialization_balanced\n",
    "import numpy as np\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples in our queried batches\n",
    "NUM_SAMPLES = 10\n",
    "\n",
    "# Draw an initial subset from the data pool\n",
    "#initial_indices = random_initialization(dataset_st, NUM_SAMPLES)\n",
    "#initial_indices = random_initialization_balanced(train_labels, NUM_SAMPLES)\n",
    "initial_indices = random_initialization_stratified(train_labels, NUM_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jnJcjaZqYIw3",
   "metadata": {
    "id": "jnJcjaZqYIw3"
   },
   "outputs": [],
   "source": [
    "### log the first data to Argilla\n",
    "import argilla as rg\n",
    "\n",
    "# Choose a name for the dataset\n",
    "DATASET_NAME = f\"{dataset_name}-with-active-learning\"\n",
    "\n",
    "# Define labeling schema\n",
    "labels = dataset_hf[\"train\"].features[\"coarse_label\"].names\n",
    "settings = rg.TextClassificationSettings(label_schema=labels)\n",
    "\n",
    "# Create dataset with a label schema\n",
    "rg.configure_dataset_settings(name=DATASET_NAME, settings=settings)\n",
    "\n",
    "# Create records from the initial batch\n",
    "records = [\n",
    "    rg.TextClassificationRecord(\n",
    "        text=dataset_hf[\"train\"][\"text\"][idx],\n",
    "        metadata={\"batch_id\": 0},\n",
    "        id=idx.item(),\n",
    "    )\n",
    "    for idx in initial_indices\n",
    "]\n",
    "\n",
    "# Log initial records to Argilla\n",
    "rg.log(records, DATASET_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SSl0OEteOJT3",
   "metadata": {
    "id": "SSl0OEteOJT3"
   },
   "outputs": [],
   "source": [
    "### create active learning loop\n",
    "from argilla.listeners import listener\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define some helper variables\n",
    "LABEL2INT = dataset_hf[\"train\"].features[\"coarse_label\"].str2int\n",
    "ACCURACIES = []\n",
    "\n",
    "# Set up the active learning loop with the listener decorator\n",
    "@listener(\n",
    "    dataset=DATASET_NAME,\n",
    "    query=\"status:Validated AND metadata.batch_id:{batch_id}\",\n",
    "    condition=lambda search: search.total == NUM_SAMPLES,\n",
    "    execution_interval_in_seconds=3,\n",
    "    batch_id=0,\n",
    ")\n",
    "def active_learning_loop(records, ctx):\n",
    "    # 1. Update active learner\n",
    "    print(f\"Updating with batch_id {ctx.query_params['batch_id']} ...\")\n",
    "    y = np.array([LABEL2INT(rec.annotation) for rec in records])\n",
    "\n",
    "    # initial update\n",
    "    if ctx.query_params[\"batch_id\"] == 0:\n",
    "        indices = np.array([rec.id for rec in records])\n",
    "        active_learner.initialize_data(indices, y)\n",
    "    # update with the prior queried indices\n",
    "    else:\n",
    "        active_learner.update(y)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    # 2. Query active learner\n",
    "    print(\"Querying new data points ...\")\n",
    "    queried_indices = active_learner.query(num_samples=NUM_SAMPLES)\n",
    "    new_batch = ctx.query_params[\"batch_id\"] + 1\n",
    "    new_records = [\n",
    "        rg.TextClassificationRecord(\n",
    "            text=dataset_hf[\"train\"][\"text\"][idx],\n",
    "            metadata={\"batch_id\": new_batch},\n",
    "            id=idx.item(),\n",
    "        )\n",
    "        for idx in queried_indices\n",
    "    ]\n",
    "\n",
    "    # 3. Log the batch to Argilla\n",
    "    rg.log(new_records, DATASET_NAME)\n",
    "\n",
    "    # 4. Evaluate current classifier on the test set\n",
    "    print(\"Evaluating current classifier ...\")\n",
    "    accuracy = accuracy_score(\n",
    "        dataset_test.y,\n",
    "        active_learner.classifier.predict(dataset_test),\n",
    "    )\n",
    "\n",
    "    ACCURACIES.append(accuracy)\n",
    "    ctx.query_params[\"batch_id\"] = new_batch\n",
    "    print(\"Done!\")\n",
    "\n",
    "    print(\"Waiting for annotations ...\")\n",
    "\n",
    "\n",
    "\n",
    "active_learning_loop.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7lwpDaQUi8Xj",
   "metadata": {
    "id": "7lwpDaQUi8Xj"
   },
   "source": [
    "## Start annotating in the browser via the ngrok link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lUHaeU06i3hX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUHaeU06i3hX",
    "outputId": "8d61e613-1b8b-42e8-ad91-6619a0f0ff0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can now start annotating with an active learning in the background!\n",
      "The public link for accessing the annotation interface is: NgrokTunnel: \"http://30b0-34-124-178-185.ngrok.io\" -> \"http://localhost:6900\"\n"
     ]
    }
   ],
   "source": [
    "print(f\"You can now start annotating with active learning in the background!\")\n",
    "print(f\"The public link for accessing the annotation interface is: {ngrok_tunnel}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3yGijvInkOJ7",
   "metadata": {
    "id": "3yGijvInkOJ7"
   },
   "source": [
    "After each iteration of 10 new annotated texts, the active learner will be re-trained and recommend a new batch of 10 texts. So you need to manually annotate exactly 10 texts to get new texts.\n",
    "\n",
    "‚ö†Ô∏è Note that it will take a while until the active learner has been re-trained and analyed all remaining data to recommend new data. This probably takes several minutes. Refresh the Argilla window after a few minutes and a new batch of 10 texts should automatically appear in the interface. If it does not work immediately, double-check if you really annotated all 10 new texts and wait a bit longer.  ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OadYmKZrcwM9",
   "metadata": {
    "id": "OadYmKZrcwM9"
   },
   "outputs": [],
   "source": [
    "# when you are done, stop active learning loop\n",
    "active_learning_loop.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sTwVoR9mYtEn",
   "metadata": {
    "id": "sTwVoR9mYtEn"
   },
   "outputs": [],
   "source": [
    "# plot learning progress over different active learning iterations\n",
    "import pandas as pd\n",
    "pd.Series(ACCURACIES).plot(xlabel=\"Iteration\", ylabel=\"Accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LDN14mL1TtCF",
   "metadata": {
    "id": "LDN14mL1TtCF"
   },
   "source": [
    "## Extract annotated data for downstream use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-aPE3yDPTvuh",
   "metadata": {
    "id": "-aPE3yDPTvuh"
   },
   "outputs": [],
   "source": [
    "## https://docs.v1.argilla.io/en/latest/getting_started/quickstart.html#Manual-extraction\n",
    "\n",
    "# load your annotations\n",
    "dataset_annotated = rg.load(DATASET_NAME)\n",
    "# convert to Hugging Face format\n",
    "dataset_annotated = dataset_annotated.prepare_for_training()\n",
    "# now you can write your annotations to .csv, use them for training etc.\n",
    "df_annotations = pd.DataFrame(dataset_annotated)\n",
    "df_annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dH8ZuaAwPAlG",
   "metadata": {
    "id": "dH8ZuaAwPAlG"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we saw how you could embed Argilla in an active learning loop on a GPU in Google Colab. We relied on small-text to use a Hugging Face transformer within an active learning setup. In the end, we gathered a sample-efficient data set by annotating only the most informative records for the model.\n",
    "\n",
    "Argilla makes it very easy to use a dedicated annotation team or subject matter experts as an oracle for your active learning system. They will only interact with the Argilla UI and do not have to worry about training or querying the system. We encourage you to try out active learning in your next project and make your and your annotator‚Äôs life a little easier."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Dec 17 2020, 03:56:09) \n[Clang 11.0.0 (clang-1100.0.33.17)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c6ac5964d63158aef0c318a650c56c288100fe36867cf6a65be3eefaa97102a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
