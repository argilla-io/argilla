{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💾 Monitor FastAPI predictions\n",
    "\n",
    "\n",
    "In this tutorial, you'll learn to monitor the predictions of a FastAPI inference endpoint\n",
    "and log model predictions in a Argilla dataset. It will walk you through 4 basic MLOps Steps:\n",
    "\n",
    "- 💾 Load the model you want to use.\n",
    "- 🔄 Convert model output to Argilla format.\n",
    "- 💻 Create a FastAPI endpoint.\n",
    "- 🤖 Add middleware to automate logging to Argilla\n",
    "\n",
    "<img src=\"../../_static/tutorials/deploying-texttokenclassification-fastapi/deploying-texttokenclassification-fastapi.png\" alt=\"Transformers Log Demo\" style=\"width: 1100px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Models are often deployed via an HTTP API endpoint that is called by a client to obtain the model's predictions.\n",
    "With [FastAPI](https://fastapi.tiangolo.com/) and *Argilla* you can easily monitor those predictions and log them to a *Argilla* dataset.\n",
    "Due to its human-centric UX, *Argilla* datasets can be comfortably viewed and explored by any team member of your organization.\n",
    "But *Argilla* also provides automatically computed metrics, both of which help you to keep track of your predictor and spot potential issues early on. \n",
    "\n",
    "FastAPI and *Argilla* allow you to deploy and monitor any model you like, but in this tutorial we will focus on the two most common frameworks in the NLP space: [spaCy](https://spacy.io/api/doc) and [transformers](https://huggingface.co/docs/transformers). Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Apart from Argilla, we'll need a few third party libraries that can be installed via pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fastapi uvicorn[standard] spacy transformers[torch] -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading models\n",
    "\n",
    "As a first step, let's load our models.\n",
    "For spacy we need to first download the model before we can instantiate a spacy pipeline with it.\n",
    "Here we use the small English model `en_core_web_sm`, but you can choose any available model on their [hub](https://spacy.io/usage/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_pipeline = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"text-classification\" pipeline by transformers download's the model for you and by default it will use the `distilbert-base-uncased-finetuned-sst-2-english` model.\n",
    "But you can instantiate the pipeline with any compatible model on their [hub](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "transformers_pipeline = pipeline(\"text-classification\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about using the `transformers` library with Argilla, check the tutorial [How to label your data and fine-tune a 🤗 sentiment classifier](01-labeling-finetuning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model output\n",
    "Let's try the transformer's pipeline in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'NEGATIVE', 'score': 0.0003226407279726118},\n",
      "  {'label': 'POSITIVE', 'score': 0.9996774196624756}]]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "batch = ['I really like argilla!']\n",
    "predictions = transformers_pipeline(batch)\n",
    "pprint(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the `predictions` is a list containing lists of two elements : \n",
    "- The first dictionary containing the `NEGATIVE` sentiment label and its score.\n",
    "- The second dictionary containing the same data but for `POSITIVE` sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert output to Argilla format\n",
    "To log the output to Argilla, we should supply a list of dictionaries, each dictionary containing two keys:\n",
    "- `labels` : value is a list of strings, each string being the label of the sentiment.\n",
    "- `scores` : value is a list of floats, each float being the probability of the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'labels': ['NEGATIVE', 'POSITIVE'],\n",
      "  'scores': [0.0003226407279726118, 0.9996774196624756]}]\n"
     ]
    }
   ],
   "source": [
    "argilla_format = [\n",
    "    {\n",
    "        \"labels\": [p[\"label\"] for p in prediction],\n",
    "        \"scores\": [p[\"score\"] for p in prediction],\n",
    "    }\n",
    "    for prediction in predictions\n",
    "]\n",
    "pprint(argilla_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create prediction endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from typing import List\n",
    "\n",
    "app_transformers = FastAPI()\n",
    "\n",
    "# prediction endpoint using transformers pipeline\n",
    "@app_transformers.post(\"/\")\n",
    "def predict_transformers(batch: List[str]):\n",
    "    predictions = transformers_pipeline(batch)\n",
    "    return [\n",
    "        {\n",
    "            \"labels\": [p[\"label\"] for p in prediction],\n",
    "            \"scores\": [p[\"score\"] for p in prediction],\n",
    "        }\n",
    "        for prediction in predictions\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add Argilla logging middleware to the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.monitoring.asgi import ArgillaLogHTTPMiddleware\n",
    "\n",
    "app_transformers.add_middleware(\n",
    "    ArgillaLogHTTPMiddleware,\n",
    "    api_endpoint=\"/transformers/\", #the endpoint that will be logged\n",
    "    dataset=\"monitoring_transformers\", #your dataset name\n",
    "    # you could post-process the predict output with a custom record_mapper function\n",
    "    # record_mapper=custom_text_classification_mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Do the same for spaCy\n",
    "We'll add a custom mapper to convert spaCy's output to `TokenClassificationRecord` format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastAPI application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.monitoring.asgi import ArgillaLogHTTPMiddleware, token_classification_mapper\n",
    "\n",
    "app_spacy = FastAPI()\n",
    "\n",
    "app_spacy.add_middleware(\n",
    "    ArgillaLogHTTPMiddleware,\n",
    "    api_endpoint=\"/spacy/\",\n",
    "    dataset=\"monitoring_spacy\",\n",
    "    records_mapper=token_classification_mapper\n",
    ")\n",
    "\n",
    "# prediction endpoint using spacy pipeline\n",
    "@app_spacy.post(\"/\")\n",
    "def predict_spacy(batch: List[str]):\n",
    "    predictions = []\n",
    "    for text in batch:\n",
    "        doc = spacy_pipeline(text)  # spaCy Doc creation\n",
    "        # Entity annotations\n",
    "        entities = [\n",
    "            {\"label\": ent.label_, \"start\": ent.start_char, \"end\": ent.end_char}\n",
    "            for ent in doc.ents\n",
    "        ]\n",
    "\n",
    "        prediction = {\n",
    "            \"text\": text,\n",
    "            \"entities\": entities,\n",
    "        }\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting it all together\n",
    "\n",
    "Now we can combine everything in order to see our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"alive\"}\n",
    "\n",
    "app.mount(\"/transformers\", app_transformers)\n",
    "app.mount(\"/spacy\", app_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch the application, copy the whole code into a file named `main.py` and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uvicorn main:app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we learned to automatically log model outputs into Argilla.\n",
    "This can be used to continuously and transparently monitor HTTP inference endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "⭐ Argilla [Github repo](https://github.com/argilla-io/argilla) to stay updated.\n",
    "\n",
    "📚 [Argilla documentation](https://docs.argilla.io) for more guides and tutorials.\n",
    "\n",
    "🙋‍♀️ Join the Argilla community! A good place to start is the [discussion forum](https://github.com/argilla-io/argilla/discussions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf5ecc3fecf17575e278cfc1ec050b1bf24f64b662a1b5421daed12bdd8255ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
