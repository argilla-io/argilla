{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BbsELFGS7tQS"
      },
      "source": [
        "# ðŸ’¾ Monitor FastAPI model endpoints\n",
        "\n",
        "\n",
        "In this tutorial, you'll learn to monitor the predictions of a FastAPI inference endpoint\n",
        "and log model predictions in a Argilla dataset. It will walk you through 4 basic MLOps Steps:\n",
        "\n",
        "- ðŸ’¾ Load the model you want to use.\n",
        "- ðŸ”„ Convert model output to Argilla format.\n",
        "- ðŸ’» Create a FastAPI endpoint.\n",
        "- ðŸ¤– Add middleware to automate logging to Argilla\n",
        "\n",
        "<img src=\"https://github.com/argilla-io/argilla/blob/docs/open_in_colab_button/docs/_source/_static/tutorials/deploying-texttokenclassification-fastapi/deploying-texttokenclassification-fastapi.png?raw=1\" alt=\"Transformers Log Demo\" style=\"width: 1100px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvglYNlp7tQW"
      },
      "source": [
        "## Introduction\n",
        "Models are often deployed via an HTTP API endpoint that is called by a client to obtain the model's predictions.\n",
        "With [FastAPI](https://fastapi.tiangolo.com/) and *Argilla* you can easily monitor those predictions and log them to a *Argilla* dataset.\n",
        "Due to its human-centric UX, *Argilla* datasets can be comfortably viewed and explored by any team member of your organization.\n",
        "But *Argilla* also provides automatically computed metrics, both of which help you to keep track of your predictor and spot potential issues early on. \n",
        "\n",
        "FastAPI and *Argilla* allow you to deploy and monitor any model you like, but in this tutorial we will focus on the two most common frameworks in the NLP space: [spaCy](https://spacy.io/api/doc) and [transformers](https://huggingface.co/docs/transformers). Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6ZoiZMP7tQX"
      },
      "source": [
        "## Running Argilla\n",
        "\n",
        "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
        "\n",
        "1. [Deploy Argilla on Hugging Face Spaces](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla): This is the fastest option and the recommended choice for connecting to external notebooks (e.g., Google Colab) if you have an account on Hugging Face.\n",
        "\n",
        "2. [Launch Argilla using Argilla's quickstart Docker image](../../getting_started/quickstart.ipynb): This is the recommended option if you want Argilla running on your local machine. Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
        "\n",
        "For more information on deployment options, please check the Deployment section of the documentation.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "\n",
        "Tip\n",
        "    \n",
        "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
        "\n",
        "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
        "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxyUtKJZ7tQY"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To complete this tutorial, you will need to install the Argilla client and a few third party libraries using `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrWOUy-I7tQZ"
      },
      "outputs": [],
      "source": [
        "%pip install argilla fastapi uvicorn[standard] spacy transformers[torch] -qqq"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's import the Argilla module for reading and writing data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac811da1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import argilla as rg"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "556046ed",
      "metadata": {
        "id": "7TRNourOwigS"
      },
      "source": [
        "If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the `URL` and `API_KEY`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ab3983e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
        "# Replace api_key if you configured a custom API key\n",
        "rg.init(\n",
        "    api_url=\"http://localhost:6900\", \n",
        "    api_key=\"team.apikey\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ccL8UFwj_CaD"
      },
      "source": [
        "Finally, let's include the imports we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "jJj5eh6k_H1o"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "import spacy\n",
        "from transformers import pipeline\n",
        "from typing import List\n",
        "\n",
        "# for adding logging to API endpoints\n",
        "from argilla.monitoring.asgi import (\n",
        "    ArgillaLogHTTPMiddleware, \n",
        "    text_classification_mapper,\n",
        "    token_classification_mapper,\n",
        ")\n",
        "\n",
        "# Instantiate our FastAPI app\n",
        "app = FastAPI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU7tjhXX7tQa"
      },
      "source": [
        "## 1. Loading models\n",
        "\n",
        "As a first step, let's load our models.\n",
        "For spacy we need to first download the model before we can instantiate a spacy pipeline with it.\n",
        "Here we use the small English model `en_core_web_sm`, but you can choose any available model on their [hub](https://spacy.io/usage/models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-HGHvyI7tQa"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkXZ6zZw7tQc"
      },
      "source": [
        "The \"text-classification\" pipeline by transformers download's the model for you and by default it will use the `distilbert-base-uncased-finetuned-sst-2-english` model.\n",
        "But you can instantiate the pipeline with any compatible model on their [hub](https://huggingface.co/models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtec5GJv7tQb",
        "outputId": "1b8ec4c2-7ae3-49cf-f67b-56fdce98ac59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "spacy_pipeline = spacy.load(\"en_core_web_sm\")\n",
        "transformers_pipeline = pipeline(\"text-classification\", return_all_scores=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2IkZPTU7tQc"
      },
      "source": [
        "For more information about using the `transformers` library with Argilla, check the tutorial [How to label your data and fine-tune a ðŸ¤— sentiment classifier](training-textclassification-transformers-pretrained.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndveqstg7tQd"
      },
      "source": [
        "### Model output\n",
        "Let's try the transformer's pipeline in this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4U1QFGP7tQd",
        "outputId": "83f2ee60-101e-4d01-9219-0ae42ba6adf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[{'label': 'NEGATIVE', 'score': 0.0029897126369178295}, {'label': 'POSITIVE', 'score': 0.9970102310180664}]]\n"
          ]
        }
      ],
      "source": [
        "batch = [\"I really like argilla!\"]\n",
        "predictions = transformers_pipeline(batch)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEk2CQEE7tQe"
      },
      "source": [
        "Looks like the `predictions` is a list containing lists of two elements : \n",
        "- The first dictionary containing the `NEGATIVE` sentiment label and its score.\n",
        "- The second dictionary containing the same data but for `POSITIVE` sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k041O1_27tQe"
      },
      "source": [
        "## 2. Convert output to Argilla format\n",
        "To log the output to Argilla, we should supply a list of dictionaries, each dictionary containing two keys:\n",
        "- `labels` : value is a list of strings, each string being the label of the sentiment.\n",
        "- `scores` : value is a list of floats, each float being the probability of the sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzzqNq2g7tQf",
        "outputId": "b2cd8fca-9970-44f1-8b58-46b7400e7619"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'labels': ['NEGATIVE', 'POSITIVE'],\n",
              "  'scores': [0.0029897126369178295, 0.9970102310180664]}]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "argilla_format = [\n",
        "    {\n",
        "        \"labels\": [p[\"label\"] for p in prediction],\n",
        "        \"scores\": [p[\"score\"] for p in prediction],\n",
        "    }\n",
        "    for prediction in predictions\n",
        "]\n",
        "\n",
        "argilla_format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3Ez4Yso7tQf"
      },
      "source": [
        "## 3. Create prediction endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "T5tmiXtH7tQf"
      },
      "outputs": [],
      "source": [
        "# prediction endpoint using transformers pipeline\n",
        "@app.post(\"/sentiment/\")\n",
        "def predict_transformers(batch: List[str]):\n",
        "    predictions = transformers_pipeline(batch)\n",
        "    return [\n",
        "        {\n",
        "            \"labels\": [p[\"label\"] for p in prediction],\n",
        "            \"scores\": [p[\"score\"] for p in prediction],\n",
        "        }\n",
        "        for prediction in predictions\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUc9Q4Se7tQg"
      },
      "source": [
        "## 4. Add Argilla logging middleware to the application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "d2gfaJ1h7tQg"
      },
      "outputs": [],
      "source": [
        "def text2records(batch: List[str], outputs: List[dict]):\n",
        "    return [\n",
        "        text_classification_mapper(data, prediction)\n",
        "        for data, prediction in zip(batch, outputs)\n",
        "    ]\n",
        "\n",
        "app.add_middleware(\n",
        "    ArgillaLogHTTPMiddleware,\n",
        "    api_endpoint=\"/transformers/\",  # the endpoint that will be logged\n",
        "    dataset=\"monitoring_transformers\",  # your dataset name\n",
        "    records_mapper=text2records, # your post-process func to adapt service inputs and outputs into an Argilla record\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5zZP2eK7tQg"
      },
      "source": [
        "## 5. NER endpoint with spaCy\n",
        "We'll add a custom mapper to convert spaCy's output to `TokenClassificationRecord` format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "KtjMRo4m7tQg"
      },
      "outputs": [],
      "source": [
        "def token2records(batch: List[str], outputs: List[dict]):\n",
        "    return [\n",
        "        token_classification_mapper(data, prediction)\n",
        "        for data, prediction in zip(batch, outputs)\n",
        "    ]\n",
        "\n",
        "app.add_middleware(\n",
        "    ArgillaLogHTTPMiddleware,\n",
        "    api_endpoint=\"/spacy/\",\n",
        "    dataset=\"monitoring_spacy\",\n",
        "    records_mapper=token2records,\n",
        ")\n",
        "\n",
        "# prediction endpoint using spacy pipeline\n",
        "@app.post(\"/ner/\")\n",
        "def predict_spacy(batch: List[str]):\n",
        "    predictions = []\n",
        "    for text in batch:\n",
        "        doc = spacy_pipeline(text)  # spaCy Doc creation\n",
        "        # Entity annotations\n",
        "        entities = [\n",
        "            {\"label\": ent.label_, \"start\": ent.start_char, \"end\": ent.end_char}\n",
        "            for ent in doc.ents\n",
        "        ]\n",
        "\n",
        "        prediction = {\n",
        "            \"text\": text,\n",
        "            \"entities\": entities,\n",
        "        }\n",
        "        predictions.append(prediction)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeE_rOJV7tQh"
      },
      "source": [
        "Now we can add method to check if the server is up and running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "ida2gxGJ7tQh"
      },
      "outputs": [],
      "source": [
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"message\": \"alive\"}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CeHq7-tF7tQh"
      },
      "source": [
        "## Launching and testing the API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRV2iBdL7tQh"
      },
      "source": [
        "To launch the application, copy the whole code into a file named `main.py` (you can find the full file content in the appendix.\n",
        "\n",
        "Once you have created the file, you can run the following command. We add the nohup command in case you are running this on Colab, but you can remove it otherwise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYPfMtxs7tQh",
        "outputId": "ddad7315-1898-42d2-8c90-15ec4b018b0c"
      },
      "outputs": [],
      "source": [
        "!nohup uvicorn main:app"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YPQvlzI_BiLY"
      },
      "source": [
        "### Testing our endpoint and prediction logging\n",
        "\n",
        "If we now start calling our API, our model inputs and outputs should be logged into their corresponding Argilla Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BHcQM4WTBrxz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'[{\"labels\":[\"NEGATIVE\",\"POSITIVE\"],\"scores\":[0.8717259168624878,0.128274068236351]},{\"labels\":[\"NEGATIVE\",\"POSITIVE\"],\"scores\":[0.9916356801986694,0.008364332839846611]}]'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "response = requests.post(\n",
        "    \"http://localhost:8000/sentiment/\", \n",
        "    json=[\"I like Argilla\", \"I hated data labelling but now I don't\"]\n",
        ")\n",
        "\n",
        "response.content"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If everything went well you should see two new records on your Argilla `monitoring_transformers` dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziWb7YVi7tQi"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, we learned to automatically log model inputs and outputs into Argilla.\n",
        "This can be used to continuously and transparently monitor HTTP inference endpoints.\n",
        "\n",
        "## Apendix: `main.py` full code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4GLPFOcE_yKa"
      },
      "outputs": [],
      "source": [
        "import argilla\n",
        "\n",
        "from fastapi import FastAPI\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import spacy\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "from argilla.monitoring.asgi import (\n",
        "    ArgillaLogHTTPMiddleware, \n",
        "    text_classification_mapper,\n",
        "    token_classification_mapper,\n",
        ")\n",
        "\n",
        "spacy_pipeline = spacy.load(\"en_core_web_sm\")\n",
        "transformers_pipeline = pipeline(\"text-classification\", return_all_scores=True)\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# prediction endpoint using transformers pipeline\n",
        "@app.post(\"/sentiment/\")\n",
        "def predict_transformers(batch: List[str]):\n",
        "    predictions = transformers_pipeline(batch)\n",
        "    return [\n",
        "        {\n",
        "            \"labels\": [p[\"label\"] for p in prediction],\n",
        "            \"scores\": [p[\"score\"] for p in prediction],\n",
        "        }\n",
        "        for prediction in predictions\n",
        "    ]\n",
        "\n",
        "def text2records(batch: List[str], outputs: List[dict]):\n",
        "    return [\n",
        "        text_classification_mapper(data, prediction)\n",
        "        for data, prediction in zip(batch, outputs)\n",
        "    ]\n",
        "\n",
        "app.add_middleware(\n",
        "    ArgillaLogHTTPMiddleware,\n",
        "    api_endpoint=\"/transformers/\",  # the endpoint that will be logged\n",
        "    dataset=\"monitoring_transformers\",  # your dataset name\n",
        "    records_mapper=text2records, # your post-process func to adapt service inputs and outputs into an Argilla record\n",
        ")\n",
        "\n",
        "def token2records(batch: List[str], outputs: List[dict]):\n",
        "    return [\n",
        "        token_classification_mapper(data, prediction)\n",
        "        for data, prediction in zip(batch, outputs)\n",
        "    ]\n",
        "\n",
        "# prediction endpoint using spacy pipeline\n",
        "@app.post(\"/ner/\")\n",
        "def predict_spacy(batch: List[str]):\n",
        "    predictions = []\n",
        "    for text in batch:\n",
        "        doc = spacy_pipeline(text)  # spaCy Doc creation\n",
        "        # Entity annotations\n",
        "        entities = [\n",
        "            {\"label\": ent.label_, \"start\": ent.start_char, \"end\": ent.end_char}\n",
        "            for ent in doc.ents\n",
        "        ]\n",
        "\n",
        "        prediction = {\n",
        "            \"text\": text,\n",
        "            \"entities\": entities,\n",
        "        }\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "app.add_middleware(\n",
        "    ArgillaLogHTTPMiddleware,\n",
        "    api_endpoint=\"/ner/\",\n",
        "    dataset=\"monitoring_spacy\",\n",
        "    records_mapper=token2records,\n",
        ")\n",
        "\n",
        "app.add_middleware(\n",
        "    ArgillaLogHTTPMiddleware,\n",
        "    api_endpoint=\"/sentiment/\",\n",
        "    dataset=\"monitoring_transformers\",\n",
        "    records_mapper=text2records,\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"message\": \"alive\"}\n",
        "\n",
        "argilla.init(\n",
        "    api_url=\"http://localhost:6900\", \n",
        "    api_key=\"team.apikey\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1 (default, Dec 17 2020, 03:56:09) \n[Clang 11.0.0 (clang-1100.0.33.17)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "6c6ac5964d63158aef0c318a650c56c288100fe36867cf6a65be3eefaa97102a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
