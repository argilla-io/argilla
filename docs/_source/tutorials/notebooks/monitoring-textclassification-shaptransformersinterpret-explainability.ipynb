{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb4af95f-9940-4e8c-832f-446f6b2d50c5",
   "metadata": {},
   "source": [
    "# üïµÔ∏è‚Äç‚ôÄÔ∏è Analize predictions with explainability methods \n",
    "\n",
    "In this tutorial you will learn to log and explore NLP model explanations using Transformers and the following Libraries\n",
    "\n",
    "* Transformers Interpret\n",
    "* Shap\n",
    "\n",
    "Interpretability and explanation information in Argilla is not limited to these two libraries. You can populate this information using your method of choice to highlight specific tokens. \n",
    "\n",
    "This tutorial is useful to get started and understand the underlying structure of explanation information in Argilla records.\n",
    "\n",
    "![monitoring-textclassification-shaptransformersinterpret-explainability](../../_static/tutorials/monitoring-textclassification-shaptransformersinterpret-explainability/monitoring-textclassification-shaptransformersinterpret-explainability.png)\n",
    "\n",
    "Beyond browsing examples during model development and evaluation, storing explainability information in Argilla can be really useful for monitoring and assessing production models (more tutorials on this soon!)\n",
    "\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6aca98af",
   "metadata": {},
   "source": [
    "## Running Argilla\n",
    "\n",
    "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
    "\n",
    "\n",
    "**Deploy Argilla on Hugging Face Spaces**: If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:\n",
    "\n",
    "[![deploy on spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/new-space?template=argilla/argilla-template-space)\n",
    "\n",
    "For details about configuring your deployment, check the [official Hugging Face Hub guide](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla).\n",
    "\n",
    "\n",
    "**Launch Argilla using Argilla's quickstart Docker image**: This is the recommended option if you want [Argilla running on your local machine](../../getting_started/quickstart.ipynb). Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
    "\n",
    "For more information on deployment options, please check the Deployment section of the documentation.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Tip\n",
    "    \n",
    "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
    "\n",
    "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
    "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01dfa90-6745-4f2f-84e3-4417c068db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install argilla transformers-interpret==0.5.2 datasets transformers shap==0.40.0 numba==0.53.1 -qqq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e68f8ad7",
   "metadata": {},
   "source": [
    "Let's import the Argilla module for reading and writing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc49b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1459fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the `URL` and `API_KEY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1bc064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "rg.init(\n",
    "    api_url=\"http://localhost:6900\", \n",
    "    api_key=\"admin.apikey\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5be64d5e",
   "metadata": {
    "id": "ccL8UFwj_CaD"
   },
   "source": [
    "Finally, let's include the imports we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff3ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "import shap\n",
    "\n",
    "from argilla import TokenAttributions, TextClassificationRecord\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a20408",
   "metadata": {},
   "source": [
    "## Token attributions and what do highlight colors mean?\n",
    "\n",
    "Argilla enables you to register token attributions as part of the dataset records. For getting token attributions, you can use methods such as Integrated Gradients or SHAP. These methods try to provide a mechanism to interpret model predictions. The attributions work as follows:\n",
    "\n",
    "* **[0,1] Positive attributions (in blue)** reflect those tokens that are making the model predict the specific predicted label.\n",
    "\n",
    "* **[-1, 0] Negative attributions (in red)** reflect those tokens that can influence the model to predict a label other than the specific predicted label."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb5b7960-34b6-45a3-9ffe-3812cd75069a",
   "metadata": {},
   "source": [
    "## Using `Transformers Interpret`\n",
    "\n",
    "In this example, we will use the `sst` sentiment dataset and a distilbert-based sentiment classifier. For getting model explanation information, we will use the excellent [Transformers Interpret](https://github.com/cdpierse/transformers-interpret) library by [Charles Pierse](https://github.com/cdpierse).\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Warning\n",
    "\n",
    "Computing model explanation information is computationally intensive and might be really slow if you don't have a GPU. Even if you have a GPU, it could take around 3-4 minutes to compute this information for 500 records of this dataset. Try reducing the number of records using modifying the `.select(range(1500))` method call.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e0021-62a5-4fec-b133-a9cceaabe365",
   "metadata": {},
   "source": [
    "### Create a fully searchable dataset with model predictions and explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c79a20-59f1-499e-a10b-fd721dec232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stanford sentiment treebank test set\n",
    "dataset = load_dataset(\"sst\", \"default\", split=\"test\")\n",
    "\n",
    "# Let's use a sentiment classifier fine-tuned on sst\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the explainer using transformers_interpret\n",
    "cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
    "\n",
    "# remove overlapping ##tokens\n",
    "def merge_word_attributions(word_attributions):\n",
    "    sentence =[]\n",
    "    i = 0\n",
    "    for attribution in word_attributions:\n",
    "        word = attribution[0]\n",
    "        score =  attribution[1]\n",
    "        \n",
    "        if \"##\" in word:\n",
    "            previous_word = sentence[i-1][0]\n",
    "            previous_score = sentence[i-1][1]\n",
    "            sentence[i-1] = (previous_word+ word[2:], previous_score if abs(previous_score) > abs(score) else score)\n",
    "        else:\n",
    "            sentence.append(attribution)\n",
    "            i+=1\n",
    "    return sentence\n",
    "\n",
    "\n",
    "records = []\n",
    "for example in dataset.select(range(500)):\n",
    "    # Build Token attributions objects\n",
    "    word_attributions = merge_word_attributions(cls_explainer(example[\"sentence\"]))\n",
    "    token_attributions = [\n",
    "        TokenAttributions(\n",
    "            token=token, attributions={cls_explainer.predicted_class_name: score}\n",
    "        )  # ignore first (CLS) and last (SEP) tokens\n",
    "        for token, score in word_attributions[1:-1]\n",
    "    ]\n",
    "    # Build Text classification records\n",
    "    record = rg.TextClassificationRecord(\n",
    "        text=example[\"sentence\"],\n",
    "        prediction=[(cls_explainer.predicted_class_name, cls_explainer.pred_probs)],\n",
    "        explanation={\"text\": token_attributions},\n",
    "    )\n",
    "    records.append(record)\n",
    "\n",
    "# Build Argilla dataset with interpretations for each record\n",
    "rg.log(records, name=\"transformers_interpret_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ffd33-851c-4339-a29d-4bb36b66adfb",
   "metadata": {},
   "source": [
    "## Using `Shap`\n",
    "\n",
    "In this example, we will use the widely-used [Shap](https://github.com/slundberg/shap) library by [\n",
    "Scott Lundberg](https://github.com/slundberg)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0650a144-c331-4b5d-b013-3ae41eb92418",
   "metadata": {},
   "source": [
    "### Create a fully searchable dataset with model predictions and explanations\n",
    "\n",
    "This example is very similar to the one above. The main difference is that we need to scale the values from Shap to match the range required by Argilla UI. This restriction is for visualization purposes. If you are more interested in monitoring use cases you might not need to rescale.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Warning\n",
    "\n",
    "Computing model explanation information is computationally intensive and might be really slow if you don't have a GPU. Even if you have a GPU, it could take around 1-2 minutes to compute this information for 100 records of this dataset. Try reducing the number of records using modifying the `.select(range(1500))` method call.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586d2b9-aad8-46c0-b40c-765c398f9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygments.token import Text\n",
    "# Let's use a sentiment classifier fine-tuned on sst\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Transformers pipeline model\n",
    "pipeline = transformers.pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    device=device,\n",
    "    top_k=None\n",
    ")\n",
    "\n",
    "# Load Stanford treebank dataset only the first 100 records for testing\n",
    "sst = load_dataset(\"sst2\", split=\"train[0:100]\")\n",
    "\n",
    "# Use shap's library text explainer\n",
    "explainer = shap.Explainer(pipeline)\n",
    "shap_values = explainer(sst[\"sentence\"])\n",
    "\n",
    "# Instantiate the scaler\n",
    "scaler = MinMaxScaler(feature_range=[-1, 1])\n",
    "\n",
    "predictions = pipeline(sst[\"sentence\"])\n",
    "\n",
    "for i in range(0, len(shap_values.values)):\n",
    "    # Scale shap values betweeen -1 and 1 (using e.g., scikit-learn MinMaxScaler\n",
    "    scaled = scaler.fit_transform(shap_values.values[i])\n",
    "\n",
    "    # get prediction label idx for indexing attributions and shap_values\n",
    "    # sorts by score to get the max score prediction\n",
    "    sorted_predictions = sorted(predictions[i], key=lambda d: d[\"score\"], reverse=True)\n",
    "    label_idx = 0 if sorted_predictions[0][\"label\"] == \"NEGATIVE\" else 1\n",
    "\n",
    "    # Build token attributions\n",
    "    token_attributions = [\n",
    "        TokenAttributions(\n",
    "            token=token, attributions={shap_values.output_names[label_idx]: score}\n",
    "        )\n",
    "        for token, score in zip(shap_values.data[i], [row[label_idx] for row in scaled])\n",
    "    ]\n",
    "    # build annotation label\n",
    "    annotation = \"POSITIVE\" if sst[\"label\"][i] == 1 else \"NEGATIVE\"\n",
    "\n",
    "    # Build Argilla record\n",
    "    record = TextClassificationRecord(\n",
    "        Text=sst[\"sentence\"][i],\n",
    "        prediction=[(pred[\"label\"], pred[\"score\"]) for pred in predictions[i]],\n",
    "        annotation=label,\n",
    "        explanation={\"text\": token_attributions},\n",
    "    )\n",
    "    # add record\n",
    "    records.append(record)\n",
    "# Log records\n",
    "rg.log(records, name=\"argilla_shap_example\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Dec 17 2020, 03:56:09) \n[Clang 11.0.0 (clang-1100.0.33.17)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8874e298d2bce9702a08b32d5709c9f02d53b2045f1d246836c6e4c8123e6782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
