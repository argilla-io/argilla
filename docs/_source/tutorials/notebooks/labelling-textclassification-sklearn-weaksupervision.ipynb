{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7641399c-d8bc-411b-952b-32a9f2100776",
   "metadata": {},
   "source": [
    "# ðŸ—‚ Weak supervision in multi-label text classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea2fec7-ded9-4752-be5d-d365ebe6c039",
   "metadata": {},
   "source": [
    "In this tutorial we use Argilla and weak supervision to tackle two multi-label classification datasets:\n",
    "\n",
    "- The first dataset is a curated version of [**GoEmotions**](https://huggingface.co/datasets/go_emotions), a dataset intended for **multi-label emotion classification**.\n",
    "- We inspect the dataset in Argilla, come up with good heuristics, and combine them with a label model to train a **weakly supervised Hugging Face transformer**.\n",
    "- In the second dataset, we [**categorize research papers**](https://www.kaggle.com/shivanandmn/multilabel-classification-dataset) by topic based on their titles, which is a **multi-label topic classification** problem.\n",
    "- We repeat the process of finding good heuristics, combine them with a label model and train a **lightweight downstream model using sklearn** in the end.\n",
    "\n",
    "![labelling-textclassification-sklearn-weaksupervision](../../_static/tutorials/labelling-textclassification-sklearn-weaksupervision/labelling-textclassification-sklearn-weaksupervision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ad103-6b57-48ea-a125-7395f561941b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "The `Snorkel` and `FlyingSquid` label models are not suited for multi-label classification tasks and do not support them.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a742259-2a99-4f2b-ac62-074bf2d6892a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this tutorial we also need some third party libraries that can be installed via pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c31a41-b80d-44e9-97a3-0d9a71cd658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets \"transformers[torch]\" scikit-multilearn ipywidgets -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37c0bbe-904a-49de-9443-234252f0acb0",
   "metadata": {},
   "source": [
    "## GoEmotions\n",
    "\n",
    "The original [GoEmotions](https://huggingface.co/datasets/go_emotions) is a challenging dataset intended for multi-label emotion classification.\n",
    "For this tutorial, we simplify it a bit by selecting only 6 out of the 28 emotions: *admiration, annoyance, approval, curiosity, gratitude, optimism*.\n",
    "We also try to accentuate the multi-label part of the dataset by down-sampling the examples that are classified with only one label.\n",
    "See Appendix A for all the details of this preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2336f09-21a0-47a1-a5f7-4d930ddc1355",
   "metadata": {},
   "source": [
    "### Define rules\n",
    "\n",
    "Let us start by downloading our curated version of the dataset from the Hugging Face Hub, and log it to Argilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83559db4-9bc1-4080-bccb-cc154ed753ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 15:30:57.381 | WARNING  | datasets.builder:_create_builder_config:412 - Using custom data configuration argilla--go_emotions_multi-label-519819f83f95366c\n",
      "2022-11-21 15:30:57.390 | WARNING  | datasets.builder:download_and_prepare:577 - Reusing dataset parquet (C:\\Users\\ufukh\\.cache\\huggingface\\datasets\\argilla___parquet\\argilla--go_emotions_multi-label-519819f83f95366c\\0.0.0\\0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    }
   ],
   "source": [
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Download preprocessed dataset\n",
    "ds_rb = rg.read_datasets(\n",
    "    load_dataset(\"argilla/go_emotions_multi-label\", split=\"train\"),\n",
    "    task=\"TextClassification\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c11e9e-9dec-424e-a582-795803e0a682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99916b93759b41018e84c7a457ef3741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4208 records logged to http://localhost:6900/datasets/argilla/go_emotions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='go_emotions', processed=4208, failed=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log dataset to Argilla to find good heuristics\n",
    "rg.log(ds_rb, name=\"go_emotions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0531d9c-b7df-4128-8b93-f8661558d603",
   "metadata": {},
   "source": [
    "After uploading the dataset, we can explore and inspect it to find good heuristic rules.\n",
    "For this we highly recommend the dedicated [*Define rules* mode](../reference/webapp/define_rules.md) of the Argilla web app, that allows you to quickly iterate over heuristic rules, compute their metrics and save them.\n",
    "\n",
    "Here we copy our rules found via the web app to the notebook for you to easily follow along the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90591ef-bedb-4069-b491-bd9d7f58edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.labeling.text_classification import Rule\n",
    "\n",
    "# Define our heuristic rules, they can surely be improved\n",
    "rules = [\n",
    "    Rule(\"thank*\", \"gratitude\"),\n",
    "    Rule(\"appreciate\", \"gratitude\"),\n",
    "    Rule(\"text:(thanks AND good)\", [\"admiration\", \"gratitude\"]),\n",
    "    Rule(\"advice\", \"admiration\"),\n",
    "    Rule(\"amazing\", \"admiration\"),\n",
    "    Rule(\"awesome\", \"admiration\"),\n",
    "    Rule(\"impressed\", \"admiration\"),\n",
    "    Rule(\"text:(good AND (point OR call OR idea OR job))\", \"admiration\"),\n",
    "    Rule(\"legend\", \"admiration\"),\n",
    "    Rule(\"exactly\", \"approval\"),\n",
    "    Rule(\"agree\", \"approval\"),\n",
    "    Rule(\"yeah\", \"optimism\"),\n",
    "    Rule(\"suck\", \"annoyance\"),\n",
    "    Rule(\"pissed\", \"annoyance\"),\n",
    "    Rule(\"annoying\", \"annoyance\"),\n",
    "    Rule(\"ruined\", \"annoyance\"),\n",
    "    Rule(\"hoping\", \"optimism\"),\n",
    "    Rule(\"joking\", [\"optimism\", \"admiration\"]),\n",
    "    Rule('text:(\"good luck\")', \"optimism\"),\n",
    "    Rule('\"nice day\"', \"optimism\"),\n",
    "    Rule('\"what is\"', \"curiosity\"),\n",
    "    Rule('\"can you\"', \"curiosity\"),\n",
    "    Rule('\"would you\"', \"curiosity\"),\n",
    "    Rule('\"do you\"', [\"curiosity\", \"admiration\"]),\n",
    "    Rule('\"great\"', [\"annoyance\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da2cd0-58b3-4bf6-af48-88852959bd48",
   "metadata": {},
   "source": [
    "We go on and apply these heuristic rules to our dataset creating our weak label matrix.\n",
    "Since we are dealing with a multi-label classification task, the weak label matrix will have 3 dimensions.\n",
    "\n",
    "> Dimensions of the weak multi label matrix: *number of records* x *number of rules* x *number of labels* \n",
    "\n",
    "It will be filled with 0 and 1, depending on if the rule voted for the respective label or not.\n",
    "If the rule abstained for a given record, the matrix will be filled with -1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1f67bb-8666-4fd9-a293-f0e6a1168f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.labeling.text_classification import WeakMultiLabels, add_rules, delete_rules, update_rules\n",
    "\n",
    "# Compute the weak labels for our dataset given the rules.\n",
    "# If your dataset already contains rules you can omit the rules argument.\n",
    "\n",
    "\n",
    "add_rules(dataset=\"go_emotions\", rules=rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d66617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f502db00f8b64d8988dff987ab672360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing rules:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3575782fafc4a1c91df2472768035be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying rules:   0%|          | 0/4208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9b10797ddd4d28a903abec0a9f325e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filling weak label matrix:   0%|          | 0/4208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weak_labels = WeakMultiLabels(\"go_emotions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2021a90-9e5b-4d5a-996d-71d8c19679d9",
   "metadata": {},
   "source": [
    "We can call the `weak_labels.summary()` method to check the precision of each rule as well as our total coverage of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdc6fea9-f248-4f55-a542-a0296f2873f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>coverage</th>\n",
       "      <th>annotated_coverage</th>\n",
       "      <th>overlaps</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>thank*</th>\n",
       "      <td>{gratitude}</td>\n",
       "      <td>0.199382</td>\n",
       "      <td>0.198925</td>\n",
       "      <td>0.048004</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appreciate</th>\n",
       "      <td>{gratitude}</td>\n",
       "      <td>0.016397</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.009981</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text:(thanks AND good)</th>\n",
       "      <td>{admiration, gratitude}</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advice</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.025428</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.004990</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.025190</td>\n",
       "      <td>0.034946</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impressed</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text:(good AND (point OR call OR idea OR job))</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.008555</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legend</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exactly</th>\n",
       "      <td>{approval}</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agree</th>\n",
       "      <td>{approval}</td>\n",
       "      <td>0.016873</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeah</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.024952</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suck</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pissed</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoying</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruined</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoping</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joking</th>\n",
       "      <td>{admiration, optimism}</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text:(\"good luck\")</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.015209</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"nice day\"</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"what is\"</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"can you\"</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"would you\"</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"do you\"</th>\n",
       "      <td>{admiration, curiosity}</td>\n",
       "      <td>0.010932</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"great\"</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.055133</td>\n",
       "      <td>0.061828</td>\n",
       "      <td>0.016873</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>{annoyance, approval, optimism, gratitude, adm...</td>\n",
       "      <td>0.379753</td>\n",
       "      <td>0.448925</td>\n",
       "      <td>0.060361</td>\n",
       "      <td>169</td>\n",
       "      <td>44</td>\n",
       "      <td>0.793427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            label  \\\n",
       "thank*                                                                                {gratitude}   \n",
       "appreciate                                                                            {gratitude}   \n",
       "text:(thanks AND good)                                                    {admiration, gratitude}   \n",
       "advice                                                                               {admiration}   \n",
       "amazing                                                                              {admiration}   \n",
       "awesome                                                                              {admiration}   \n",
       "impressed                                                                            {admiration}   \n",
       "text:(good AND (point OR call OR idea OR job))                                       {admiration}   \n",
       "legend                                                                               {admiration}   \n",
       "exactly                                                                                {approval}   \n",
       "agree                                                                                  {approval}   \n",
       "yeah                                                                                   {optimism}   \n",
       "suck                                                                                  {annoyance}   \n",
       "pissed                                                                                {annoyance}   \n",
       "annoying                                                                              {annoyance}   \n",
       "ruined                                                                                {annoyance}   \n",
       "hoping                                                                                 {optimism}   \n",
       "joking                                                                     {admiration, optimism}   \n",
       "text:(\"good luck\")                                                                     {optimism}   \n",
       "\"nice day\"                                                                             {optimism}   \n",
       "\"what is\"                                                                             {curiosity}   \n",
       "\"can you\"                                                                             {curiosity}   \n",
       "\"would you\"                                                                           {curiosity}   \n",
       "\"do you\"                                                                  {admiration, curiosity}   \n",
       "\"great\"                                                                               {annoyance}   \n",
       "total                                           {annoyance, approval, optimism, gratitude, adm...   \n",
       "\n",
       "                                                coverage  annotated_coverage  \\\n",
       "thank*                                          0.199382            0.198925   \n",
       "appreciate                                      0.016397            0.021505   \n",
       "text:(thanks AND good)                          0.007842            0.010753   \n",
       "advice                                          0.008317            0.008065   \n",
       "amazing                                         0.025428            0.021505   \n",
       "awesome                                         0.025190            0.034946   \n",
       "impressed                                       0.002139            0.005376   \n",
       "text:(good AND (point OR call OR idea OR job))  0.008555            0.018817   \n",
       "legend                                          0.001901            0.002688   \n",
       "exactly                                         0.007842            0.010753   \n",
       "agree                                           0.016873            0.021505   \n",
       "yeah                                            0.024952            0.021505   \n",
       "suck                                            0.002139            0.008065   \n",
       "pissed                                          0.002139            0.008065   \n",
       "annoying                                        0.003327            0.018817   \n",
       "ruined                                          0.000713            0.002688   \n",
       "hoping                                          0.003565            0.005376   \n",
       "joking                                          0.000238            0.000000   \n",
       "text:(\"good luck\")                              0.015209            0.018817   \n",
       "\"nice day\"                                      0.000713            0.005376   \n",
       "\"what is\"                                       0.004040            0.005376   \n",
       "\"can you\"                                       0.004278            0.008065   \n",
       "\"would you\"                                     0.000951            0.005376   \n",
       "\"do you\"                                        0.010932            0.018817   \n",
       "\"great\"                                         0.055133            0.061828   \n",
       "total                                           0.379753            0.448925   \n",
       "\n",
       "                                                overlaps  correct  incorrect  \\\n",
       "thank*                                          0.048004       74          0   \n",
       "appreciate                                      0.009981        7          1   \n",
       "text:(thanks AND good)                          0.007842        8          0   \n",
       "advice                                          0.007605        3          0   \n",
       "amazing                                         0.004990        8          0   \n",
       "awesome                                         0.007605       12          1   \n",
       "impressed                                       0.000000        2          0   \n",
       "text:(good AND (point OR call OR idea OR job))  0.003089        7          0   \n",
       "legend                                          0.000475        1          0   \n",
       "exactly                                         0.002376        3          1   \n",
       "agree                                           0.003327        6          2   \n",
       "yeah                                            0.006179        2          6   \n",
       "suck                                            0.000475        3          0   \n",
       "pissed                                          0.000713        2          1   \n",
       "annoying                                        0.001188        7          0   \n",
       "ruined                                          0.000238        1          0   \n",
       "hoping                                          0.000713        2          0   \n",
       "joking                                          0.000000        0          0   \n",
       "text:(\"good luck\")                              0.002614        4          3   \n",
       "\"nice day\"                                      0.000000        2          0   \n",
       "\"what is\"                                       0.001188        2          0   \n",
       "\"can you\"                                       0.000713        3          0   \n",
       "\"would you\"                                     0.000238        2          0   \n",
       "\"do you\"                                        0.002376        7          7   \n",
       "\"great\"                                         0.016873        1         22   \n",
       "total                                           0.060361      169         44   \n",
       "\n",
       "                                                precision  \n",
       "thank*                                           1.000000  \n",
       "appreciate                                       0.875000  \n",
       "text:(thanks AND good)                           1.000000  \n",
       "advice                                           1.000000  \n",
       "amazing                                          1.000000  \n",
       "awesome                                          0.923077  \n",
       "impressed                                        1.000000  \n",
       "text:(good AND (point OR call OR idea OR job))   1.000000  \n",
       "legend                                           1.000000  \n",
       "exactly                                          0.750000  \n",
       "agree                                            0.750000  \n",
       "yeah                                             0.250000  \n",
       "suck                                             1.000000  \n",
       "pissed                                           0.666667  \n",
       "annoying                                         1.000000  \n",
       "ruined                                           1.000000  \n",
       "hoping                                           1.000000  \n",
       "joking                                                NaN  \n",
       "text:(\"good luck\")                               0.571429  \n",
       "\"nice day\"                                       1.000000  \n",
       "\"what is\"                                        1.000000  \n",
       "\"can you\"                                        1.000000  \n",
       "\"would you\"                                      1.000000  \n",
       "\"do you\"                                         0.500000  \n",
       "\"great\"                                          0.043478  \n",
       "total                                            0.793427  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check coverage/precision of our rules\n",
    "weak_labels.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3fffe",
   "metadata": {},
   "source": [
    "we can observe that \"joking\" does not have any support and also \"do you\" is not informative, because its correct/incorrect ratio equals to 1. We can delete these two rules from the dataset using \"delete_rules\" method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f047f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_to_delete = [\n",
    "    Rule(\"joking\", [\"optimism\", \"admiration\"]),\n",
    "    Rule('\"do you\"', [\"curiosity\", \"admiration\"])]\n",
    "\n",
    "delete_rules(dataset=\"go_emotions\", rules=rules_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c8167",
   "metadata": {},
   "source": [
    "# lets apply Weak Labeling again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d053adfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dd8997c5d14ba394d84db90e604738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing rules:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccd38b09557457fae9c5dd0980b92a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying rules:   0%|          | 0/4208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b27f7dedffe477e8a13e7eedfa3d28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filling weak label matrix:   0%|          | 0/4208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weak_labels = WeakMultiLabels(\"go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ad4fbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>coverage</th>\n",
       "      <th>annotated_coverage</th>\n",
       "      <th>overlaps</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>thank*</th>\n",
       "      <td>{gratitude}</td>\n",
       "      <td>0.199382</td>\n",
       "      <td>0.198925</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appreciate</th>\n",
       "      <td>{gratitude}</td>\n",
       "      <td>0.016397</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text:(thanks AND good)</th>\n",
       "      <td>{admiration, gratitude}</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advice</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.007367</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.025428</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.004990</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.025190</td>\n",
       "      <td>0.034946</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impressed</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text:(good AND (point OR call OR idea OR job))</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.008555</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legend</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exactly</th>\n",
       "      <td>{approval}</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agree</th>\n",
       "      <td>{approval}</td>\n",
       "      <td>0.016873</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeah</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.024952</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suck</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pissed</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoying</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruined</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoping</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text:(\"good luck\")</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.015209</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"nice day\"</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"what is\"</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"can you\"</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"would you\"</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"great\"</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.055133</td>\n",
       "      <td>0.061828</td>\n",
       "      <td>0.016397</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>{annoyance, approval, optimism, gratitude, adm...</td>\n",
       "      <td>0.370960</td>\n",
       "      <td>0.435484</td>\n",
       "      <td>0.058222</td>\n",
       "      <td>162</td>\n",
       "      <td>37</td>\n",
       "      <td>0.814070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            label  \\\n",
       "thank*                                                                                {gratitude}   \n",
       "appreciate                                                                            {gratitude}   \n",
       "text:(thanks AND good)                                                    {admiration, gratitude}   \n",
       "advice                                                                               {admiration}   \n",
       "amazing                                                                              {admiration}   \n",
       "awesome                                                                              {admiration}   \n",
       "impressed                                                                            {admiration}   \n",
       "text:(good AND (point OR call OR idea OR job))                                       {admiration}   \n",
       "legend                                                                               {admiration}   \n",
       "exactly                                                                                {approval}   \n",
       "agree                                                                                  {approval}   \n",
       "yeah                                                                                   {optimism}   \n",
       "suck                                                                                  {annoyance}   \n",
       "pissed                                                                                {annoyance}   \n",
       "annoying                                                                              {annoyance}   \n",
       "ruined                                                                                {annoyance}   \n",
       "hoping                                                                                 {optimism}   \n",
       "text:(\"good luck\")                                                                     {optimism}   \n",
       "\"nice day\"                                                                             {optimism}   \n",
       "\"what is\"                                                                             {curiosity}   \n",
       "\"can you\"                                                                             {curiosity}   \n",
       "\"would you\"                                                                           {curiosity}   \n",
       "\"great\"                                                                               {annoyance}   \n",
       "total                                           {annoyance, approval, optimism, gratitude, adm...   \n",
       "\n",
       "                                                coverage  annotated_coverage  \\\n",
       "thank*                                          0.199382            0.198925   \n",
       "appreciate                                      0.016397            0.021505   \n",
       "text:(thanks AND good)                          0.007842            0.010753   \n",
       "advice                                          0.008317            0.008065   \n",
       "amazing                                         0.025428            0.021505   \n",
       "awesome                                         0.025190            0.034946   \n",
       "impressed                                       0.002139            0.005376   \n",
       "text:(good AND (point OR call OR idea OR job))  0.008555            0.018817   \n",
       "legend                                          0.001901            0.002688   \n",
       "exactly                                         0.007842            0.010753   \n",
       "agree                                           0.016873            0.021505   \n",
       "yeah                                            0.024952            0.021505   \n",
       "suck                                            0.002139            0.008065   \n",
       "pissed                                          0.002139            0.008065   \n",
       "annoying                                        0.003327            0.018817   \n",
       "ruined                                          0.000713            0.002688   \n",
       "hoping                                          0.003565            0.005376   \n",
       "text:(\"good luck\")                              0.015209            0.018817   \n",
       "\"nice day\"                                      0.000713            0.005376   \n",
       "\"what is\"                                       0.004040            0.005376   \n",
       "\"can you\"                                       0.004278            0.008065   \n",
       "\"would you\"                                     0.000951            0.005376   \n",
       "\"great\"                                         0.055133            0.061828   \n",
       "total                                           0.370960            0.435484   \n",
       "\n",
       "                                                overlaps  correct  incorrect  \\\n",
       "thank*                                          0.047766       74          0   \n",
       "appreciate                                      0.009743        7          1   \n",
       "text:(thanks AND good)                          0.007842        8          0   \n",
       "advice                                          0.007367        3          0   \n",
       "amazing                                         0.004990        8          0   \n",
       "awesome                                         0.007129       12          1   \n",
       "impressed                                       0.000000        2          0   \n",
       "text:(good AND (point OR call OR idea OR job))  0.003089        7          0   \n",
       "legend                                          0.000475        1          0   \n",
       "exactly                                         0.002139        3          1   \n",
       "agree                                           0.003327        6          2   \n",
       "yeah                                            0.006179        2          6   \n",
       "suck                                            0.000475        3          0   \n",
       "pissed                                          0.000475        2          1   \n",
       "annoying                                        0.001188        7          0   \n",
       "ruined                                          0.000238        1          0   \n",
       "hoping                                          0.000713        2          0   \n",
       "text:(\"good luck\")                              0.002614        4          3   \n",
       "\"nice day\"                                      0.000000        2          0   \n",
       "\"what is\"                                       0.001188        2          0   \n",
       "\"can you\"                                       0.000713        3          0   \n",
       "\"would you\"                                     0.000238        2          0   \n",
       "\"great\"                                         0.016397        1         22   \n",
       "total                                           0.058222      162         37   \n",
       "\n",
       "                                                precision  \n",
       "thank*                                           1.000000  \n",
       "appreciate                                       0.875000  \n",
       "text:(thanks AND good)                           1.000000  \n",
       "advice                                           1.000000  \n",
       "amazing                                          1.000000  \n",
       "awesome                                          0.923077  \n",
       "impressed                                        1.000000  \n",
       "text:(good AND (point OR call OR idea OR job))   1.000000  \n",
       "legend                                           1.000000  \n",
       "exactly                                          0.750000  \n",
       "agree                                            0.750000  \n",
       "yeah                                             0.250000  \n",
       "suck                                             1.000000  \n",
       "pissed                                           0.666667  \n",
       "annoying                                         1.000000  \n",
       "ruined                                           1.000000  \n",
       "hoping                                           1.000000  \n",
       "text:(\"good luck\")                               0.571429  \n",
       "\"nice day\"                                       1.000000  \n",
       "\"what is\"                                        1.000000  \n",
       "\"can you\"                                        1.000000  \n",
       "\"would you\"                                      1.000000  \n",
       "\"great\"                                          0.043478  \n",
       "total                                            0.814070  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8019280",
   "metadata": {},
   "source": [
    "We can observe that following rules are not working well; \n",
    "\n",
    "        Rule('\"great\"', [\"annoyance\"])\n",
    "\n",
    "        Rule(\"yeah\", \"optimism\"),\n",
    "\n",
    "Let's update this two rules such that:\n",
    "\n",
    "        Rule('\"great\"', [\"admiration\"])\n",
    "\n",
    "        Rule(\"yeah\", \"approval\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c299231",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_to_update = [\n",
    "    Rule('\"great\"', [\"admiration\"]),\n",
    "    Rule(\"yeah\", \"approval\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8d85e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_rules(dataset=\"go_emotions\", rules=rules_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ba48a",
   "metadata": {},
   "source": [
    "Lets' run weak labeling with final rules of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb5add74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53965de2d8742e5ba8b93c036fdbfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing rules:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad19fc987e1443d1b70865889546e759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying rules:   0%|          | 0/4208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574af058575247078b9e1363fbee7225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filling weak label matrix:   0%|          | 0/4208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weak_labels = WeakMultiLabels(dataset=\"go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449d7a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>coverage</th>\n",
       "      <th>annotated_coverage</th>\n",
       "      <th>overlaps</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>thank*</th>\n",
       "      <td>{gratitude}</td>\n",
       "      <td>0.199382</td>\n",
       "      <td>0.198925</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appreciate</th>\n",
       "      <td>{gratitude}</td>\n",
       "      <td>0.016397</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text:(thanks AND good)</th>\n",
       "      <td>{admiration, gratitude}</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advice</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.007367</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.025428</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.004990</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.025190</td>\n",
       "      <td>0.034946</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impressed</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text:(good AND (point OR call OR idea OR job))</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.008555</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legend</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exactly</th>\n",
       "      <td>{approval}</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agree</th>\n",
       "      <td>{approval}</td>\n",
       "      <td>0.016873</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeah</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.024952</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suck</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pissed</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoying</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruined</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoping</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text:(\"good luck\")</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.015209</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"nice day\"</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"what is\"</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"can you\"</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"would you\"</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"great\"</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.055133</td>\n",
       "      <td>0.061828</td>\n",
       "      <td>0.016397</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>{annoyance, approval, optimism, gratitude, adm...</td>\n",
       "      <td>0.370960</td>\n",
       "      <td>0.435484</td>\n",
       "      <td>0.058222</td>\n",
       "      <td>162</td>\n",
       "      <td>37</td>\n",
       "      <td>0.814070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            label  \\\n",
       "thank*                                                                                {gratitude}   \n",
       "appreciate                                                                            {gratitude}   \n",
       "text:(thanks AND good)                                                    {admiration, gratitude}   \n",
       "advice                                                                               {admiration}   \n",
       "amazing                                                                              {admiration}   \n",
       "awesome                                                                              {admiration}   \n",
       "impressed                                                                            {admiration}   \n",
       "text:(good AND (point OR call OR idea OR job))                                       {admiration}   \n",
       "legend                                                                               {admiration}   \n",
       "exactly                                                                                {approval}   \n",
       "agree                                                                                  {approval}   \n",
       "yeah                                                                                   {optimism}   \n",
       "suck                                                                                  {annoyance}   \n",
       "pissed                                                                                {annoyance}   \n",
       "annoying                                                                              {annoyance}   \n",
       "ruined                                                                                {annoyance}   \n",
       "hoping                                                                                 {optimism}   \n",
       "text:(\"good luck\")                                                                     {optimism}   \n",
       "\"nice day\"                                                                             {optimism}   \n",
       "\"what is\"                                                                             {curiosity}   \n",
       "\"can you\"                                                                             {curiosity}   \n",
       "\"would you\"                                                                           {curiosity}   \n",
       "\"great\"                                                                               {annoyance}   \n",
       "total                                           {annoyance, approval, optimism, gratitude, adm...   \n",
       "\n",
       "                                                coverage  annotated_coverage  \\\n",
       "thank*                                          0.199382            0.198925   \n",
       "appreciate                                      0.016397            0.021505   \n",
       "text:(thanks AND good)                          0.007842            0.010753   \n",
       "advice                                          0.008317            0.008065   \n",
       "amazing                                         0.025428            0.021505   \n",
       "awesome                                         0.025190            0.034946   \n",
       "impressed                                       0.002139            0.005376   \n",
       "text:(good AND (point OR call OR idea OR job))  0.008555            0.018817   \n",
       "legend                                          0.001901            0.002688   \n",
       "exactly                                         0.007842            0.010753   \n",
       "agree                                           0.016873            0.021505   \n",
       "yeah                                            0.024952            0.021505   \n",
       "suck                                            0.002139            0.008065   \n",
       "pissed                                          0.002139            0.008065   \n",
       "annoying                                        0.003327            0.018817   \n",
       "ruined                                          0.000713            0.002688   \n",
       "hoping                                          0.003565            0.005376   \n",
       "text:(\"good luck\")                              0.015209            0.018817   \n",
       "\"nice day\"                                      0.000713            0.005376   \n",
       "\"what is\"                                       0.004040            0.005376   \n",
       "\"can you\"                                       0.004278            0.008065   \n",
       "\"would you\"                                     0.000951            0.005376   \n",
       "\"great\"                                         0.055133            0.061828   \n",
       "total                                           0.370960            0.435484   \n",
       "\n",
       "                                                overlaps  correct  incorrect  \\\n",
       "thank*                                          0.047766       74          0   \n",
       "appreciate                                      0.009743        7          1   \n",
       "text:(thanks AND good)                          0.007842        8          0   \n",
       "advice                                          0.007367        3          0   \n",
       "amazing                                         0.004990        8          0   \n",
       "awesome                                         0.007129       12          1   \n",
       "impressed                                       0.000000        2          0   \n",
       "text:(good AND (point OR call OR idea OR job))  0.003089        7          0   \n",
       "legend                                          0.000475        1          0   \n",
       "exactly                                         0.002139        3          1   \n",
       "agree                                           0.003327        6          2   \n",
       "yeah                                            0.006179        2          6   \n",
       "suck                                            0.000475        3          0   \n",
       "pissed                                          0.000475        2          1   \n",
       "annoying                                        0.001188        7          0   \n",
       "ruined                                          0.000238        1          0   \n",
       "hoping                                          0.000713        2          0   \n",
       "text:(\"good luck\")                              0.002614        4          3   \n",
       "\"nice day\"                                      0.000000        2          0   \n",
       "\"what is\"                                       0.001188        2          0   \n",
       "\"can you\"                                       0.000713        3          0   \n",
       "\"would you\"                                     0.000238        2          0   \n",
       "\"great\"                                         0.016397        1         22   \n",
       "total                                           0.058222      162         37   \n",
       "\n",
       "                                                precision  \n",
       "thank*                                           1.000000  \n",
       "appreciate                                       0.875000  \n",
       "text:(thanks AND good)                           1.000000  \n",
       "advice                                           1.000000  \n",
       "amazing                                          1.000000  \n",
       "awesome                                          0.923077  \n",
       "impressed                                        1.000000  \n",
       "text:(good AND (point OR call OR idea OR job))   1.000000  \n",
       "legend                                           1.000000  \n",
       "exactly                                          0.750000  \n",
       "agree                                            0.750000  \n",
       "yeah                                             0.250000  \n",
       "suck                                             1.000000  \n",
       "pissed                                           0.666667  \n",
       "annoying                                         1.000000  \n",
       "ruined                                           1.000000  \n",
       "hoping                                           1.000000  \n",
       "text:(\"good luck\")                               0.571429  \n",
       "\"nice day\"                                       1.000000  \n",
       "\"what is\"                                        1.000000  \n",
       "\"can you\"                                        1.000000  \n",
       "\"would you\"                                      1.000000  \n",
       "\"great\"                                          0.043478  \n",
       "total                                            0.814070  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffe13e",
   "metadata": {},
   "source": [
    "Lets consider we want to try a rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39669aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimism_rule = Rule(\"wish*\", \"optimism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aa2e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimism_rule.apply(dataset=\"go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4a1a051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 0.006178707224334601,\n",
       " 'annotated_coverage': 0.0,\n",
       " 'correct': 0,\n",
       " 'incorrect': 0,\n",
       " 'precision': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimism_rule.metrics(dataset=\"go_emotions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb4a12",
   "metadata": {},
   "source": [
    "__optimism_rule__ is not informative so we don't add it to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c1c0c",
   "metadata": {},
   "source": [
    "Let's try a rule for __curiosity__ class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a16426a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "curiosity_rule = Rule(\"could you\", \"curiosity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "707d7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curiosity_rule.apply(\"go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6d9dfeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 0.005465779467680608,\n",
       " 'annotated_coverage': 0.002688172043010753,\n",
       " 'correct': 1,\n",
       " 'incorrect': 0,\n",
       " 'precision': 1.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curiosity_rule.metrics(dataset=\"go_emotions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812deb5e",
   "metadata": {},
   "source": [
    "__curiosity_rule__ have a positive support, we can add it to dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "267a7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "curiosity_rule.add_to_dataset(dataset=\"go_emotions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f6ea0c",
   "metadata": {},
   "source": [
    "Let's apply Weak Labeling again with final rule set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87f81588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521efc1b153a4200a257060aa9b4caed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing rules:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e951461de764401e9cc47905e9e5b7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying rules:   0%|          | 0/4208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3800c423483f4005820a6be2dc0febd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filling weak label matrix:   0%|          | 0/4208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weak_labels = WeakMultiLabels(dataset=\"go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eebedefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>coverage</th>\n",
       "      <th>annotated_coverage</th>\n",
       "      <th>overlaps</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>thank*</th>\n",
       "      <td>{gratitude}</td>\n",
       "      <td>0.199382</td>\n",
       "      <td>0.198925</td>\n",
       "      <td>0.048004</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appreciate</th>\n",
       "      <td>{gratitude}</td>\n",
       "      <td>0.016397</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text:(thanks AND good)</th>\n",
       "      <td>{admiration, gratitude}</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advice</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.007367</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.025428</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.004990</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.025190</td>\n",
       "      <td>0.034946</td>\n",
       "      <td>0.007367</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impressed</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text:(good AND (point OR call OR idea OR job))</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.008555</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legend</th>\n",
       "      <td>{admiration}</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exactly</th>\n",
       "      <td>{approval}</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agree</th>\n",
       "      <td>{approval}</td>\n",
       "      <td>0.016873</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeah</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.024952</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suck</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pissed</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoying</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruined</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoping</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text:(\"good luck\")</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.015209</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"nice day\"</th>\n",
       "      <td>{optimism}</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"what is\"</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"can you\"</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"would you\"</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"great\"</th>\n",
       "      <td>{annoyance}</td>\n",
       "      <td>0.055133</td>\n",
       "      <td>0.061828</td>\n",
       "      <td>0.016397</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>could you</th>\n",
       "      <td>{curiosity}</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>{annoyance, approval, optimism, gratitude, adm...</td>\n",
       "      <td>0.375238</td>\n",
       "      <td>0.435484</td>\n",
       "      <td>0.059173</td>\n",
       "      <td>163</td>\n",
       "      <td>37</td>\n",
       "      <td>0.815000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            label  \\\n",
       "thank*                                                                                {gratitude}   \n",
       "appreciate                                                                            {gratitude}   \n",
       "text:(thanks AND good)                                                    {admiration, gratitude}   \n",
       "advice                                                                               {admiration}   \n",
       "amazing                                                                              {admiration}   \n",
       "awesome                                                                              {admiration}   \n",
       "impressed                                                                            {admiration}   \n",
       "text:(good AND (point OR call OR idea OR job))                                       {admiration}   \n",
       "legend                                                                               {admiration}   \n",
       "exactly                                                                                {approval}   \n",
       "agree                                                                                  {approval}   \n",
       "yeah                                                                                   {optimism}   \n",
       "suck                                                                                  {annoyance}   \n",
       "pissed                                                                                {annoyance}   \n",
       "annoying                                                                              {annoyance}   \n",
       "ruined                                                                                {annoyance}   \n",
       "hoping                                                                                 {optimism}   \n",
       "text:(\"good luck\")                                                                     {optimism}   \n",
       "\"nice day\"                                                                             {optimism}   \n",
       "\"what is\"                                                                             {curiosity}   \n",
       "\"can you\"                                                                             {curiosity}   \n",
       "\"would you\"                                                                           {curiosity}   \n",
       "\"great\"                                                                               {annoyance}   \n",
       "could you                                                                             {curiosity}   \n",
       "total                                           {annoyance, approval, optimism, gratitude, adm...   \n",
       "\n",
       "                                                coverage  annotated_coverage  \\\n",
       "thank*                                          0.199382            0.198925   \n",
       "appreciate                                      0.016397            0.021505   \n",
       "text:(thanks AND good)                          0.007842            0.010753   \n",
       "advice                                          0.008317            0.008065   \n",
       "amazing                                         0.025428            0.021505   \n",
       "awesome                                         0.025190            0.034946   \n",
       "impressed                                       0.002139            0.005376   \n",
       "text:(good AND (point OR call OR idea OR job))  0.008555            0.018817   \n",
       "legend                                          0.001901            0.002688   \n",
       "exactly                                         0.007842            0.010753   \n",
       "agree                                           0.016873            0.021505   \n",
       "yeah                                            0.024952            0.021505   \n",
       "suck                                            0.002139            0.008065   \n",
       "pissed                                          0.002139            0.008065   \n",
       "annoying                                        0.003327            0.018817   \n",
       "ruined                                          0.000713            0.002688   \n",
       "hoping                                          0.003565            0.005376   \n",
       "text:(\"good luck\")                              0.015209            0.018817   \n",
       "\"nice day\"                                      0.000713            0.005376   \n",
       "\"what is\"                                       0.004040            0.005376   \n",
       "\"can you\"                                       0.004278            0.008065   \n",
       "\"would you\"                                     0.000951            0.005376   \n",
       "\"great\"                                         0.055133            0.061828   \n",
       "could you                                       0.005466            0.002688   \n",
       "total                                           0.375238            0.435484   \n",
       "\n",
       "                                                overlaps  correct  incorrect  \\\n",
       "thank*                                          0.048004       74          0   \n",
       "appreciate                                      0.009743        7          1   \n",
       "text:(thanks AND good)                          0.007842        8          0   \n",
       "advice                                          0.007367        3          0   \n",
       "amazing                                         0.004990        8          0   \n",
       "awesome                                         0.007367       12          1   \n",
       "impressed                                       0.000000        2          0   \n",
       "text:(good AND (point OR call OR idea OR job))  0.003089        7          0   \n",
       "legend                                          0.000475        1          0   \n",
       "exactly                                         0.002139        3          1   \n",
       "agree                                           0.003565        6          2   \n",
       "yeah                                            0.006179        2          6   \n",
       "suck                                            0.000475        3          0   \n",
       "pissed                                          0.000475        2          1   \n",
       "annoying                                        0.001188        7          0   \n",
       "ruined                                          0.000238        1          0   \n",
       "hoping                                          0.000713        2          0   \n",
       "text:(\"good luck\")                              0.002614        4          3   \n",
       "\"nice day\"                                      0.000000        2          0   \n",
       "\"what is\"                                       0.001188        2          0   \n",
       "\"can you\"                                       0.000713        3          0   \n",
       "\"would you\"                                     0.000475        2          0   \n",
       "\"great\"                                         0.016397        1         22   \n",
       "could you                                       0.001188        1          0   \n",
       "total                                           0.059173      163         37   \n",
       "\n",
       "                                                precision  \n",
       "thank*                                           1.000000  \n",
       "appreciate                                       0.875000  \n",
       "text:(thanks AND good)                           1.000000  \n",
       "advice                                           1.000000  \n",
       "amazing                                          1.000000  \n",
       "awesome                                          0.923077  \n",
       "impressed                                        1.000000  \n",
       "text:(good AND (point OR call OR idea OR job))   1.000000  \n",
       "legend                                           1.000000  \n",
       "exactly                                          0.750000  \n",
       "agree                                            0.750000  \n",
       "yeah                                             0.250000  \n",
       "suck                                             1.000000  \n",
       "pissed                                           0.666667  \n",
       "annoying                                         1.000000  \n",
       "ruined                                           1.000000  \n",
       "hoping                                           1.000000  \n",
       "text:(\"good luck\")                               0.571429  \n",
       "\"nice day\"                                       1.000000  \n",
       "\"what is\"                                        1.000000  \n",
       "\"can you\"                                        1.000000  \n",
       "\"would you\"                                      1.000000  \n",
       "\"great\"                                          0.043478  \n",
       "could you                                        1.000000  \n",
       "total                                            0.815000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19f6b8-520a-4caa-87a4-307f52749b92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create training set\n",
    "\n",
    "When we are happy with our heuristics, it is time to combine them and compute weak labels for the training of our downstream model.\n",
    "For this we will use the `MajorityVoter`.\n",
    "In the multi-label case, it sets the probability of a label to 0 or 1 depending on whether at least one non-abstaining rule voted for the respective label or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6985e6b7-7c28-4efc-84d2-08b7fff02ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.labeling.text_classification import MajorityVoter\n",
    "\n",
    "# Use the majority voter as the label model\n",
    "label_model = MajorityVoter(weak_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82fe715-e703-457a-a158-78709f442bbd",
   "metadata": {},
   "source": [
    "From our label model we get the training records together with its weak labels and probabilities.\n",
    "We will use the weak labels with a probability greater than 0.5 as labels for our training, and hence copy them to the `annotation` property of our records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "152abc10-1538-4660-aa3a-0e210887c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get records with the predictions from the label model to train a down-stream model\n",
    "train_rg = label_model.predict()\n",
    "\n",
    "# Copy label model predictions to annotation with a threshold of 0.5\n",
    "for rec in train_rg:\n",
    "    rec.annotation = [pred[0] for pred in rec.prediction if pred[1] > 0.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24414843-70c2-4dc2-902d-e6e8bcc2a449",
   "metadata": {},
   "source": [
    "We extract the test set with manual annotations from our `WeakMultiLabels` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fce0c5a0-bfa5-4649-a59a-f0114f7891a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get records with manual annotations to use as test set for the down-stream model\n",
    "test_rg = rg.DatasetForTextClassification(weak_labels.records(has_annotation=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7738210-4273-49c2-a70c-634b18fa008e",
   "metadata": {},
   "source": [
    "We will use the convenient `DatasetForTextClassification.prepare_for_training()` method to create datasets optimized for training with the Hugging Face transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "232a2d46-1516-4c7d-8d39-d55fe0d8130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Create dataset dictionary and shuffle training set\n",
    "ds = DatasetDict(\n",
    "    train=train_rg.prepare_for_training().shuffle(seed=42),\n",
    "    test=test_rg.prepare_for_training(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be5d9c-1532-4265-8f7a-18e346e9caaf",
   "metadata": {},
   "source": [
    "Let us push the dataset to the Hub to share it with our colleagues.\n",
    "It is also an easy way to outsource the training of the model to an environment with an accelerator, like Google Colab for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "072218ae-2167-48d8-8b93-243daff497b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 15:41:18.740 | WARNING  | datasets.dataset_dict:push_to_hub:1348 - Pushing split train to the Hub.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You need to provide a `token` or be logged in to Hugging Face with `huggingface-cli login`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ufukh\\OneDrive\\Documents\\Recognai\\rubrix\\docs\\_source\\tutorials\\notebooks\\labelling-textclassification-sklearn-weaksupervision.ipynb Cell 51\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/tutorials/notebooks/labelling-textclassification-sklearn-weaksupervision.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Push dataset for training our down-stream model to the HF hub\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ufukh/OneDrive/Documents/Recognai/rubrix/docs/_source/tutorials/notebooks/labelling-textclassification-sklearn-weaksupervision.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ds\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39m\"\u001b[39;49m\u001b[39margilla/go_emotions_training\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\datasets\\dataset_dict.py:1350\u001b[0m, in \u001b[0;36mDatasetDict.push_to_hub\u001b[1;34m(self, repo_id, private, token, branch, max_shard_size, shard_size, embed_external_files)\u001b[0m\n\u001b[0;32m   1348\u001b[0m logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPushing split \u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m to the Hub.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1349\u001b[0m \u001b[39m# The split=key needs to be removed before merging\u001b[39;00m\n\u001b[1;32m-> 1350\u001b[0m repo_id, split, uploaded_size, dataset_nbytes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m[split]\u001b[39m.\u001b[39;49m_push_parquet_shards_to_hub(\n\u001b[0;32m   1351\u001b[0m     repo_id,\n\u001b[0;32m   1352\u001b[0m     split\u001b[39m=\u001b[39;49msplit,\n\u001b[0;32m   1353\u001b[0m     private\u001b[39m=\u001b[39;49mprivate,\n\u001b[0;32m   1354\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   1355\u001b[0m     branch\u001b[39m=\u001b[39;49mbranch,\n\u001b[0;32m   1356\u001b[0m     max_shard_size\u001b[39m=\u001b[39;49mmax_shard_size,\n\u001b[0;32m   1357\u001b[0m     embed_external_files\u001b[39m=\u001b[39;49membed_external_files,\n\u001b[0;32m   1358\u001b[0m )\n\u001b[0;32m   1359\u001b[0m total_uploaded_size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m uploaded_size\n\u001b[0;32m   1360\u001b[0m total_dataset_nbytes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dataset_nbytes\n",
      "File \u001b[1;32mc:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\datasets\\arrow_dataset.py:3857\u001b[0m, in \u001b[0;36mDataset._push_parquet_shards_to_hub\u001b[1;34m(self, repo_id, split, private, token, branch, max_shard_size, embed_external_files)\u001b[0m\n\u001b[0;32m   3854\u001b[0m token \u001b[39m=\u001b[39m token \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m HfFolder\u001b[39m.\u001b[39mget_token()\n\u001b[0;32m   3856\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 3857\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   3858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to provide a `token` or be logged in to Hugging Face with \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3859\u001b[0m     )\n\u001b[0;32m   3861\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3862\u001b[0m     split \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mOSError\u001b[0m: You need to provide a `token` or be logged in to Hugging Face with `huggingface-cli login`."
     ]
    }
   ],
   "source": [
    "# Push dataset for training our down-stream model to the HF hub\n",
    "ds.push_to_hub(\"argilla/go_emotions_training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0957ad2-f05d-4ea4-b303-d77495a449e9",
   "metadata": {},
   "source": [
    "### Train a transformer downstream model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8cea7b-cd6f-408d-9667-249e3d66eac0",
   "metadata": {},
   "source": [
    "The following steps are basically a copy&paste from the amazing documentation of the [Hugging Face transformers](https://huggingface.co/docs/transformers) library.\n",
    "\n",
    "First, we will load the tokenizer corresponding to our model, which we choose to be the [distilled version](https://huggingface.co/distilbert-base-uncased) of the infamous BERT.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "Since we will use a full-blown transformer as a downstream model (albeit a distilled one), we recommend executing the following code on a machine with a GPU, or in a Google Colab with a GPU backend enabled.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88365498-15df-4aff-ae3a-48c22e4e1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0cb66a-c31e-4811-abc3-453179ee567f",
   "metadata": {},
   "source": [
    "Afterward, we tokenize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6635d069-e242-4d89-92be-ecce502b92db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4a07e214e745f8880ebbd0fca9c6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738ee5a29dc94b979b9a421a7ff25d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# Tokenize the data\n",
    "tokenized_ds = ds.map(tokenize_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d70c03-9602-472f-aa79-28777fe3b0e2",
   "metadata": {},
   "source": [
    "The transformer model expects our labels to follow a common multi-label format of binaries, so let us use [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html) for this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7efcbc6-938c-4341-9e83-e8761974deda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736451640f374c94b957b1a3c76bec7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b271da18f44712b2b9c30258c65d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Turn labels into multi-label format\n",
    "mb = MultiLabelBinarizer()\n",
    "mb.fit(ds[\"test\"][\"label\"])\n",
    "\n",
    "\n",
    "def binarize_labels(examples):\n",
    "    return {\"label\": mb.transform(examples[\"label\"])}\n",
    "\n",
    "\n",
    "binarized_tokenized_ds = tokenized_ds.map(binarize_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e2cd7-50e4-4dd1-883f-f2e31d1ec051",
   "metadata": {},
   "source": [
    "Before we start the training, it is important to define our metric for the evaluation.\n",
    "Here we settle on the commonly used micro averaged *F1* metric, but we will also keep track of the *F1 per label*, for a more in-depth error analysis afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d50f9ae8-2b1a-4905-b19f-62bcfa8c64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "# Define our metrics\n",
    "metric = load_metric(\"f1\", config_name=\"multilabel\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # apply sigmoid\n",
    "    predictions = (1.0 / (1 + np.exp(-logits))) > 0.5\n",
    "\n",
    "    # f1 micro averaged\n",
    "    metrics = metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"micro\"\n",
    "    )\n",
    "    # f1 per label\n",
    "    per_label_metric = metric.compute(\n",
    "        predictions=predictions, references=labels, average=None\n",
    "    )\n",
    "    for label, f1 in zip(\n",
    "        ds[\"train\"].features[\"label\"][0].names, per_label_metric[\"f1\"]\n",
    "    ):\n",
    "        metrics[f\"f1_{label}\"] = f1\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf20aa-0117-438d-a960-59a85adb4d37",
   "metadata": {},
   "source": [
    "Now we are ready to load our pretrained transformer model and prepare it for our task: multi-label text classification with 6 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fa9ee91-82f0-4cb1-a1b6-10369804e22a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Init our down-stream model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", problem_type=\"multi_label_classification\", num_labels=6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29498e65-8edc-4086-8ab7-f2db90f8a113",
   "metadata": {},
   "source": [
    "The only thing missing for the training is the `Trainer` and its `TrainingArguments`.\n",
    "To keep it simple, we mostly rely on the default arguments, that often work out of the box, but tweak a bit the batch size to train faster. \n",
    "We also checked that 2 epochs are enough for our rather small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2dd0c0bd-b7d9-4e1c-a734-8c2a8e9d57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Set our training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61e0896c-dc7b-4844-95b6-a258aa8f8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Init the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=binarized_tokenized_ds[\"train\"],\n",
    "    eval_dataset=binarized_tokenized_ds[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09668b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ddd25cb-1cf8-4721-a8e8-c16eb0c7aaf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1417\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 178\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "2022-11-21 15:41:38.459 | ERROR    | wandb.jupyter:notebook_metadata:231 - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "# Train the down-stream model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77e8d7e-1463-42c8-96e2-13dfe98cc318",
   "metadata": {},
   "source": [
    "We achieved an micro averaged *F1* of abut 0.54, which is not perfect, but a good baseline for this challenging dataset.\n",
    "When inspecting the *F1s per label*, we clearly see that the worst performing labels are the ones with the poorest heuristics in terms of accuracy and coverage, which comes to no surprise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0dfbaa-90f3-4803-8e93-b86e1440bd51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Research topic dataset\n",
    "\n",
    "After covering a multi-label emotion classification task, we will try to do the same for a multi-label classification task related to topic modeling.\n",
    "In this dataset, research papers were classified with 6 non-exclusive labels based on their title and abstract.\n",
    "\n",
    "We will try to classify the papers only based on the title, which is considerably harder, but allows us to quickly scan through the data and come up with heuristics.\n",
    "See Appendix B for all the details of the minimal data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba65805-bfb2-4027-9231-28538a7873e8",
   "metadata": {},
   "source": [
    "### Define rules\n",
    "\n",
    "Let us start by downloading our preprocessed dataset from the Hugging Face Hub, and log it to Argilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4cdd38-604c-4fe6-8104-fc6b136d331a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503e1cd187a04bb6ab214479b85de70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 16:32:02.429 | WARNING  | datasets.builder:_create_builder_config:412 - Using custom data configuration argilla--research_titles_multi-label-b196580940b58959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 1.17 MiB, generated: 2.10 MiB, post-processed: Unknown size, total: 3.27 MiB) to C:\\Users\\ufukh\\.cache\\huggingface\\datasets\\argilla___parquet\\argilla--research_titles_multi-label-b196580940b58959\\0.0.0\\0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ec5e3b9e1844bca1c14bd5c59ac2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb7fdf14a284282bbee2adc6c804974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466fde84adac447e8d18c91ec936ad25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:\\Users\\ufukh\\.cache\\huggingface\\datasets\\argilla___parquet\\argilla--research_titles_multi-label-b196580940b58959\\0.0.0\\0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Download preprocessed dataset\n",
    "ds_rb = rg.read_datasets(\n",
    "    load_dataset(\"argilla/research_titles_multi-label\", split=\"train\"),\n",
    "    task=\"TextClassification\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34899478-62ae-4864-8196-36348aa568c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0ee48c98d748e8ab2cf161e4c9135e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20972 records logged to http://localhost:6900/datasets/argilla/research_titles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='research_titles', processed=20972, failed=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log dataset to Argilla to find good heuristics\n",
    "rg.log(ds_rb, \"research_titles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a95ac66-480b-4bf2-b35c-e548df16a084",
   "metadata": {},
   "source": [
    "After uploading the dataset, we can explore and inspect it to find good heuristic rules.\n",
    "For this we highly recommend the dedicated [*Define rules* mode](../../reference/webapp/features.html#weak-labelling) of the Argilla web app, that allows you to quickly iterate over heuristic rules, compute their metrics and save them.\n",
    "\n",
    "Here we copy our rules found via the web app to the notebook for you to easily follow along the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f85fd20-7086-4581-9078-2a28c9155997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.labeling.text_classification import Rule\n",
    "\n",
    "# Define our heuristic rules (can probably be improved)\n",
    "\n",
    "rules = [\n",
    "    Rule(\"stock*\", \"Quantitative Finance\"),\n",
    "    Rule(\"*asset*\", \"Quantitative Finance\"),\n",
    "    Rule(\"pric*\", \"Quantitative Finance\"),\n",
    "    Rule(\"economy\", \"Quantitative Finance\"),\n",
    "    Rule(\"deep AND neural AND network*\", \"Computer Science\"),\n",
    "    Rule(\"convolutional\", \"Computer Science\"),\n",
    "    Rule(\"allocat* AND *net*\", \"Computer Science\"),\n",
    "    Rule(\"program\", \"Computer Science\"),\n",
    "    Rule(\"classification* AND (label* OR deep)\", \"Computer Science\"),\n",
    "    Rule(\"scattering\", \"Physics\"),\n",
    "    Rule(\"astro*\", \"Physics\"),\n",
    "    Rule(\"optical\", \"Physics\"),\n",
    "    Rule(\"ray\", \"Physics\"),\n",
    "    Rule(\"entangle*\", \"Physics\"),\n",
    "    Rule(\"*algebra*\", \"Mathematics\"),\n",
    "    Rule(\"spaces\", \"Mathematics\"),\n",
    "    Rule(\"operators\", \"Mathematics\"),\n",
    "    Rule(\"estimation\", \"Statistics\"),\n",
    "    Rule(\"mixture\", \"Statistics\"),\n",
    "    Rule(\"gaussian\", \"Statistics\"),\n",
    "    Rule(\"gene\", \"Quantitative Biology\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93311e7a-4522-4036-9151-10edc1101d3d",
   "metadata": {},
   "source": [
    "We go on and apply these heuristic rules to our dataset creating our weak label matrix.\n",
    "As mentioned in the [GoEmotions](#goemotions) section, the weak label matrix will have 3 dimensions and values of -1, 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c41d2c90-e550-4d44-9935-b5eeea415c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf699a04c6c4885b90982d8f1a84a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing rules:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d6e2e21bee434a8baac2ae1d58bd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying rules:   0%|          | 0/20972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6ceabfe55c4ad4bb61aa627b0bb0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filling weak label matrix:   0%|          | 0/20972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from argilla.labeling.text_classification import WeakMultiLabels\n",
    "\n",
    "# Compute the weak labels for our dataset given the rules\n",
    "# If your dataset already contains rules you can omit the rules argument.\n",
    "\n",
    "\n",
    "add_rules(dataset=\"research_titles\", rules=rules)\n",
    "weak_labels = WeakMultiLabels(\"research_titles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a2585-58cc-4eae-b5da-e31e29fd188d",
   "metadata": {},
   "source": [
    "Let us get an overview of the our heuristics and how they perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5bfc002-0845-4d36-b2c5-8133995561ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>coverage</th>\n",
       "      <th>annotated_coverage</th>\n",
       "      <th>overlaps</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stock*</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>*asset*</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pric*</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economy</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep AND neural AND network*</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.009155</td>\n",
       "      <td>0.010250</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.744186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convolutional</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.010109</td>\n",
       "      <td>0.009297</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>0.820513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allocat* AND *net*</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>program</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classification* AND (label* OR deep)</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scattering</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>astro*</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optical</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.007105</td>\n",
       "      <td>0.006913</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0.931034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ray</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.007390</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>0.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entangle*</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>*algebra*</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.014829</td>\n",
       "      <td>0.018355</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>70</td>\n",
       "      <td>7</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spaces</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.010586</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operators</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.005959</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimation</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.021216</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>65</td>\n",
       "      <td>24</td>\n",
       "      <td>0.730337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixture</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gaussian</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>0.765957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene</th>\n",
       "      <td>{Quantitative Biology}</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>{Mathematics, Physics, Quantitative Finance, S...</td>\n",
       "      <td>0.111911</td>\n",
       "      <td>0.118951</td>\n",
       "      <td>0.008154</td>\n",
       "      <td>447</td>\n",
       "      <td>89</td>\n",
       "      <td>0.833955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  label  \\\n",
       "stock*                                                           {Quantitative Finance}   \n",
       "*asset*                                                          {Quantitative Finance}   \n",
       "pric*                                                            {Quantitative Finance}   \n",
       "economy                                                          {Quantitative Finance}   \n",
       "deep AND neural AND network*                                         {Computer Science}   \n",
       "convolutional                                                        {Computer Science}   \n",
       "allocat* AND *net*                                                   {Computer Science}   \n",
       "program                                                              {Computer Science}   \n",
       "classification* AND (label* OR deep)                                 {Computer Science}   \n",
       "scattering                                                                    {Physics}   \n",
       "astro*                                                                        {Physics}   \n",
       "optical                                                                       {Physics}   \n",
       "ray                                                                           {Physics}   \n",
       "entangle*                                                                     {Physics}   \n",
       "*algebra*                                                                 {Mathematics}   \n",
       "spaces                                                                    {Mathematics}   \n",
       "operators                                                                 {Mathematics}   \n",
       "estimation                                                                 {Statistics}   \n",
       "mixture                                                                    {Statistics}   \n",
       "gaussian                                                                   {Statistics}   \n",
       "gene                                                             {Quantitative Biology}   \n",
       "total                                 {Mathematics, Physics, Quantitative Finance, S...   \n",
       "\n",
       "                                      coverage  annotated_coverage  overlaps  \\\n",
       "stock*                                0.000954            0.000715  0.000191   \n",
       "*asset*                               0.000477            0.000715  0.000238   \n",
       "pric*                                 0.003433            0.003337  0.000668   \n",
       "economy                               0.000238            0.000238  0.000000   \n",
       "deep AND neural AND network*          0.009155            0.010250  0.002575   \n",
       "convolutional                         0.010109            0.009297  0.002003   \n",
       "allocat* AND *net*                    0.000763            0.000715  0.000000   \n",
       "program                               0.002623            0.003099  0.000095   \n",
       "classification* AND (label* OR deep)  0.003338            0.004052  0.001287   \n",
       "scattering                            0.004053            0.002861  0.000572   \n",
       "astro*                                0.003099            0.004052  0.000477   \n",
       "optical                               0.007105            0.006913  0.000811   \n",
       "ray                                   0.005865            0.007390  0.000668   \n",
       "entangle*                             0.002623            0.002861  0.000048   \n",
       "*algebra*                             0.014829            0.018355  0.000429   \n",
       "spaces                                0.010586            0.009774  0.001287   \n",
       "operators                             0.006151            0.005959  0.001192   \n",
       "estimation                            0.021266            0.021216  0.001621   \n",
       "mixture                               0.003290            0.003099  0.000906   \n",
       "gaussian                              0.009250            0.011204  0.001526   \n",
       "gene                                  0.001287            0.001669  0.000143   \n",
       "total                                 0.111911            0.118951  0.008154   \n",
       "\n",
       "                                      correct  incorrect  precision  \n",
       "stock*                                      3          0   1.000000  \n",
       "*asset*                                     3          0   1.000000  \n",
       "pric*                                       9          5   0.642857  \n",
       "economy                                     1          0   1.000000  \n",
       "deep AND neural AND network*               32         11   0.744186  \n",
       "convolutional                              32          7   0.820513  \n",
       "allocat* AND *net*                          3          0   1.000000  \n",
       "program                                    11          2   0.846154  \n",
       "classification* AND (label* OR deep)       14          3   0.823529  \n",
       "scattering                                 10          2   0.833333  \n",
       "astro*                                     17          0   1.000000  \n",
       "optical                                    27          2   0.931034  \n",
       "ray                                        27          4   0.870968  \n",
       "entangle*                                  11          1   0.916667  \n",
       "*algebra*                                  70          7   0.909091  \n",
       "spaces                                     38          3   0.926829  \n",
       "operators                                  22          3   0.880000  \n",
       "estimation                                 65         24   0.730337  \n",
       "mixture                                    10          3   0.769231  \n",
       "gaussian                                   36         11   0.765957  \n",
       "gene                                        6          1   0.857143  \n",
       "total                                     447         89   0.833955  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check coverage/precision of our rules\n",
    "weak_labels.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ed61a",
   "metadata": {},
   "source": [
    "Consider the case we have come up with new rules and want to add them to dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "948d2058",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_rules = [\n",
    "    Rule(\"trading\", \"Quantitative Finance\"),\n",
    "    Rule(\"finance\", \"Quantitative Finance\"),\n",
    "    Rule(\"memor* AND (design* OR network*)\", \"Computer Science\"),\n",
    "    Rule(\"system* AND design*\", \"Computer Science\"),\n",
    "    Rule(\"material*\", \"Physics\"),\n",
    "    Rule(\"spin\", \"Physics\"),\n",
    "    Rule(\"magnetic\", \"Physics\"),\n",
    "    Rule(\"manifold* AND (NOT learn*)\", \"Mathematics\"),\n",
    "    Rule(\"equation\", \"Mathematics\"),\n",
    "    Rule(\"regression\", \"Statistics\"),\n",
    "    Rule(\"bayes*\", \"Statistics\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae80efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_rules(dataset=\"research_titles\", rules=additional_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0cc85ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa971d152e3942f4937d062591eae1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing rules:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab23ccb70644fc794580c7595b19403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying rules:   0%|          | 0/20972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce8ba54095148258977a579ac66703c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filling weak label matrix:   0%|          | 0/20972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weak_labels = WeakMultiLabels(\"research_titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09d35bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>coverage</th>\n",
       "      <th>annotated_coverage</th>\n",
       "      <th>overlaps</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stock*</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>*asset*</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pric*</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economy</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep AND neural AND network*</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.009155</td>\n",
       "      <td>0.010250</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.744186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convolutional</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.010109</td>\n",
       "      <td>0.009297</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>0.820513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allocat* AND *net*</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>program</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classification* AND (label* OR deep)</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scattering</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>astro*</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optical</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.007105</td>\n",
       "      <td>0.006913</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0.931034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ray</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.007390</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>0.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entangle*</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>*algebra*</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.014829</td>\n",
       "      <td>0.018355</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>70</td>\n",
       "      <td>7</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spaces</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.010586</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operators</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.005959</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimation</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.021216</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>65</td>\n",
       "      <td>24</td>\n",
       "      <td>0.730337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixture</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gaussian</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>0.765957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene</th>\n",
       "      <td>{Quantitative Biology}</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trading</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memor* AND (design* OR network*)</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>system* AND design*</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>material*</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spin</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>0.015018</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magnetic</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.011301</td>\n",
       "      <td>0.012872</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>0.907407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manifold* AND (NOT learn*)</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.007057</td>\n",
       "      <td>0.008343</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equation</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.010681</td>\n",
       "      <td>0.007867</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regression</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.868421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bayes*</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.015306</td>\n",
       "      <td>0.014779</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>{Mathematics, Physics, Quantitative Finance, S...</td>\n",
       "      <td>0.176616</td>\n",
       "      <td>0.185936</td>\n",
       "      <td>0.017833</td>\n",
       "      <td>720</td>\n",
       "      <td>135</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  label  \\\n",
       "stock*                                                           {Quantitative Finance}   \n",
       "*asset*                                                          {Quantitative Finance}   \n",
       "pric*                                                            {Quantitative Finance}   \n",
       "economy                                                          {Quantitative Finance}   \n",
       "deep AND neural AND network*                                         {Computer Science}   \n",
       "convolutional                                                        {Computer Science}   \n",
       "allocat* AND *net*                                                   {Computer Science}   \n",
       "program                                                              {Computer Science}   \n",
       "classification* AND (label* OR deep)                                 {Computer Science}   \n",
       "scattering                                                                    {Physics}   \n",
       "astro*                                                                        {Physics}   \n",
       "optical                                                                       {Physics}   \n",
       "ray                                                                           {Physics}   \n",
       "entangle*                                                                     {Physics}   \n",
       "*algebra*                                                                 {Mathematics}   \n",
       "spaces                                                                    {Mathematics}   \n",
       "operators                                                                 {Mathematics}   \n",
       "estimation                                                                 {Statistics}   \n",
       "mixture                                                                    {Statistics}   \n",
       "gaussian                                                                   {Statistics}   \n",
       "gene                                                             {Quantitative Biology}   \n",
       "trading                                                          {Quantitative Finance}   \n",
       "finance                                                          {Quantitative Finance}   \n",
       "memor* AND (design* OR network*)                                     {Computer Science}   \n",
       "system* AND design*                                                  {Computer Science}   \n",
       "material*                                                                     {Physics}   \n",
       "spin                                                                          {Physics}   \n",
       "magnetic                                                                      {Physics}   \n",
       "manifold* AND (NOT learn*)                                                {Mathematics}   \n",
       "equation                                                                  {Mathematics}   \n",
       "regression                                                                 {Statistics}   \n",
       "bayes*                                                                     {Statistics}   \n",
       "total                                 {Mathematics, Physics, Quantitative Finance, S...   \n",
       "\n",
       "                                      coverage  annotated_coverage  overlaps  \\\n",
       "stock*                                0.000954            0.000715  0.000334   \n",
       "*asset*                               0.000477            0.000715  0.000286   \n",
       "pric*                                 0.003433            0.003337  0.000715   \n",
       "economy                               0.000238            0.000238  0.000000   \n",
       "deep AND neural AND network*          0.009155            0.010250  0.002909   \n",
       "convolutional                         0.010109            0.009297  0.002241   \n",
       "allocat* AND *net*                    0.000763            0.000715  0.000000   \n",
       "program                               0.002623            0.003099  0.000143   \n",
       "classification* AND (label* OR deep)  0.003338            0.004052  0.001335   \n",
       "scattering                            0.004053            0.002861  0.001001   \n",
       "astro*                                0.003099            0.004052  0.000620   \n",
       "optical                               0.007105            0.006913  0.001097   \n",
       "ray                                   0.005865            0.007390  0.001192   \n",
       "entangle*                             0.002623            0.002861  0.000095   \n",
       "*algebra*                             0.014829            0.018355  0.000620   \n",
       "spaces                                0.010586            0.009774  0.001860   \n",
       "operators                             0.006151            0.005959  0.001526   \n",
       "estimation                            0.021266            0.021216  0.003385   \n",
       "mixture                               0.003290            0.003099  0.001287   \n",
       "gaussian                              0.009250            0.011204  0.002766   \n",
       "gene                                  0.001287            0.001669  0.000191   \n",
       "trading                               0.000954            0.000238  0.000191   \n",
       "finance                               0.000048            0.000238  0.000000   \n",
       "memor* AND (design* OR network*)      0.001383            0.002145  0.000286   \n",
       "system* AND design*                   0.001144            0.002384  0.000238   \n",
       "material*                             0.004148            0.003099  0.000238   \n",
       "spin                                  0.013542            0.015018  0.002146   \n",
       "magnetic                              0.011301            0.012872  0.002432   \n",
       "manifold* AND (NOT learn*)            0.007057            0.008343  0.000858   \n",
       "equation                              0.010681            0.007867  0.000954   \n",
       "regression                            0.009393            0.009058  0.002575   \n",
       "bayes*                                0.015306            0.014779  0.003147   \n",
       "total                                 0.176616            0.185936  0.017833   \n",
       "\n",
       "                                      correct  incorrect  precision  \n",
       "stock*                                      3          0   1.000000  \n",
       "*asset*                                     3          0   1.000000  \n",
       "pric*                                       9          5   0.642857  \n",
       "economy                                     1          0   1.000000  \n",
       "deep AND neural AND network*               32         11   0.744186  \n",
       "convolutional                              32          7   0.820513  \n",
       "allocat* AND *net*                          3          0   1.000000  \n",
       "program                                    11          2   0.846154  \n",
       "classification* AND (label* OR deep)       14          3   0.823529  \n",
       "scattering                                 10          2   0.833333  \n",
       "astro*                                     17          0   1.000000  \n",
       "optical                                    27          2   0.931034  \n",
       "ray                                        27          4   0.870968  \n",
       "entangle*                                  11          1   0.916667  \n",
       "*algebra*                                  70          7   0.909091  \n",
       "spaces                                     38          3   0.926829  \n",
       "operators                                  22          3   0.880000  \n",
       "estimation                                 65         24   0.730337  \n",
       "mixture                                    10          3   0.769231  \n",
       "gaussian                                   36         11   0.765957  \n",
       "gene                                        6          1   0.857143  \n",
       "trading                                     1          0   1.000000  \n",
       "finance                                     1          0   1.000000  \n",
       "memor* AND (design* OR network*)            9          0   1.000000  \n",
       "system* AND design*                         9          1   0.900000  \n",
       "material*                                  10          3   0.769231  \n",
       "spin                                       60          3   0.952381  \n",
       "magnetic                                   49          5   0.907407  \n",
       "manifold* AND (NOT learn*)                 28          7   0.800000  \n",
       "equation                                   24          9   0.727273  \n",
       "regression                                 33          5   0.868421  \n",
       "bayes*                                     49         13   0.790323  \n",
       "total                                     720        135   0.842105  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08afaae4",
   "metadata": {},
   "source": [
    "Let's create new rules and see their affects, if they are informative enough we can proceed by adding them to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b3f483c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 0.004672897196261682,\n",
       " 'annotated_coverage': 0.004529201430274136,\n",
       " 'correct': 17,\n",
       " 'incorrect': 2,\n",
       " 'precision': 0.8947368421052632}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a statistics rule and get its metrics\n",
    "statistics_rule = Rule(\"sample\", \"Statistics\")\n",
    "statistics_rule.apply(\"research_titles\")\n",
    "statistics_rule.metrics(\"research_titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b23accdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 0.004815945069616631,\n",
       " 'annotated_coverage': 0.004290822407628129,\n",
       " 'correct': 1,\n",
       " 'incorrect': 17,\n",
       " 'precision': 0.05555555555555555}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_rule = Rule(\"risk\", \"Quantitative Finance\")\n",
    "finance_rule.apply(\"research_titles\")\n",
    "finance_rule.metrics(\"research_titles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5533291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_rule.add_to_dataset(\"research_titles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d572a",
   "metadata": {},
   "source": [
    "Our assertion does not seem correct lets update this rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff59d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_rule =  Rule(\"risk\", \"Statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "83f689f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 0.004815945069616631,\n",
       " 'annotated_coverage': 0.004290822407628129,\n",
       " 'correct': 11,\n",
       " 'incorrect': 7,\n",
       " 'precision': 0.6111111111111112}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_rule.metrics(\"research_titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4364f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_rule.update_at_dataset(\"research_titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3888f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_rule.add_to_dataset(\"research_titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0df8ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative_biology_rule = Rule(\"dna\", \"Quantitative Biology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0949eb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 0.0013351134846461949,\n",
       " 'annotated_coverage': 0.0011918951132300357,\n",
       " 'correct': 4,\n",
       " 'incorrect': 1,\n",
       " 'precision': 0.8}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantitative_biology_rule.metrics(\"research_titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "170b7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative_biology_rule.add_to_dataset(\"research_titles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722fa8f9",
   "metadata": {},
   "source": [
    "Lets see the final matrix with new added rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "204ddcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a30d8bac5704404a2bff915b753e2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing rules:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a959ba567df4681a083f976da47a5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying rules:   0%|          | 0/20972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f14cf104ec4f3296054f41353810c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filling weak label matrix:   0%|          | 0/20972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weak_labels = WeakMultiLabels(\"research_titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eece91f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>coverage</th>\n",
       "      <th>annotated_coverage</th>\n",
       "      <th>overlaps</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stock*</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>*asset*</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pric*</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economy</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep AND neural AND network*</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.009155</td>\n",
       "      <td>0.010250</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.744186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convolutional</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.010109</td>\n",
       "      <td>0.009297</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>0.820513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allocat* AND *net*</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>program</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classification* AND (label* OR deep)</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scattering</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>astro*</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optical</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.007105</td>\n",
       "      <td>0.006913</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0.931034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ray</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.007390</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>0.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entangle*</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>*algebra*</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.014829</td>\n",
       "      <td>0.018355</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>70</td>\n",
       "      <td>7</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spaces</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.010586</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operators</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.005959</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimation</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.021216</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>65</td>\n",
       "      <td>24</td>\n",
       "      <td>0.730337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixture</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gaussian</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>0.765957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene</th>\n",
       "      <td>{Quantitative Biology}</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trading</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>{Quantitative Finance}</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memor* AND (design* OR network*)</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>system* AND design*</th>\n",
       "      <td>{Computer Science}</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>material*</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spin</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>0.015018</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magnetic</th>\n",
       "      <td>{Physics}</td>\n",
       "      <td>0.011301</td>\n",
       "      <td>0.012872</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>0.907407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manifold* AND (NOT learn*)</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.007057</td>\n",
       "      <td>0.008343</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equation</th>\n",
       "      <td>{Mathematics}</td>\n",
       "      <td>0.010681</td>\n",
       "      <td>0.007867</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regression</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.868421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bayes*</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.015306</td>\n",
       "      <td>0.014779</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>risk</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.004291</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample</th>\n",
       "      <td>{Statistics}</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.004529</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dna</th>\n",
       "      <td>{Quantitative Biology}</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>{Mathematics, Physics, Quantitative Finance, S...</td>\n",
       "      <td>0.185390</td>\n",
       "      <td>0.194041</td>\n",
       "      <td>0.019788</td>\n",
       "      <td>752</td>\n",
       "      <td>145</td>\n",
       "      <td>0.838350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  label  \\\n",
       "stock*                                                           {Quantitative Finance}   \n",
       "*asset*                                                          {Quantitative Finance}   \n",
       "pric*                                                            {Quantitative Finance}   \n",
       "economy                                                          {Quantitative Finance}   \n",
       "deep AND neural AND network*                                         {Computer Science}   \n",
       "convolutional                                                        {Computer Science}   \n",
       "allocat* AND *net*                                                   {Computer Science}   \n",
       "program                                                              {Computer Science}   \n",
       "classification* AND (label* OR deep)                                 {Computer Science}   \n",
       "scattering                                                                    {Physics}   \n",
       "astro*                                                                        {Physics}   \n",
       "optical                                                                       {Physics}   \n",
       "ray                                                                           {Physics}   \n",
       "entangle*                                                                     {Physics}   \n",
       "*algebra*                                                                 {Mathematics}   \n",
       "spaces                                                                    {Mathematics}   \n",
       "operators                                                                 {Mathematics}   \n",
       "estimation                                                                 {Statistics}   \n",
       "mixture                                                                    {Statistics}   \n",
       "gaussian                                                                   {Statistics}   \n",
       "gene                                                             {Quantitative Biology}   \n",
       "trading                                                          {Quantitative Finance}   \n",
       "finance                                                          {Quantitative Finance}   \n",
       "memor* AND (design* OR network*)                                     {Computer Science}   \n",
       "system* AND design*                                                  {Computer Science}   \n",
       "material*                                                                     {Physics}   \n",
       "spin                                                                          {Physics}   \n",
       "magnetic                                                                      {Physics}   \n",
       "manifold* AND (NOT learn*)                                                {Mathematics}   \n",
       "equation                                                                  {Mathematics}   \n",
       "regression                                                                 {Statistics}   \n",
       "bayes*                                                                     {Statistics}   \n",
       "risk                                                                       {Statistics}   \n",
       "sample                                                                     {Statistics}   \n",
       "dna                                                              {Quantitative Biology}   \n",
       "total                                 {Mathematics, Physics, Quantitative Finance, S...   \n",
       "\n",
       "                                      coverage  annotated_coverage  overlaps  \\\n",
       "stock*                                0.000954            0.000715  0.000334   \n",
       "*asset*                               0.000477            0.000715  0.000334   \n",
       "pric*                                 0.003433            0.003337  0.000811   \n",
       "economy                               0.000238            0.000238  0.000048   \n",
       "deep AND neural AND network*          0.009155            0.010250  0.002956   \n",
       "convolutional                         0.010109            0.009297  0.002336   \n",
       "allocat* AND *net*                    0.000763            0.000715  0.000048   \n",
       "program                               0.002623            0.003099  0.000191   \n",
       "classification* AND (label* OR deep)  0.003338            0.004052  0.001335   \n",
       "scattering                            0.004053            0.002861  0.001049   \n",
       "astro*                                0.003099            0.004052  0.000668   \n",
       "optical                               0.007105            0.006913  0.001097   \n",
       "ray                                   0.005865            0.007390  0.001240   \n",
       "entangle*                             0.002623            0.002861  0.000095   \n",
       "*algebra*                             0.014829            0.018355  0.000620   \n",
       "spaces                                0.010586            0.009774  0.001860   \n",
       "operators                             0.006151            0.005959  0.001574   \n",
       "estimation                            0.021266            0.021216  0.003862   \n",
       "mixture                               0.003290            0.003099  0.001335   \n",
       "gaussian                              0.009250            0.011204  0.003052   \n",
       "gene                                  0.001287            0.001669  0.000191   \n",
       "trading                               0.000954            0.000238  0.000191   \n",
       "finance                               0.000048            0.000238  0.000000   \n",
       "memor* AND (design* OR network*)      0.001383            0.002145  0.000286   \n",
       "system* AND design*                   0.001144            0.002384  0.000238   \n",
       "material*                             0.004148            0.003099  0.000238   \n",
       "spin                                  0.013542            0.015018  0.002146   \n",
       "magnetic                              0.011301            0.012872  0.002432   \n",
       "manifold* AND (NOT learn*)            0.007057            0.008343  0.000858   \n",
       "equation                              0.010681            0.007867  0.001001   \n",
       "regression                            0.009393            0.009058  0.002718   \n",
       "bayes*                                0.015306            0.014779  0.003481   \n",
       "risk                                  0.004816            0.004291  0.001097   \n",
       "sample                                0.004673            0.004529  0.000811   \n",
       "dna                                   0.001335            0.001192  0.000143   \n",
       "total                                 0.185390            0.194041  0.019788   \n",
       "\n",
       "                                      correct  incorrect  precision  \n",
       "stock*                                      3          0   1.000000  \n",
       "*asset*                                     3          0   1.000000  \n",
       "pric*                                       9          5   0.642857  \n",
       "economy                                     1          0   1.000000  \n",
       "deep AND neural AND network*               32         11   0.744186  \n",
       "convolutional                              32          7   0.820513  \n",
       "allocat* AND *net*                          3          0   1.000000  \n",
       "program                                    11          2   0.846154  \n",
       "classification* AND (label* OR deep)       14          3   0.823529  \n",
       "scattering                                 10          2   0.833333  \n",
       "astro*                                     17          0   1.000000  \n",
       "optical                                    27          2   0.931034  \n",
       "ray                                        27          4   0.870968  \n",
       "entangle*                                  11          1   0.916667  \n",
       "*algebra*                                  70          7   0.909091  \n",
       "spaces                                     38          3   0.926829  \n",
       "operators                                  22          3   0.880000  \n",
       "estimation                                 65         24   0.730337  \n",
       "mixture                                    10          3   0.769231  \n",
       "gaussian                                   36         11   0.765957  \n",
       "gene                                        6          1   0.857143  \n",
       "trading                                     1          0   1.000000  \n",
       "finance                                     1          0   1.000000  \n",
       "memor* AND (design* OR network*)            9          0   1.000000  \n",
       "system* AND design*                         9          1   0.900000  \n",
       "material*                                  10          3   0.769231  \n",
       "spin                                       60          3   0.952381  \n",
       "magnetic                                   49          5   0.907407  \n",
       "manifold* AND (NOT learn*)                 28          7   0.800000  \n",
       "equation                                   24          9   0.727273  \n",
       "regression                                 33          5   0.868421  \n",
       "bayes*                                     49         13   0.790323  \n",
       "risk                                       11          7   0.611111  \n",
       "sample                                     17          2   0.894737  \n",
       "dna                                         4          1   0.800000  \n",
       "total                                     752        145   0.838350  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd5c07-7e51-4c3f-bd7d-c6bbec5194c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create training set\n",
    "\n",
    "When we are happy with our heuristics, it is time to combine them and compute weak labels for the training of our downstream model.\n",
    "As for the \"GoEmotions\" dataset, we will use the simple `MajorityVoter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8152fabd-969d-40b1-a4fc-956f8783ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.labeling.text_classification import MajorityVoter\n",
    "\n",
    "# Use the majority voter as the label model\n",
    "label_model = MajorityVoter(weak_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda9587-7531-4780-9bc9-489f7e6d7523",
   "metadata": {},
   "source": [
    "From our label model we get the training records together with its weak labels and probabilities.\n",
    "Since we are going to train an sklearn model, we will put the records in a pandas DataFrame that generally has a good integration with the sklearn ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7edc676-5c9a-4b21-82d6-1a820b466483",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = label_model.predict().to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f7528-a499-49aa-a851-f41779b099df",
   "metadata": {},
   "source": [
    "Before training our model, we need to extract the training labels from the label model predictions and transform them into a multi-label compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f45f92a3-b9a8-482a-9f37-52e4802790b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels in multi-label format, we will use a threshold of 0.5 for the probability\n",
    "def multi_label_binarizer(predictions, threshold=0.5):\n",
    "    predicted_labels = [label for label, prob in predictions if prob > threshold]\n",
    "    binary_labels = [\n",
    "        1 if label in predicted_labels else 0 for label in weak_labels.labels\n",
    "    ]\n",
    "    return binary_labels\n",
    "\n",
    "\n",
    "train_df[\"label\"] = train_df.prediction.map(multi_label_binarizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976259c-15b6-4d61-b4e3-640f5a4282b2",
   "metadata": {},
   "source": [
    "Now, let us define our downstream model and train it.\n",
    "\n",
    "We will use the [scikit-multilearn library](http://scikit.ml/) to wrap a multinomial **Naive Bayes classifier** that is suitable for classification with discrete features (e.g., word counts for text classification).\n",
    "The `BinaryRelevance` class transforms the multi-label problem with L labels into L single-label binary classification problems, so in the end we will automatically fit L naive bayes classifiers to our data.\n",
    "\n",
    "The features for our classifier will be the counts of different word [n-grams](https://en.wikipedia.org/wiki/N-gram): that is, for each example we count the number of contiguous sequences of *n* words, where n goes from 1 to 5.\n",
    "We extract these features with the `CountVectorizer`.\n",
    "\n",
    "Finally, we will put our feature extractor and multi-label classifier in a sklearn pipeline that makes fitting and scoring the model a breeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7f2a5d4b-4d6a-4dc0-8533-287f92c745f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define our down-stream model\n",
    "classifier = Pipeline(\n",
    "    [(\"vect\", CountVectorizer()), (\"clf\", BinaryRelevance(MultinomialNB()))]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb281796-cce4-40b8-b583-ae2574fde3e7",
   "metadata": {},
   "source": [
    "Training the model is as easy as calling the `fit` method on the our pipeline, and provide our training text and training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "019aae12-aab3-4d9c-9695-80018541c3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 BinaryRelevance(classifier=MultinomialNB(),\n",
       "                                 require_dense=[True, True]))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 BinaryRelevance(classifier=MultinomialNB(),\n",
       "                                 require_dense=[True, True]))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">clf: BinaryRelevance</label><div class=\"sk-toggleable__content\"><pre>BinaryRelevance(classifier=MultinomialNB(), require_dense=[True, True])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()),\n",
       "                ('clf',\n",
       "                 BinaryRelevance(classifier=MultinomialNB(),\n",
       "                                 require_dense=[True, True]))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fit the down-stream classifier\n",
    "classifier.fit(\n",
    "    X=train_df.text,\n",
    "    y=np.array(train_df.label.tolist()),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb69af9-8edc-46f8-80c9-b842d99986d2",
   "metadata": {},
   "source": [
    "To score our trained model, we retrieve its predictions of the test set and use sklearn's `classification_report` to get all important classification metrics in a nicely formatted string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "858f1c6e-99df-4918-803c-18647e2edd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for test set\n",
    "predictions = classifier.predict(\n",
    "    X=[rec.text for rec in weak_labels.records(has_annotation=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8b12a128-a184-494d-a926-6100a9d252da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Computer Science       0.81      0.24      0.38      1740\n",
      "         Mathematics       0.79      0.58      0.67      1141\n",
      "             Physics       0.88      0.65      0.74      1186\n",
      "Quantitative Biology       0.67      0.02      0.04       109\n",
      "Quantitative Finance       0.46      0.13      0.21        45\n",
      "          Statistics       0.52      0.69      0.60      1069\n",
      "\n",
      "           micro avg       0.71      0.49      0.58      5290\n",
      "           macro avg       0.69      0.39      0.44      5290\n",
      "        weighted avg       0.76      0.49      0.56      5290\n",
      "         samples avg       0.58      0.52      0.53      5290\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ufukh\\anaconda3\\envs\\argilla\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Compute metrics\n",
    "print(\n",
    "    classification_report(\n",
    "        weak_labels.annotation(), predictions, target_names=weak_labels.labels\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8a43e-0405-4d45-b117-aff8865ed305",
   "metadata": {},
   "source": [
    "We obtain a micro averaged F1 score of around 0.59, which again is not perfect but can serve as a decent baseline for future improvements.\n",
    "Looking at the F1 per label, we see that the main problem is the recall of our heuristics and we should either define more of them, or try to find more general ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e81db-da5d-49ff-ab1a-dc8749b8783d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial we saw how you can use *Argilla* to tackle multi-label text classification problems with weak supervision.\n",
    "We showed you how to train two downstream models on two different multi-label datasets using the discovered heuristics.\n",
    "\n",
    "For the emotion classification task, we trained a full-blown transformer model with Hugging Face, while for the topic classification task, we relied on a more lightweight Bayes classifier from sklearn.\n",
    "Although the results are not perfect, they can serve as a good baseline for future improvements.\n",
    "\n",
    "So the next time you encounter a multi-label classification problem, maybe try out weak supervision with *Argilla* and save some time for your annotation team ðŸ˜€."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b7565-f21a-48c8-99ce-278450a2705b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda0f2f-683f-4820-8516-dd12e58bd321",
   "metadata": {},
   "source": [
    "â­ Star Argilla [Github repo](https://github.com/argilla-io/argilla) to stay updated.\n",
    "\n",
    "ðŸ“š [Argilla documentation](https://docs.argilla.io) for more guides and tutorials.\n",
    "\n",
    "ðŸ™‹â€â™€ï¸ Join the Argilla community! A good place to start is the [discussion forum](https://github.com/argilla-io/argilla/discussions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c082b-38c7-4f3a-94a7-ab0cdbc5acbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Appendix A\n",
    "\n",
    "This appendix summarizes the preprocessing steps for our curated *GoEmotions* dataset.\n",
    "The goal was to limit the labels, and down-sample single-label annotations to move the focus to multi-label outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16336e27-c99c-4a51-b81b-bc63fb26daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original dataset and check label frequencies\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "go_emotions = datasets.load_dataset(\"go_emotions\")\n",
    "df = go_emotions[\"test\"].to_pandas()\n",
    "\n",
    "\n",
    "def int2str(i):\n",
    "    # return int(i)\n",
    "    return go_emotions[\"train\"].features[\"labels\"].feature.int2str(int(i))\n",
    "\n",
    "\n",
    "label_freq = []\n",
    "idx_multi = df.labels.map(lambda x: len(x) > 1)\n",
    "df[\"is_single\"] = df.labels.map(lambda x: 0 if len(x) > 1 else 1)\n",
    "df[idx_multi].labels.map(lambda x: [label_freq.append(int(l)) for l in x])\n",
    "pd.Series(label_freq).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b129f2-b92c-4ac7-a2aa-e6601dc9404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit labels, down-sample single-label annotations and create Argilla records\n",
    "\n",
    "import argilla as rg\n",
    "\n",
    "\n",
    "def create(split: str) -> pd.DataFrame:\n",
    "    df = go_emotions[split].to_pandas()\n",
    "    df[\"is_single\"] = df.labels.map(lambda x: 0 if len(x) > 1 else 1)\n",
    "\n",
    "    # ['admiration', 'approval', 'annoyance', 'gratitude', 'curiosity', 'optimism', 'amusement']\n",
    "    idx_most_common = df.labels.map(\n",
    "        lambda x: all([int(label) in [0, 4, 3, 15, 7, 15, 20] for label in x])\n",
    "    )\n",
    "    df_multi = df[(df.is_single == 0) & idx_most_common]\n",
    "    df_single = df[idx_most_common].sample(\n",
    "        3 * len(df_multi), weights=\"is_single\", axis=0, random_state=42\n",
    "    )\n",
    "    return pd.concat([df_multi, df_single]).sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def make_records(row, is_train: bool) -> rg.TextClassificationRecord:\n",
    "    annotation = [int2str(i) for i in row.labels] if not is_train else None\n",
    "    return rg.TextClassificationRecord(\n",
    "        inputs=row.text,\n",
    "        annotation=annotation,\n",
    "        multi_label=True,\n",
    "        id=row.id,\n",
    "    )\n",
    "\n",
    "\n",
    "train_recs = create(\"train\").apply(make_records, axis=1, is_train=True)\n",
    "test_recs = create(\"test\").apply(make_records, axis=1, is_train=False)\n",
    "\n",
    "records = train_recs.to_list() + test_recs.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f854543-63ea-4b52-bf17-cf33ec604e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish dataset in the Hub\n",
    "\n",
    "ds_rg = rg.DatasetForTextClassification(records).to_datasets()\n",
    "\n",
    "ds_rg.push_to_hub(\"argilla/go_emotions_multi-label\", private=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1de2b1-97be-4372-8919-2dfc422a86b1",
   "metadata": {},
   "source": [
    "## Appendix B\n",
    "\n",
    "This appendix summarizes the minimal preprocessing done to [this multi-label classification dataset](https://www.kaggle.com/shivanandmn/multilabel-classification-dataset) from Kaggle.\n",
    "You can download the original data (`train.csv`) following the Kaggle link.\n",
    "\n",
    "The preprocessing consists of extracting only the title from the research paper, and split the data into a train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ac8e3-b809-430a-941a-ac0d5fb36446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the title and split the data\n",
    "\n",
    "import pandas as pd\n",
    "import argilla as rg\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "_, test_id = train_test_split(df.ID, test_size=0.2, random_state=42)\n",
    "\n",
    "labels = [\n",
    "    \"Computer Science\",\n",
    "    \"Physics\",\n",
    "    \"Mathematics\",\n",
    "    \"Statistics\",\n",
    "    \"Quantitative Biology\",\n",
    "    \"Quantitative Finance\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_record(row):\n",
    "    annotation = [label for label in labels if row[label] == 1]\n",
    "    return rg.TextClassificationRecord(\n",
    "        inputs=row.TITLE,\n",
    "        # inputs={\"title\": row.TITLE, \"abstract\": row.ABSTRACT},\n",
    "        annotation=annotation if row.ID in test_id else None,\n",
    "        multi_label=True,\n",
    "        id=row.ID,\n",
    "    )\n",
    "\n",
    "\n",
    "records = df.apply(make_record, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a6b98-d0a1-459e-b5bd-3514d5692d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish the dataset in the Hub\n",
    "\n",
    "dataset_rg = rg.DatasetForTextClassification(records.tolist())\n",
    "\n",
    "dataset_rg.to_datasets().push_to_hub(\"argilla/research_titles_multi-label\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('argilla')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "83e13ff0de9ea08cace169d1016bf08ce368842307fd88824f08736a0a9ca04b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
