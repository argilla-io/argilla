{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a77a750-072d-4dae-afb8-d725e21be584",
   "metadata": {},
   "source": [
    "# üì∞ Building a news classifier with weak supervision\n",
    "\n",
    "In this tutorial, we will build a news classifier using rules and weak supervision: \n",
    "\n",
    "- üì∞ For this example, we use the AG News dataset but you can follow this process to programmatically label any dataset.\n",
    "- ü§ø The train split without labels is used to build a training set with rules, Argilla and Snorkel's Label model.\n",
    "- üîß The test set is used for evaluating our weak labels, label model and downstream news classifier.\n",
    "- ü§Ø We achieve 0.82 macro avg. f1-score without using a single example from the original dataset and using a pretty lightweight model (scikit-learn's `MultinomialNB`).\n",
    "\n",
    "The following diagram shows the overall process for using Weak supervision with Argilla:\n",
    "\n",
    "![labelling-textclassification-snorkel-weaksupervision](../../_static/tutorials/labelling-textclassification-snorkel-weaksupervision/labelling-textclassification-snorkel-weaksupervision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b144d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "> *Weak supervision is a branch of machine learning where noisy, limited, or imprecise sources are used to provide supervision signal for labeling large amounts of training data in a supervised learning setting. This approach alleviates the burden of obtaining hand-labeled data sets, which can be costly or impractical. Instead, inexpensive weak labels are employed with the understanding that they are imperfect, but can nonetheless be used to create a strong predictive model.* [[Wikipedia]](https://en.wikipedia.org/wiki/Weak_supervision)\n",
    "\n",
    "For a broader introduction to weak supervision, as well as further references, we recommend the excellent [overview by Alex Ratner et al.](https://www.snorkel.org/blog/weak-supervision).\n",
    "\n",
    "This tutorial aims to be a practical introduction to weak supervision and will walk you through its entire process.\n",
    "First we will generate weak labels with *Argilla*, combine these labels with *Snorkel*, and finally train a classifier with *Scikit Learn*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90076262",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this tutorial we also need some third party libraries that can be installed via pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54886a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%pip install snorkel datasets sklearn -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f8cf20",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "If you want to skip the first three sections of this tutorial, and only prepare the training set and train a downstream model, you can load the records directly from the [Hugging Face Hub](https://huggingface.co/datasets):\n",
    "\n",
    "```python\n",
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "\n",
    "# this replaces the `records = label_model.predict()` line of section 4\n",
    "records = rg.read_datasets(\n",
    "    load_dataset(\"argilla/news\", split=\"train\"),\n",
    "    task=\"TextClassification\",\n",
    ")\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ef86e-26e1-4cdc-b05b-60605bf13e25",
   "metadata": {},
   "source": [
    "## 1. Load test and unlabelled datasets into Argilla\n",
    "\n",
    "First, let's download the `ag_news` data set and have a quick look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa9c430-201b-40ce-94e7-37941d0d4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load our data\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# get the index to label mapping\n",
    "labels = dataset[\"test\"].features[\"label\"].names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e3399-6fad-483f-9f9c-7011a4793a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# quick look at our data\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(dataset[\"test\"].to_pandas().head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770a307-34e8-4d08-9001-526a19d3a169",
   "metadata": {},
   "source": [
    "Now we will log the test split of our data set to *Argilla*, which we will be using for testing our label and downstream models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f033bb-a731-47b5-abcb-d9ea913b20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# build our test records\n",
    "records = [\n",
    "    rg.TextClassificationRecord(\n",
    "        text=record[\"text\"],\n",
    "        metadata={\"split\": \"test\"},\n",
    "        annotation=labels[record[\"label\"]],\n",
    "    )\n",
    "    for record in dataset[\"test\"]\n",
    "]\n",
    "\n",
    "# log the records to Argilla\n",
    "rg.log(records, name=\"news\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57bf497-2a75-4c79-9487-b3a84602bbea",
   "metadata": {},
   "source": [
    "In a second step we log the train split without labels.\n",
    "Remember, our goal is to programmatically build a training set using rules and weak supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d06c80-bb41-4668-ad07-d96da8e4b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build our training records without labels\n",
    "records = [\n",
    "    rg.TextClassificationRecord(\n",
    "        text=record[\"text\"],\n",
    "        metadata={\"split\": \"unlabelled\"},\n",
    "    )\n",
    "    for record in dataset[\"train\"]\n",
    "]\n",
    "\n",
    "# log the records to Argilla\n",
    "rg.log(records, name=\"news\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b943c5e-2a5d-40b5-9805-7fd78e5161a3",
   "metadata": {},
   "source": [
    "The result of the above is the following dataset in Argilla, with **127,600 records** (120,000 unlabelled and 7,600 for testing). \n",
    "\n",
    "You can use the web app to find good rules for programmatic labeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7255c-9f67-483c-b253-f186df8286cf",
   "metadata": {},
   "source": [
    "## 2. Interactive weak labeling: Finding and defining rules\n",
    "\n",
    "After logging the dataset, you can find and save rules directly with the UI. Then, you can read the rules with Python to train a label or downstream model, as we'll see in the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23dc1c6-9e5f-455f-b8b9-0466a062eda5",
   "metadata": {},
   "source": [
    "## 3. Denoise weak labels with Snorkel's Label Model\n",
    "\n",
    "The goal at this step is to **denoise** the weak labels we've just created using rules. There are several approaches to this problem using different statistical methods.\n",
    "\n",
    "In this tutorial, we're going to use Snorkel but you can actually use any other Label model or weak supervision method, such as FlyingSquid for example (see the [Weak supervision guide](../../guides/techniques/weak_supervision.html) for more details).\n",
    "For convenience, Argilla defines a simple wrapper over Snorkel's Label Model so it's easier to use with Argilla weak labels and datasets\n",
    "\n",
    "Let's first read the rules defined in our dataset and create our weak labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ccd1be-0b63-4c69-a405-d9b4b44e154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.labeling.text_classification import WeakLabels\n",
    "\n",
    "weak_labels = WeakLabels(dataset=\"news\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd9bee-9207-4e5e-8d79-41c04d9d0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_labels.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f7820-0144-4827-86b6-08f1fd190334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.labeling.text_classification import Snorkel\n",
    "\n",
    "# create the label model\n",
    "label_model = Snorkel(weak_labels)\n",
    "\n",
    "# fit the model\n",
    "label_model.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572368b-9b1c-4d68-ab36-ef86ca3f3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_model.score(output_str=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c5862-903b-48c4-bf34-233a2430b2a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Prepare our training set\n",
    "\n",
    "Now, we already have a \"denoised\" training set, which we can prepare for training a downstream model. \n",
    "The label model predict returns `TextClassificationRecord` objects with the `predictions` from the label model. \n",
    "\n",
    "We can either refine and review these records using the Argilla web app, use them as is, or filter them by score, for example.\n",
    "\n",
    "In this case, we assume the predictions are precise enough and use them without any revision. \n",
    "Our training set has ~38,000 records, which corresponds to all records where the label model has not abstained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c321871-3bfc-45b9-902b-4beb45f4ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get records with the predictions from the label model\n",
    "records = label_model.predict()\n",
    "# you can replace this line with\n",
    "# records = rg.read_datasets(\n",
    "#    load_dataset(\"argilla/news\", split=\"train\"),\n",
    "#    task=\"TextClassification\",\n",
    "# )\n",
    "\n",
    "# we could also use the `weak_labels.label2int` dict\n",
    "label2int = {\"Sports\": 0, \"Sci/Tech\": 1, \"World\": 2, \"Business\": 3}\n",
    "\n",
    "# extract training data\n",
    "X_train = [rec.text for rec in records]\n",
    "y_train = [label2int[rec.prediction[0][0]] for rec in records]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81c6c3-75fa-4534-9e27-e888d3f5257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at our training data with the weak labels from our label model\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(pd.DataFrame({\"text\": X_train, \"label\": y_train}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e6b973-fab5-43d0-994c-591b99835f90",
   "metadata": {},
   "source": [
    "## 5. Train a downstream model with scikit-learn\n",
    "\n",
    "Now, let's train our final model using `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e1aa9-68c9-4a65-bf52-ff5254cdd9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define our final classifier\n",
    "classifier = Pipeline([(\"vect\", CountVectorizer()), (\"clf\", MultinomialNB())])\n",
    "\n",
    "# fit the classifier\n",
    "classifier.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d608b29-9043-4183-af25-07290b404bf3",
   "metadata": {},
   "source": [
    "To test our trained model, we use the records with validated annotations, that is the original ag_news test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c41f9-bde8-4bc8-8406-a048e2e73aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve records with annotations\n",
    "test_ds = weak_labels.records(has_annotation=True)\n",
    "# you can replace this line with\n",
    "# test_ds = rg.read_datasets(\n",
    "#    load_dataset(\"argilla/news_test\", split=\"train\"),\n",
    "#    task=\"TextClassification\",\n",
    "# )\n",
    "\n",
    "# extract text and labels\n",
    "X_test = [rec.text for rec in test_ds]\n",
    "y_test = [label2int[rec.annotation] for rec in test_ds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac5b5ae-7c0d-48d6-800c-d95934b3d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the test accuracy\n",
    "accuracy = classifier.score(\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    ")\n",
    "\n",
    "print(f\"Test accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24b07b-dd15-4362-a88c-d3611748736e",
   "metadata": {},
   "source": [
    "Not too bad! ü•≥\n",
    "\n",
    "We have achieved around **0.82 accuracy** without even using a single example from the original `ag_news` train set and with a small set of 16 rules. \n",
    "Also, we've improved over the 0.75 accuracy of our Label Model.\n",
    "\n",
    "Finally, let's take a look at more detailed metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f169a-30ed-480e-8ab8-ae5af2c0a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# get predictions for the test set\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted, target_names=label2int.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b7ab7-6d11-46c2-8320-2f80fcb8a446",
   "metadata": {},
   "source": [
    "At this point, we could go back to the UI to define more rules for those labels with less performance. Looking at the above table, we might want to add some more rules for increasing the recall of the `Business` label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb807b8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we saw how you can leverage weak supervision to quickly build up a large training data set, and use it for the training of a first lightweight model.\n",
    "\n",
    "*Argilla* is a very handy tool to start the weak supervision process by making it easy to find a good set of starting rules, and to reiterate on them dynamically.\n",
    "Since *Argilla* also provides built-in support for the most common label models, you can get from rules to weak labels in a few straight forward steps.\n",
    "For more suggestions on how to leverage weak labels, you can checkout our [weak supervision guide](../../guides/techniques/weak_supervision.html) where we describe an [interesting approach](../../guides/techniques/weak_supervision.html#Joint-Model-with-Weasel) to jointly train the label and a transformers downstream model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b8ffd-4918-462d-88b5-a6a60f7e9a4b",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "‚≠ê Argilla [Github repo](https://github.com/argilla-io/argilla) to stay updated.\n",
    "\n",
    "üìö [Argilla documentation](https://docs.argilla.io) for more guides and tutorials.\n",
    "\n",
    "üôã‚Äç‚ôÄÔ∏è Join the Argilla community on [Slack](https://bit.ly/3o0Pfyk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a2443-78bb-4540-9eca-7fb5d1a3f62e",
   "metadata": {},
   "source": [
    "## Appendix I: Create rules and weak labels from Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc6a66-5e4b-4c84-b8ea-9f986339fd55",
   "metadata": {},
   "source": [
    "For some use cases, you might want to use Python for defining labeling rules and generating weak labels. Argilla provides you with the ability to define and test rules and labeling functions directly using Python. This might be useful for combining it with rules defined in the UI, and for leveraging structured resources such as lexicons and gazeteers which are easier to use directly a programmatic environment.\n",
    "\n",
    "In this section, we define the rules we've defined in the UI, this time directly using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d264315-83bf-40e4-895f-44b4b82cae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.labeling.text_classification import Rule\n",
    "\n",
    "# define queries and patterns for each category (using ES DSL)\n",
    "queries = [\n",
    "    ([\"money\", \"financ*\", \"dollar*\"], \"Business\"),\n",
    "    ([\"war\", \"gov*\", \"minister*\", \"conflict\"], \"World\"),\n",
    "    ([\"footbal*\", \"sport*\", \"game\", \"play*\"], \"Sports\"),\n",
    "    ([\"sci*\", \"techno*\", \"computer*\", \"software\", \"web\"], \"Sci/Tech\"),\n",
    "]\n",
    "\n",
    "# define rules\n",
    "rules = [Rule(query=term, label=label) for terms, label in queries for term in terms]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f1453-878b-4679-b95a-d32f9288e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla.labeling.text_classification import WeakLabels, add_rules\n",
    "\n",
    "# generate the weak labels\n",
    "\n",
    "add_rules(dataset=\"news\")\n",
    "weak_labels = WeakLabels(dataset=\"news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4b2fa",
   "metadata": {},
   "source": [
    "If you want to apply the rules without adding to dataset it's also possible as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_labels = WeakLabels(rules=rules, dataset=\"news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c86209-291f-46ef-a49e-7c21af7e2d22",
   "metadata": {},
   "source": [
    "On our machine it took around 24 seconds to apply the rules and to generate weak labels for the 127,600 examples.\n",
    "\n",
    "Typically, you want to iterate on the rules and check their statistics. \n",
    "For this, you can use `weak_labels.summary` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac6b226-0a90-4bff-a1d3-ba6b38041796",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_labels.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9703cf38-1d0a-4d56-9511-a0534d5b360a",
   "metadata": {},
   "source": [
    "From the above, we see that our rules cover around **30% of the original training set** with an **average precision of 0.73**. Our hope is that the label and downstream models will improve both the recall and the precision of the final classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afacb08d",
   "metadata": {},
   "source": [
    "## Appendix II: Log datasets to the Hugging Face Hub\n",
    "\n",
    "Here we will show you how we pushed our Argilla datasets (records) to the [Hugging Face Hub](https://huggingface.co/datasets).\n",
    "In this way you can effectively version any of your Argilla datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71b00e8-d471-4921-a8eb-cb6e0c13cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rg = rg.DatasetForTextClassification(label_model.predict())\n",
    "train_rg.to_datasets().push_to_hub(\"argilla/news\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6a766-6e3b-4fc3-8a45-6ee88c796d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rg = rg.load(\"news\", query=\"status:Validated\")\n",
    "test_rg.to_datasets().push_to_hub(\"argilla/news_test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('argilla')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "83e13ff0de9ea08cace169d1016bf08ce368842307fd88824f08736a0a9ca04b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
