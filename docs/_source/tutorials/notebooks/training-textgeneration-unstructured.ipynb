{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bb44a8e",
   "metadata": {},
   "source": [
    "# üï∏Ô∏è Train a summarization model with Unstructured and Transformers\n",
    "\n",
    "In this notebook, we'll show you how you can use the amazing library [unstructured](https://github.com/Unstructured-IO/unstructured) together with [argilla](https://github.com/argilla-io/argilla), and HuggingFace [transformers](https://github.com/huggingface/transformers) to train a custom summarization model. In this case, we're going to build a summarization model targeted at summarizing the [Institute for the Study of War's](https://www.understandingwar.org/) daily reports on the war in Ukraine. You can see an example of one of the reports [here](https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-december-12), and a screen shot appears below.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Attribution üéâ\n",
    "\n",
    "This [notebook](https://github.com/Unstructured-IO/unstructured/blob/main/examples/argilla-summarization/isw-summarization.ipynb) has been developed by Matt Robinson, from [Unstructured](https://unstructured.io). Unstructured is the recommended library for collecting unstructured formats for Argilla datasets, such as HTML docs and PDFs. If you don't know Unstructured yet, go to the [unstructured GitHub repo](https://github.com/Unstructured-IO/unstructured) and leave a star if you like what they're building.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Combining the `unstructured`, `argilla`, and `transformers` libraries, we're able to complete a data science project that previously could have taken a week or more in just a few hours!\n",
    "\n",
    "- Section 1: Data Collection and Staging with unstructured\n",
    "- Section 2: Label Verification with Argilla\n",
    "- Section 3: Model Training with transformers\n",
    "\n",
    "![ISW](../../_static/tutorials/training-textgeneration-unstructured/isw.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9da6035",
   "metadata": {},
   "source": [
    "## Running Argilla\n",
    "\n",
    "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
    "\n",
    "\n",
    "**Deploy Argilla on Hugging Face Spaces**: If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:\n",
    "\n",
    "[![deploy on spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/new-space?template=argilla/argilla-template-space)\n",
    "\n",
    "For details about configuring your deployment, check the [official Hugging Face Hub guide](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla).\n",
    "\n",
    "\n",
    "**Launch Argilla using Argilla's quickstart Docker image**: This is the recommended option if you want [Argilla running on your local machine](../../getting_started/quickstart.ipynb). Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
    "\n",
    "For more information on deployment options, please check the Deployment section of the documentation.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Tip\n",
    "    \n",
    "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
    "\n",
    "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
    "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c07da-1d89-468c-ac7b-dffab6a12c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"unstructured==0.4.4\" -qqq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fabdd145",
   "metadata": {},
   "source": [
    "Let's import the Argilla module for reading and writing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac811da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "556046ed",
   "metadata": {
    "id": "7TRNourOwigS"
   },
   "source": [
    "If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the `URL` and `API_KEY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "rg.init(\n",
    "    api_url=\"http://localhost:6900\", \n",
    "    api_key=\"admin.apikey\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5be64d5e",
   "metadata": {
    "id": "ccL8UFwj_CaD"
   },
   "source": [
    "Finally, let's include the imports we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "import tqdm\n",
    "\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.documents.elements import NarrativeText, ListItem\n",
    "from unstructured.staging.argilla import stage_for_argilla\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902631a0",
   "metadata": {},
   "source": [
    "## Section 1: Data Collection and Staging with `unstructured` <a id=\"collection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef78ea",
   "metadata": {},
   "source": [
    "First, we'll pull our documents from the ISW website. We'll use the built-in Python `datetime` and `calendar` libraries to iterate over the dates for the reports we want to pull and fine the associated URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb80d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISW_BASE_URL = \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment\"\n",
    "\n",
    "def datetime_to_url(dt):\n",
    "    month = dt.strftime(\"%B\").lower()\n",
    "    return f\"{ISW_BASE_URL}-{month}-{dt.day}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "245bdfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "year = 2022\n",
    "for month in range(3, 13):\n",
    "    _, last_day = calendar.monthrange(year, month)\n",
    "    for day in range(1, last_day + 1):\n",
    "        dt = datetime(year, month, day)\n",
    "        urls.append(datetime_to_url(dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0415f6",
   "metadata": {},
   "source": [
    "Once we have the URLs, we can pull the HTML document for each report from the web using the `requests` library. Normally, you'd need to write custom HTML parsing code using a library like `lxml` or `beautifulsoup` to extract the narrative text from the webpage for model training. With the `unstructured` library, you can simply call the `partition_html` function to extract the content of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d7d84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_elements(url):\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        return None\n",
    "        \n",
    "    elements = partition_html(text=r.text)    \n",
    "    return elements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2cd324b",
   "metadata": {},
   "source": [
    "After partitioning the document, we'll extract the `Key Takeaways` section from the ISW reports, which is shown in the screenshot below. The `Key Takeaways` section will serve as the target text for our summarization model. While it would be time consuming the write HTML parsing code to find this content, with the `unstructured` library it is easy. Since the `partition_html` function breaks down the elements of the document into different categories like `Title`, `NarrativeText`, and `ListItem`, all we need to do is find the `Key Takeaways` title and then grab `ListItem` elements until the list ends. This logic is implemented in the `get_key_takeaways` function.\n",
    "\n",
    "<img src=\"../../_static/tutorials/training-textgeneration-unstructured/isw-key-takeaways.png\" width=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82ce0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_key_takeaways_idx(elements):\n",
    "    for idx, element in enumerate(elements):\n",
    "        if element.text == \"Key Takeaways\":\n",
    "            return idx\n",
    "\n",
    "def get_key_takeaways(elements):\n",
    "    key_takeaways_idx = _find_key_takeaways_idx(elements)\n",
    "    if not key_takeaways_idx:\n",
    "        return None\n",
    "    \n",
    "    takeaways = []\n",
    "    for element in elements[key_takeaways_idx + 1:]:\n",
    "        if not isinstance(element, ListItem):\n",
    "            break\n",
    "        takeaways.append(element)\n",
    "\n",
    "    takeaway_text = \" \".join([el.text for el in takeaways])\n",
    "    return NarrativeText(text=takeaway_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a90b939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = url_to_elements(urls[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95a1b3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian forces continue to prioritize strategically meaningless offensive operations around Donetsk City and Bakhmut over defending against continued Ukrainian counter-offensive operations in Kharkiv Oblast. Ukrainian forces liberated a settlement southwest of Lyman and are likely continuing to expand their positions in the area. Ukrainian forces continued to conduct an interdiction campaign in Kherson Oblast. Russian forces continued to conduct unsuccessful assaults around Bakhmut and Avdiivka. Ukrainian sources reported extensive partisan attacks on Russian military assets and logistics in southern Zaporizhia Oblast. Russian officials continued to undertake crypto-mobilization measures to generate forces for war Russian war efforts. Russian authorities are working to place 125 ‚Äúorphan‚Äù Ukrainian children from occupied Donetsk Oblast with Russian families.\n"
     ]
    }
   ],
   "source": [
    "print(get_key_takeaways(elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa2396",
   "metadata": {},
   "source": [
    "Next we'll grab the narrative text from the document as input for our model. Again, this is easy with `unstructured` because the `partition_html` function already splits out the text. We'll just grab all of the `NarrativeText` elements that exceed a minimum length threshold. While we're in there, we'll also clean out the raw text for citations within the document, which isn't natural language and could impact the quality of our summarization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cefabbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_narrative(elements):\n",
    "    narrative_text = \"\"\n",
    "    for element in elements:        \n",
    "        if isinstance(element, NarrativeText) and len(element.text) > 500:\n",
    "            # NOTE: Removes citations like [3] from the text\n",
    "            element_text = re.sub(\"\\[\\d{1,3}\\]\", \"\", element.text)\n",
    "            narrative_text += f\"\\n\\n{element_text}\"\n",
    "        \n",
    "    return NarrativeText(text=narrative_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e55a22fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian forces continue to conduct meaningless offensive operations around Donetsk City and Bakhmut instead of focusing on defending against Ukrainian counteroffensives that continue to advance.¬†Russian troops continue to attack Bakhmut and various villages near Donetsk City of emotional significance to pro-war residents of the Donetsk People‚Äôs Republic (DNR) but little other importance. The Russians are apparently directing some of the very limited reserves available in Ukraine to these efforts rather than to the vulnerable Russian defensive lines hastily thrown up along the Oskil River in eastern Kharkiv Oblast. The Russians cannot hope to make gains around Bakhmut or Donetsk City on a large enough scale to derail Ukrainian counteroffensives and appear to be continuing an almost robotic effort to gain ground in Donetsk Oblast that seems increasingly divorced from the overall realities of the theater.\n",
      "\n",
      "Russian failures to rush large-scale reinforcements to eastern Kharkiv and to Luhansk Oblasts leave most of Russian-occupied northeastern Ukraine highly vulnerable to continuing Ukrainian counter-offensives.¬†The Russians may have decided not to defend this area, despite Russian President Vladimir Putin‚Äôs repeated declarations that the purpose of the ‚Äúspecial military operation‚Äù is to ‚Äúliberate‚Äù Donetsk and Luhansk oblasts. Prioritizing the defense of Russian gains in southern Ukraine over holding northeastern Ukraine makes strategic sense since Kherson and Zaporizhia Oblasts are critical terrain for both Russia and Ukraine whereas the sparsely-populated agricultural areas in the northeast are much less so. But the continued Russian offensive operations around Bakhmut and Donetsk City, which are using some of Russia‚Äôs very limited effective combat power at the expense of defending against Ukrainian counteroffensives, might indicate that Russian theater decision-making remains questionable.\n",
      "\n",
      "Ukrainian forces appear to be expanding positions east of the Oskil River and \n"
     ]
    }
   ],
   "source": [
    "# show a sample of narrative text\n",
    "print(get_narrative(elements).text[0:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47ebc3",
   "metadata": {},
   "source": [
    "Now the we have everything set up, let's collect all of the reports! This step could take a while, we added a sleep call to the loop to avoid overwhelming ISW's webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "annotations = []\n",
    "for url in tqdm.tqdm(urls):\n",
    "    elements = url_to_elements(url)\n",
    "    if url is None or not elements:\n",
    "        continue\n",
    "    \n",
    "    text = get_narrative(elements)\n",
    "    annotation = get_key_takeaways(elements)\n",
    "    \n",
    "    if text and annotation:\n",
    "        inputs.append(text)\n",
    "        annotations.append(annotation.text)\n",
    "    # NOTE: Sleeping to reduce the volume of requests to ISW\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28d84bbf",
   "metadata": {},
   "source": [
    "## Section 2: Label Verification with `argilla` <a id=\"verification\"></a>\n",
    "\n",
    "Now that we've collected the data and prepared it with `unstructured`, we're ready to work on our data labels in `argilla`. First, we'll use the `stage_for_argilla` staging brick from the `unstructured` library. This will automatically convert our dataset to a `DatasetForText2Text` object, which we can then import into Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "286b29c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = stage_for_argilla(inputs, \"text2text\", annotation=annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f202c5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_agent</th>\n",
       "      <th>annotation</th>\n",
       "      <th>annotation_agent</th>\n",
       "      <th>vectors</th>\n",
       "      <th>id</th>\n",
       "      <th>metadata</th>\n",
       "      <th>status</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>metrics</th>\n",
       "      <th>search_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russian forces are completing the reinforcemen...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Russian forces are setting conditions to envel...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1a5b66dcbf80159ce2c340b17644d639</td>\n",
       "      <td>{}</td>\n",
       "      <td>Validated</td>\n",
       "      <td>2023-01-31 11:19:52.784880</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Russian forces resumed offensive operations in...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Russian forces resumed offensive operations ag...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>32e2f136256a7003de06c5792a5474fe</td>\n",
       "      <td>{}</td>\n",
       "      <td>Validated</td>\n",
       "      <td>2023-01-31 11:19:52.784941</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Russian military has continued its unsucce...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Russian forces opened a new line of advance fr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6e4c94cdc2512ee7b915c303161ada1d</td>\n",
       "      <td>{}</td>\n",
       "      <td>Validated</td>\n",
       "      <td>2023-01-31 11:19:52.784983</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russian forces continue their focus on encircl...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Russian forces have advanced rapidly on the ea...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5c123326055aa4832014ed9ab07e80f1</td>\n",
       "      <td>{}</td>\n",
       "      <td>Validated</td>\n",
       "      <td>2023-01-31 11:19:52.785022</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Russian forces remain deployed in the position...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Russian forces conducted no major offensive op...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>b6597ad2ca8a352bfc46a04b85b22421</td>\n",
       "      <td>{}</td>\n",
       "      <td>Validated</td>\n",
       "      <td>2023-01-31 11:19:52.785060</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text prediction  \\\n",
       "0  Russian forces are completing the reinforcemen...       None   \n",
       "1  Russian forces resumed offensive operations in...       None   \n",
       "2  The Russian military has continued its unsucce...       None   \n",
       "3  Russian forces continue their focus on encircl...       None   \n",
       "4  Russian forces remain deployed in the position...       None   \n",
       "\n",
       "  prediction_agent                                         annotation  \\\n",
       "0             None  Russian forces are setting conditions to envel...   \n",
       "1             None  Russian forces resumed offensive operations ag...   \n",
       "2             None  Russian forces opened a new line of advance fr...   \n",
       "3             None  Russian forces have advanced rapidly on the ea...   \n",
       "4             None  Russian forces conducted no major offensive op...   \n",
       "\n",
       "  annotation_agent vectors                                id metadata  \\\n",
       "0             None    None  1a5b66dcbf80159ce2c340b17644d639       {}   \n",
       "1             None    None  32e2f136256a7003de06c5792a5474fe       {}   \n",
       "2             None    None  6e4c94cdc2512ee7b915c303161ada1d       {}   \n",
       "3             None    None  5c123326055aa4832014ed9ab07e80f1       {}   \n",
       "4             None    None  b6597ad2ca8a352bfc46a04b85b22421       {}   \n",
       "\n",
       "      status            event_timestamp metrics search_keywords  \n",
       "0  Validated 2023-01-31 11:19:52.784880    None            None  \n",
       "1  Validated 2023-01-31 11:19:52.784941    None            None  \n",
       "2  Validated 2023-01-31 11:19:52.784983    None            None  \n",
       "3  Validated 2023-01-31 11:19:52.785022    None            None  \n",
       "4  Validated 2023-01-31 11:19:52.785060    None            None  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas().head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c76982c",
   "metadata": {},
   "source": [
    "After staging the data for argilla, we can call the `rg.log` function from the `argilla` Python library to upload the data to the Argilla UI. Before running this step, ensure that you have the Argilla Server running in the background. After logging the data to Argilla, your UI should look like the screenshot below.\n",
    "\n",
    "<img src=\"../../_static/tutorials/training-textgeneration-unstructured/argilla-dataset.png\" width=800 /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.log(dataset, name=\"isw-summarization\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "311cf33b",
   "metadata": {},
   "source": [
    "After uploading the dataset, head over to the Argilla UI and validate and/or adjust the summaries we pulled from the ISW site. You can also check out the [Argilla docs](https://docs.argilla.io/) for more information on all of the exciting tools Argilla provides to help you label, assess, and refine your training data!\n",
    "\n",
    "<img src=\"../../_static/tutorials/training-textgeneration-unstructured/argilla-annotation.png\" width=800 /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083edaf",
   "metadata": {},
   "source": [
    "## Section 3: Model Training with `transformers` <a id=\"training\"></a>\n",
    "\n",
    "After refining our traning data in Argilla, we're ready to fine-tune our model using the `transformers` library. Luckily, `argilla` has a utility for converting datasets to a `dataset.Dataset`, which is the format required by the `transformers` `Trainer` object. In this example, we'll train a `t5-small` model to keep the runtime for the notebook reasonable. You can play around with larger models to get higher quality results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6db0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = rg.load(\"isw-summarization\").to_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ebe5d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21113d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7919d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"annotation\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea639902",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = training_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7839a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d8c62b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"t5-small-isw-summaries\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1717994",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "555b18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b147430",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7eab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"t5-small-isw-summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_model = pipeline(\n",
    "task=\"summarization\",\n",
    "model=\"./t5-small-isw-summaries\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea363821",
   "metadata": {},
   "source": [
    "Now that our model is trained, we can save it locally and use our `unstructured` helper functions to grab future reports for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d6843b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian forces continue to attack Bakhmut and various villages near Donetsk City . the Russians are apparently directing some of the very limited reserves available in Ukraine to these efforts rather than to the vulnerable Russian defensive lines hastily thrown up . Russian sources claimed that Russian forces are repelled a Ukrainian ground attack on Pravdyne .\n"
     ]
    }
   ],
   "source": [
    "elements = url_to_elements(urls[200])\n",
    "narrative_text = get_narrative(elements)\n",
    "results = summarization_model(str(narrative_text), max_length=100)\n",
    "print(results[0][\"summary_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Dec 17 2020, 03:56:09) \n[Clang 11.0.0 (clang-1100.0.33.17)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8874e298d2bce9702a08b32d5709c9f02d53b2045f1d246836c6e4c8123e6782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
