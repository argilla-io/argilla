{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rubrix Cookbook\n",
    "\n",
    "This guide is a collection of recipes. It shows examples for using Rubrix with some of the most popular NLP Python libraries. \n",
    "\n",
    "Rubrix is *agnostic*, it can be used  with any library or framework, no need to implement any interface or modify your existing toolbox and workflows. \n",
    "\n",
    "With these examples you'll be able to start exploring and annnotating data with these libraries or get some inspiration if your library of choice is not in this guide.\n",
    "\n",
    "If you miss a library in this guide, leave a message at the [Rubrix Github forum](https://github.com/recognai/rubrix/discussions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Transformers\n",
    "\n",
    "[Hugging Face](https://huggingface.co) has made working with NLP easier than ever before. With a few lines of code we can take a pretrained Transformer model from the [Hub](https://huggingface.co/models), start making some predictions and log them into Rubrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "\n",
    "\n",
    "#### Inference\n",
    "\n",
    "Let's try a zero-shot classifier using `typeform/distilbert-base-uncased-mnli` for predicting the topic of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from transformers import pipeline\n",
    "\n",
    "input_text = \"I love watching rock climbing competitions!\"\n",
    "\n",
    "# We define our HuggingFace Pipeline\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"typeform/distilbert-base-uncased-mnli\",\n",
    "    framework=\"pt\",\n",
    ")\n",
    "\n",
    "# Making the prediction\n",
    "prediction = classifier(\n",
    "    input_text,\n",
    "    candidate_labels=['World', 'Sports', 'Business', 'Sci/Tech'],\n",
    "    hypothesis_template=\"This text is about {}.\",\n",
    ")\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (label, probability)\n",
    "prediction = list(zip(prediction[\"labels\"], prediction[\"scores\"]))\n",
    "\n",
    "# Building a TextClassificationRecord\n",
    "record = rb.TextClassificationRecord(\n",
    "    inputs=input_text,\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"typeform/distilbert-base-uncased-mnli\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"zeroshot-topic-classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Let's read a Rubrix dataset, prepare a training set and use the `Trainer` API for fine-tuning a `distilbert-base-uncased` model. Take into account that a `labelled_dataset` is expected to be found in your Rubrix client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import rubrix as rb\n",
    "\n",
    "# load rubrix dataset\n",
    "df = rb.load('labelled_dataset')\n",
    "\n",
    "# inputs can be dicts to support multifield classifiers, we just use the text here. \n",
    "df['text'] = df.inputs.transform(lambda r: r['text'])\n",
    "\n",
    "# we create a dict for turning our annotations (labels) into numeric ids\n",
    "label2id = {label: id for id, label in enumerate(df.annotation.unique())}\n",
    "\n",
    "\n",
    "# create 🤗 dataset from pandas with labels as numeric ids\n",
    "dataset = Dataset.from_pandas(df[['text', 'annotation']])\n",
    "dataset = dataset.map(lambda example: {'labels': label2id[example['annotation']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer\n",
    "\n",
    "# from here, it's just regular fine-tuning with 🤗 transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = dataset.map(tokenize_function, batched=True).shuffle(seed=42)\n",
    "\n",
    "trainer = Trainer(model=model, train_dataset=train_dataset)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Classification\n",
    "\n",
    "We will explore a DistilBERT NER classifier fine-tuned for NER using the conll03 English dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from transformers import pipeline\n",
    "\n",
    "input_text = \"My name is Sarah and I live in London\"\n",
    "\n",
    "# We define our HuggingFace Pipeline\n",
    "classifier = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"elastic/distilbert-base-cased-finetuned-conll03-english\",\n",
    "    framework=\"pt\",\n",
    ")\n",
    "\n",
    "# Making the prediction\n",
    "predictions = classifier(\n",
    "    input_text,\n",
    ")\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [(pred[\"entity\"], pred[\"start\"], pred[\"end\"]) for pred in predictions]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=input_text.split(),\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"https://huggingface.co/elastic/distilbert-base-cased-finetuned-conll03-english\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"zeroshot-ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "[spaCy](https://spacy.io) offers industrial-strength Natural Language Processing, with support for 64+ languages, trained pipelines, multi-task learning with pretrained Transformers, pretrained word vectors and much more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Classification\n",
    "\n",
    "We will focus our spaCy recipes into Token Classification tasks, showing you how to log data from NER and POS tagging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER\n",
    "\n",
    "For this recipe, we are going to try the French language model to extract NER entities from some sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import spacy\n",
    "\n",
    "input_text = \"Paris a un enfant et la forêt a un oiseau ; l’oiseau s’appelle le moineau ; l’enfant s’appelle le gamin\"\n",
    "\n",
    "# Loading spaCy model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Creating spaCy doc\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [(ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "\n",
    "# Building TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[token.text for token in doc],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"spacy.fr_core_news_sm\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"lesmiserables-ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging\n",
    "\n",
    "Changing very few parameters, we can make a POS tagging experiment, instead of NER. Let's try it out with the same input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import spacy\n",
    "\n",
    "input_text = \"Paris a un enfant et la forêt a un oiseau ; l’oiseau s’appelle le moineau ; l’enfant s’appelle le gamin\"\n",
    "\n",
    "# Loading spaCy model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Creating spaCy doc\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (tag, start_char, end_char)\n",
    "prediction = [(token.pos_, token.idx, token.idx + len(token)) for token in doc]\n",
    "\n",
    "# Building TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[token.text for token in doc],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"spacy.fr_core_news_sm\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"lesmiserables-pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flair\n",
    "\n",
    "It's a framework that provides a state-of-the-art NLP library, a text embedding library and a PyTorch framework for NLP. [Flair](https://github.com/flairNLP/flair) offers sequence tagging language models in English, Spanish, Dutch, German and many more, and they are also hosted on [HuggingFace Model Hub](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error message when trying to import flair due to issues for downloading the wordnet_ic package try running the following and manually download the `wordnet_ic` package (available under the All Packages tab). Otherwise you can skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "\n",
    "#### Zero-shot and Few-shot classifiers\n",
    "\n",
    "Flair enables you to use few-shot and zero-shot learning for text classification with Task-aware representation of sentences (TARS), introduced by Halder et al. (2020), see [Flair's  documentation](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_10_TRAINING_ZERO_SHOT_MODEL.md) for more details. \n",
    "\n",
    "Let's see an example of the base zero-shot TARS model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from flair.models import TARSClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "# Load our pre-trained TARS model for English\n",
    "tars = TARSClassifier.load('tars-base')\n",
    "\n",
    "# Define labels\n",
    "labels = [\"happy\", \"sad\"]\n",
    "\n",
    "# Create a sentence\n",
    "input_text = \"I am so glad you liked it!\"\n",
    "sentence = Sentence(input_text)\n",
    "\n",
    "# Predict for these labels\n",
    "tars.predict_zero_shot(sentence, labels)\n",
    "\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (label, probability)\n",
    "prediction = [(pred.value, pred.score) for pred in sentence.labels]\n",
    "\n",
    "# Building a TextClassificationRecord\n",
    "record = rb.TextClassificationRecord(\n",
    "    inputs=input_text,\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"tars-base\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"en-emotion-zeroshot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom and pre-trained classifiers\n",
    "\n",
    "Let’s see an example with Deutch offensive language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "input_text = \"Du erzählst immer Quatsch.\"  # something like: \"You are always narrating silliness.\"\n",
    "\n",
    "# Load our pre-trained classifier\n",
    "classifier = TextClassifier.load(\"de-offensive-language\")\n",
    "\n",
    "# Creating Sentence object\n",
    "sentence = Sentence(input_text)\n",
    "\n",
    "# Make the prediction\n",
    "classifier.predict(sentence, return_probabilities_for_all_classes=True)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (label, probability)\n",
    "prediction = [(pred.value, pred.score) for pred in sentence.labels]\n",
    "\n",
    "# Building a TextClassificationRecord\n",
    "record = rb.TextClassificationRecord(\n",
    "    inputs=input_text,\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"de-offensive-language\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"german-offensive-language\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Classification\n",
    "\n",
    "Flair offers a lot of tools for Token Classification, supporting tasks like named entity recognition (NER), part-of-speech tagging (POS), special support for biomedical data, etc. with a growing number of supported languages.\n",
    "\n",
    "Let's see some examples for NER and POS tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER\n",
    "\n",
    "In this example, we will try the pretrained Dutch NER model from Flair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "input_text = \"De Nachtwacht is in het Rijksmuseum\"\n",
    "\n",
    "# Loading our NER model from flair\n",
    "tagger = SequenceTagger.load(\"flair/ner-dutch\")\n",
    "\n",
    "# Creating Sentence object\n",
    "sentence = Sentence(input_text)\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [\n",
    "    (entity.get_labels()[0].value, entity.start_pos, entity.end_pos)\n",
    "    for entity in sentence.get_spans(\"ner\")\n",
    "]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[token.text for token in sentence],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"flair/ner-dutch\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"dutch-flair-ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging\n",
    "\n",
    "In the following snippet we will use de multilingual POS tagging model from Flair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "input_text = \"George Washington went to Washington. Dort kaufte er einen Hut.\"\n",
    "\n",
    "# Loading our POS tagging model from flair\n",
    "tagger = SequenceTagger.load(\"flair/upos-multi\")\n",
    "\n",
    "# Creating Sentence object\n",
    "sentence = Sentence(input_text)\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [\n",
    "    (entity.get_labels()[0].value, entity.start_pos, entity.end_pos)\n",
    "    for entity in sentence.get_spans()\n",
    "]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[token.text for token in sentence],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"flair/upos-multi\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"flair-pos-tagging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza\n",
    "\n",
    "[Stanza](https://stanfordnlp.github.io/stanza/) is a collection of efficient tools for many NLP tasks and processes, all in one library. It's maintained by the [Standford NLP Group](https://nlp.stanford.edy). We are going to take a look at a few interactions that can be done with Rubrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "\n",
    "Let's start by using a Sentiment Analysis model to log some `TextClassificationRecords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import stanza\n",
    "\n",
    "input_text = (\n",
    "    \"There are so many NLP libraries available, I don't know which one to choose!\"\n",
    ")\n",
    "\n",
    "# Downloading our model, in case we don't have it cached\n",
    "stanza.download(\"en\")\n",
    "\n",
    "# Creating the pipeline\n",
    "nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,sentiment\")\n",
    "\n",
    "# Analizing the input text\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# This model returns 0 for negative, 1 for neutral and 2 for positive outcome.\n",
    "# We are going to log them into Rubrix using a dictionary to translate numbers to labels.\n",
    "num_to_labels = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "\n",
    "# Build a prediction entities list\n",
    "# Stanza, at the moment, only output the most likely label without probability.\n",
    "# So we will suppouse Stanza predicts the most likely label with 1.0 probability, and the rest with 0.\n",
    "entities = []\n",
    "\n",
    "for _, sentence in enumerate(doc.sentences):\n",
    "    for key in num_to_labels:\n",
    "        if key == sentence.sentiment:\n",
    "            entities.append((num_to_labels[key], 1))\n",
    "        else:\n",
    "            entities.append((num_to_labels[key], 0))\n",
    "\n",
    "# Building a TextClassificationRecord\n",
    "record = rb.TextClassificationRecord(\n",
    "    inputs=input_text,\n",
    "    prediction=entities,\n",
    "    prediction_agent=\"stanza/en\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"stanza-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Classification\n",
    "\n",
    "Stanza offers so many different pretrained language models for Token Classification Tasks, and the list does not stop growing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging\n",
    "\n",
    "We can use one of the many UD models, used for POS tags, morphological features and syntantic relations. UD stands for [Universal Dependencies](https://universaldependencies.org), the framework where these models has been trained. For this example, let's try to extract POS tags of some Catalan lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import stanza\n",
    "\n",
    "# Loading a cool Obrint Pas lyric\n",
    "input_text = \"Viure sempre corrent, avançant amb la gent, rellevant contra el vent, transportant sentiments.\"\n",
    "\n",
    "# Downloading our model, in case we don't have it cached\n",
    "stanza.download(\"ca\")\n",
    "\n",
    "# Creating the pipeline\n",
    "nlp = stanza.Pipeline(lang=\"ca\", processors=\"tokenize,mwt,pos\")\n",
    "\n",
    "# Analizing the input text\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (tag, start_char, end_char)\n",
    "prediction = [\n",
    "    (word.pos, token.start_char, token.end_char)\n",
    "    for sent in doc.sentences\n",
    "    for token in sent.tokens\n",
    "    for word in token.words\n",
    "]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[word.text for sent in doc.sentences for word in sent.words],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"stanza/catalan\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"stanza-catalan-pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER\n",
    "\n",
    "Stanza also offers a list of available pretrained models for NER tasks. So, let's try Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import stanza\n",
    "\n",
    "input_text = (\n",
    "    \"Герра-и-Пас - одна из моих любимых книг\"  # War and Peace is one my favourite books\n",
    ")\n",
    "\n",
    "# Downloading our model, in case we don't have it cached\n",
    "stanza.download(\"ru\")\n",
    "\n",
    "# Creating the pipeline\n",
    "nlp = stanza.Pipeline(lang=\"ru\", processors=\"tokenize,ner\")\n",
    "\n",
    "# Analizing the input text\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [\n",
    "    (token.ner, token.start_char, token.end_char)\n",
    "    for sent in doc.sentences\n",
    "    for token in sent.tokens\n",
    "]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[word.text for sent in doc.sentences for word in sent.words],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"flair/russian\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"stanza-russian-ner\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b709380ea7d1cb2eb4650c0f11ac7e002ec6a534602815725771481b4784238c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
