{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd00f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd",
   "display_name": "Python 3.8.10 64-bit ('rubrix': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Rubrix Cookbook\n",
    "\n",
    "Yeah, you heard it right! Not a cheatsheet, but a cookbook. A notebook of recipes. \n",
    "\n",
    "In this quick guide, we are going to show you how easily can Rubrix be used side by side with some of the most popular AI Python libraries. Rubrix is *agnostic*, it can be used  with any library or framework, no need to implement any interface or modify your existing toolbox and workflows. With these few example you will be able to start loging and exploring your data for any of these libraries with just a glance, and maybe pick up some inspiration if your library of choice is not in this list.\n",
    "\n",
    "If you miss one AI library in this list, tell us about it at [our Github forum](https://github.com/recognai/rubrix/discussions)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## HuggingFace Transformers\n",
    "\n",
    "[HuggingFace](https://huggingface.co) has given to the NLP community many useful tools, and with HuggingFace Transformers produce cutting-edge models is easier than ever. With a few lines of code we can take a Transformer model from their hub, start making some predictions and log them into Rubrix."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Text Classification\n",
    "\n",
    "Let's try a zero-shot classifier using SqueezeBERT for predicting the topic of a sentence."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from transformers import pipeline\n",
    "\n",
    "input_text = \"I love watching rock climbing competitions!\"\n",
    "\n",
    "# We define our HuggingFace Pipeline\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"typeform/squeezebert-mnli\",\n",
    "    framework=\"pt\",\n",
    ")\n",
    "\n",
    "# Making the prediction\n",
    "prediction = classifier(\n",
    "    input_text,\n",
    "    candidate_labels=[\n",
    "        \"politics\",\n",
    "        \"sports\",\n",
    "        \"technology\",\n",
    "    ],\n",
    "    hypothesis_template=\"This text is about {}.\",\n",
    ")\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (label, probability)\n",
    "prediction = list(zip(prediction[\"labels\"], prediction[\"scores\"]))\n",
    "\n",
    "# Building a TextClassificationRecord\n",
    "record = rb.TextClassificationRecord(\n",
    "    inputs=input_text,\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"https://huggingface.co/typeform/squeezebert-mnli\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"zeroshot-topic-classifier\")"
   ]
  },
  {
   "source": [
    "### Token Classification\n",
    "\n",
    "We will explore a DistilBERT NER classifier fine-tuned for NER using the conll03 English dataset. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from transformers import pipeline\n",
    "\n",
    "input_text = \"My name is Sarah and I live in London\"\n",
    "\n",
    "# We define our HuggingFace Pipeline\n",
    "classifier = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"elastic/distilbert-base-cased-finetuned-conll03-english\",\n",
    "    framework=\"pt\",\n",
    ")\n",
    "\n",
    "# Making the prediction\n",
    "predictions = classifier(\n",
    "    input_text,\n",
    ")\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [(pred[\"entity\"], pred[\"start\"], pred[\"end\"]) for pred in predictions]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=input_text.split(),\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"https://huggingface.co/elastic/distilbert-base-cased-finetuned-conll03-english\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"zeroshot-ner\")"
   ]
  },
  {
   "source": [
    "## spaCy\n",
    "\n",
    "[spaCy](https://spacy.io) offers industrial-strength Natural Language Processing, with support for 64+ languages, trained pipelines, multi-task learning with pretrained Transformers, pretrained word vectors and much more. Combining spaCy with Rubrix allows you to combine these learning capabilities with the power to monitor your predictions, collect and iterate through ground truth and build custom applications and dashboards."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Token Classification\n",
    "\n",
    "We will focus our spaCy recipes into Token Classification tasks, showing you how to log data from NER and POS tagging. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### NER\n",
    "\n",
    "For this recipe, we are going to try the French language model to extract NER entities from some sentences."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import spacy\n",
    "\n",
    "input_text = \"Paris a un enfant et la forêt a un oiseau ; l’oiseau s’appelle le moineau ; l’enfant s’appelle le gamin\"\n",
    "\n",
    "# Loading spaCy model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Creating spaCy doc\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [(ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "\n",
    "# Building TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[token.text for token in doc],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"spacy.fr_core_news_sm\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"lesmiserables-ner\")"
   ]
  },
  {
   "source": [
    "#### POS tagging\n",
    "\n",
    "Changing very few parameters, we can make a POS tagging experiment, instead of NER. Let's try it out with the same input sentence."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import spacy\n",
    "\n",
    "input_text = \"Paris a un enfant et la forêt a un oiseau ; l’oiseau s’appelle le moineau ; l’enfant s’appelle le gamin\"\n",
    "\n",
    "# Loading spaCy model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Creating spaCy doc\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (tag, start_char, end_char)\n",
    "prediction = [(token.pos_, token.idx, token.idx + len(token)) for token in doc]\n",
    "\n",
    "# Building TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[token.text for token in doc],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"spacy.fr_core_news_sm\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"lesmiserables-pos\")"
   ]
  },
  {
   "source": [
    "## Flair\n",
    "\n",
    "It's a framework that provides a state-of-the-art NLP library, a text embedding library and a PyTorch framework for NLP. [Flair](https://github.com/flairNLP/flair) offers sequence tagging language models in English, Spanish, Dutch, German and many more, and they are also hosted on [HuggingFace Model Hub](https://huggingface.co/models)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Text Classification\n",
    "\n",
    "Flair offers some zero-shot models ready to be used, which we are going to use to introduce logging `TextClassificationRecords` with Rubrix. Let's see how to integrate Rubrix in their Deutch offensive language model (we promise to not get very explicit)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "input_text = \"Du erzählst immer Quatsch.\"  # something like: \"You are always narrating silliness.\"\n",
    "\n",
    "# Load our pre-trained TARS model for English\n",
    "classifier = TextClassifier.load(\"de-offensive-language\")\n",
    "\n",
    "# Creating Sentence object\n",
    "sentence = Sentence(input_text)\n",
    "\n",
    "# Make the prediction\n",
    "classifier.predict(sentence, multi_class_prob=True)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (label, probability)\n",
    "prediction = [(pred.value, pred.score) for pred in sentence.labels]\n",
    "\n",
    "# Building a TextClassificationRecord\n",
    "record = rb.TextClassificationRecord(\n",
    "    inputs=input_text,\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"de-offensive-language\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"german-offensive-language\")"
   ]
  },
  {
   "source": [
    "### Token Classification\n",
    "\n",
    "Flair offers a lot of tools for Token Classification, supporting tasks like named entity recognition (NER), part-of-speech tagging (POS), special support for biomedical data... and with a growing number of supported languages. Lets see some examples for NER and POS tagging."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### NER\n",
    "\n",
    "In this example, we will try the pretrained Dutch NER model from Flair."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "input_text = \"De Nachtwacht is in het Rijksmuseum\"\n",
    "\n",
    "# Loading our NER model from flair\n",
    "tagger = SequenceTagger.load(\"flair/ner-dutch\")\n",
    "\n",
    "# Creating Sentence object\n",
    "sentence = Sentence(input_text)\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [\n",
    "    (entity.get_labels()[0].value, entity.start_pos, entity.end_pos)\n",
    "    for entity in sentence.get_spans(\"ner\")\n",
    "]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[token.text for token in sentence],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"flair/ner-dutch\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"dutch-flair-ner\")"
   ]
  },
  {
   "source": [
    "#### POS tagging\n",
    "\n",
    "In the following snippet we will use de multilingual POS tagging model from Flair."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "input_text = \"George Washington went to Washington. Dort kaufte er einen Hut.\"\n",
    "\n",
    "# Loading our POS tagging model from flair\n",
    "tagger = SequenceTagger.load(\"flair/upos-multi\")\n",
    "\n",
    "# Creating Sentence object\n",
    "sentence = Sentence(input_text)\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [\n",
    "    (entity.get_labels()[0].value, entity.start_pos, entity.end_pos)\n",
    "    for entity in sentence.get_spans()\n",
    "]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[token.text for token in sentence],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"flair/upos-multi\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"flair-pos-tagging\")"
   ]
  },
  {
   "source": [
    "## Stanza\n",
    "\n",
    "[Stanza]() is a collection of efficient tools for many NLP tasks and processes, all in one library. It was created and it's maintained by the [Standford NLP Group](https://nlp.stanford.edy). We are going to take a look at a few interactions that can be done with Rubrix."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Text Classification\n",
    "\n",
    "Let's start by using a Sentiment Analysis model to log some `TextClassificationRecords`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import stanza\n",
    "\n",
    "input_text = (\n",
    "    \"There are so many NLP libraries available, I don't know which one to choose!\"\n",
    ")\n",
    "\n",
    "# Downloading our model, in case we don't have it cached\n",
    "stanza.download(\"en\")\n",
    "\n",
    "# Creating the pipeline\n",
    "nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,sentiment\")\n",
    "\n",
    "# Analizing the input text\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# This model returns 0 for negative, 1 for neutral and 2 for positive outcome.\n",
    "# We are going to log them into Rubrix using a dictionary to translate numbers to labels.\n",
    "num_to_labels = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "\n",
    "# Build a prediction entities list\n",
    "# Stanza, at the moment, only output the most likely label without probability.\n",
    "# So we will suppouse Stanza predicts the most likely label with 1.0 probability, and the rest with 0.\n",
    "entities = []\n",
    "\n",
    "for _, sentence in enumerate(doc.sentences):\n",
    "    for key in num_to_labels:\n",
    "        if key == sentence.sentiment:\n",
    "            entities.append((num_to_labels[key], 1))\n",
    "        else:\n",
    "            entities.append((num_to_labels[key], 0))\n",
    "\n",
    "# Building a TextClassificationRecord\n",
    "record = rb.TextClassificationRecord(\n",
    "    inputs=input_text,\n",
    "    prediction=entities,\n",
    "    prediction_agent=\"stanza/en\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"stanza-sentiment\")"
   ]
  },
  {
   "source": [
    "### Token Classification\n",
    "\n",
    "Stanza offers so many different pretrained language models for Token Classification Tasks, and the list does not stop growing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### POS tagging\n",
    "\n",
    "We can use one of the many UD models, used for POS tags, morphological features and syntantic relations. UD stands for [Universal Dependencies](https://universaldependencies.org), the framework where these models has been trained. For this example, let's try to extract POS tags of some Catalan lyrics."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import stanza\n",
    "\n",
    "# Loading a cool Obrint Pas lyric\n",
    "input_text = \"Viure mantenint viva la flama a través del temps. La flama de tot un poble en moviment\"\n",
    "\n",
    "# Downloading our model, in case we don't have it cached\n",
    "stanza.download(\"ca\")\n",
    "\n",
    "# Creating the pipeline\n",
    "nlp = stanza.Pipeline(lang=\"ca\", processors=\"tokenize,mwt,pos\")\n",
    "\n",
    "# Analizing the input text\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (tag, start_char, end_char)\n",
    "prediction = [\n",
    "    (word.pos, token.start_char, token.end_char)\n",
    "    for sent in doc.sentences\n",
    "    for token in sent.tokens\n",
    "    for word in token.words\n",
    "]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[word.text for sent in doc.sentences for word in sent.words],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"stanza/catalan\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"stanza-catalan-pos\")"
   ]
  },
  {
   "source": [
    "#### NER\n",
    "\n",
    "Stanza also offers a list of available pretrained models for NER tasks. So, let's try Russian"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import stanza\n",
    "\n",
    "input_text = (\n",
    "    \"Герра-и-Пас - одна из моих любимых книг\"  # War and Peace is one my favourite books\n",
    ")\n",
    "\n",
    "# Downloading our model, in case we don't have it cached\n",
    "stanza.download(\"ru\")\n",
    "\n",
    "# Creating the pipeline\n",
    "nlp = stanza.Pipeline(lang=\"ru\", processors=\"tokenize,ner\")\n",
    "\n",
    "# Analizing the input text\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [\n",
    "    (token.ner, token.start_char, token.end_char)\n",
    "    for sent in doc.sentences\n",
    "    for token in sent.tokens\n",
    "]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[word.text for sent in doc.sentences for word in sent.words],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"flair/russian\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"stanza-russian-ner\")"
   ]
  }
 ]
}