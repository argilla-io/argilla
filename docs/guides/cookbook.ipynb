{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rubrix Cookbook\n",
    "\n",
    "This guide is a collection of recipes. It shows examples for using Rubrix with some of the most popular NLP Python libraries. \n",
    "\n",
    "Rubrix can be used with any library or framework inside your favourite IDE, be it VS Code, or Jupyter Lab.\n",
    "\n",
    "With these examples, you'll be able to start exploring and annnotating data with these libraries and get some inspiration if your library of choice is not in this guide.\n",
    "\n",
    "If you miss a library in this guide that, leave a message in the [Rubrix Discussion forum](https://github.com/recognai/rubrix/discussions) or open an issue or PR, we'll be very happy to receive contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Transformers\n",
    "\n",
    "[Hugging Face](https://huggingface.co) has made working with NLP easier than ever before. With a few lines of code we can take a pretrained Transformer model from the [Hub](https://huggingface.co/models), start making some predictions and log them into Rubrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers datasets -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "\n",
    "For text and zeroshot classification pipelines, Rubrix's `rb.monitor` method makes it really easy to store data in Rubrix. \n",
    "\n",
    "Let's see some examples.\n",
    "\n",
    "#### Zero-shot classification pipelines\n",
    "\n",
    "Let's load a `zero-shot-classification` pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"zero-shot-classification\", model=\"typeform/distilbert-base-uncased-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `rb.monitor` method, which will asynchronously log our pipeline predictions. Now every time we predict with this pipeline the records will be logged in Rubrix. For example the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample rate = 1 means we'll be logging every prediction\n",
    "# for monitoring production models a lower rate might be preferable\n",
    "nlp = rb.monitor(nlp, dataset=\"zeroshot_example\", sample_rate=1)\n",
    "nlp(\"this is a test\", candidate_labels=['World', 'Sports', 'Business', 'Sci/Tech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will create the following dataset:\n",
    "\n",
    "![zeroshot example](./img/cookbook/zeroshot1.png)\n",
    "\n",
    "which contains the following record:\n",
    "\n",
    "![zeroshot_example](./img/cookbook/zeroshot2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to log a larger dataset we can use the batch prediction method from pipelines in a similar way. Let's load a dataset from the Hugging Face Hub and use the `dataset.map` method to parallelize the inference. The following will log the predictions for the first 20 records in the `ag_news` test dataset. You can use the same idea for any custom dataset, using `pandas.read_csv` for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ag_news\", split=\"test[0:20]\")\n",
    "\n",
    "dataset.map(\n",
    "    lambda examples: {\"predictions\": nlp(examples[\"text\"], candidate_labels=['World', 'Sports', 'Business', 'Sci/Tech'])},\n",
    "    batch_size=5, \n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text classification pipelines\n",
    "\n",
    "For text classification pipelines it will work in the same way as above. Let's see an example, this time using pandas.\n",
    "\n",
    "Let's read a dataset with tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.289494e+17</td>\n",
       "      <td>dear @Microsoft the newOoffice for Mac is grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.289766e+17</td>\n",
       "      <td>@Microsoft how about you make a system that do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.290232e+17</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.291792e+17</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.291863e+17</td>\n",
       "      <td>If I make a game as a #windows10 Universal App...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id                                           Category\n",
       "0  6.289494e+17  dear @Microsoft the newOoffice for Mac is grea...\n",
       "1  6.289766e+17  @Microsoft how about you make a system that do...\n",
       "2  6.290232e+17                                      Not Available\n",
       "3  6.291792e+17                                      Not Available\n",
       "4  6.291863e+17  If I make a game as a #windows10 Universal App..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# a url to a dataset containing tweets\n",
    "url = \"https://raw.githubusercontent.com/ajayshewale/Sentiment-Analysis-of-Text-Data-Tweets-/master/data/test.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use a sentiment analysis pipeline with the `rb.monitor` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "nlp = rb.monitor(nlp, dataset=\"text_classification_example\", sample_rate=1)\n",
    "\n",
    "for i,example in df.iterrows():\n",
    "    nlp(example.Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which will create the following dataset:\n",
    "    \n",
    "![text_classification_example](./img/cookbook/textcat1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "The above examples have shown how to store data in Rubrix, using pre-trained models. You can use Rubrix for storing datasets without predictions and without annotations, or a combination of both annotations and predictions. \n",
    "\n",
    "One of the main features of Rubrix is data annotation, which lets you rapidly create training sets. In this example, let's see how we can take labelled dataset from Rubrix to fine-tune a Hugging Face transformers text classifier.\n",
    "\n",
    "Let's read a Rubrix dataset, prepare a training set and use the `Trainer` API for fine-tuning a `distilbert-base-uncased` model. \n",
    "\n",
    "Take into account that a `zeroshot_example` is contain annotations. You can go to this dataset (if you have run the previous example) and do some manual annotation using the Annotation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import rubrix as rb\n",
    "\n",
    "# load rubrix dataset\n",
    "df = rb.load('zeroshot_example')\n",
    "\n",
    "# inputs can be dicts to support multifield classifiers, we just use the text here. \n",
    "df['text'] = df.inputs.transform(lambda r: r['text'])\n",
    "\n",
    "# we create a dict for turning our annotations (labels) into numeric ids\n",
    "label2id = {label: id for id, label in enumerate(df.annotation.unique())}\n",
    "\n",
    "\n",
    "# create 🤗 dataset from pandas with labels as numeric ids\n",
    "dataset = Dataset.from_pandas(df[['text', 'annotation']])\n",
    "dataset = dataset.map(lambda example: {'labels': label2id[example['annotation']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer\n",
    "\n",
    "# from here, it's just regular fine-tuning with 🤗 transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = dataset.map(tokenize_function, batched=True).shuffle(seed=42)\n",
    "\n",
    "trainer = Trainer(model=model, train_dataset=train_dataset)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Classification\n",
    "\n",
    "We will explore a DistilBERT NER classifier fine-tuned for NER using the conll03 English dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from transformers import pipeline\n",
    "\n",
    "input_text = \"My name is Sarah and I live in London\"\n",
    "\n",
    "# We define our HuggingFace Pipeline\n",
    "classifier = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"elastic/distilbert-base-cased-finetuned-conll03-english\",\n",
    "    framework=\"pt\",\n",
    ")\n",
    "\n",
    "# Making the prediction\n",
    "predictions = classifier(\n",
    "    input_text,\n",
    ")\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [(pred[\"entity\"], pred[\"start\"], pred[\"end\"]) for pred in predictions]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=input_text.split(),\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"https://huggingface.co/elastic/distilbert-base-cased-finetuned-conll03-english\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"zeroshot-ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "[spaCy](https://spacy.io) offers industrial-strength Natural Language Processing, with support for 64+ languages, trained pipelines, multi-task learning with pretrained Transformers, pretrained word vectors and much more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Classification\n",
    "\n",
    "We will focus our spaCy recipes into Token Classification tasks, showing you how to log data from NER and POS tagging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER\n",
    "\n",
    "For this recipe, we are going to try the French language model to extract NER entities from some sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import spacy\n",
    "\n",
    "input_text = \"Paris a un enfant et la forêt a un oiseau ; l’oiseau s’appelle le moineau ; l’enfant s’appelle le gamin\"\n",
    "\n",
    "# Loading spaCy model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Creating spaCy doc\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [(ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "\n",
    "# Building TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[token.text for token in doc],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"spacy.fr_core_news_sm\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"lesmiserables-ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging\n",
    "\n",
    "Changing very few parameters, we can make a POS tagging experiment, instead of NER. Let's try it out with the same input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import spacy\n",
    "\n",
    "input_text = \"Paris a un enfant et la forêt a un oiseau ; l’oiseau s’appelle le moineau ; l’enfant s’appelle le gamin\"\n",
    "\n",
    "# Loading spaCy model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Creating spaCy doc\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (tag, start_char, end_char)\n",
    "prediction = [(token.pos_, token.idx, token.idx + len(token)) for token in doc]\n",
    "\n",
    "# Building TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[token.text for token in doc],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"spacy.fr_core_news_sm\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"lesmiserables-pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flair\n",
    "\n",
    "It's a framework that provides a state-of-the-art NLP library, a text embedding library and a PyTorch framework for NLP. [Flair](https://github.com/flairNLP/flair) offers sequence tagging language models in English, Spanish, Dutch, German and many more, and they are also hosted on [HuggingFace Model Hub](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Classification (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = rb.monitor(SequenceTagger.load(\"flair/ner-english\"), dataset=\"flair-example\", sample_rate=1.0)\n",
    "\n",
    "# make example sentence\n",
    "sentence = Sentence(\"George Washington went to Washington\")\n",
    "\n",
    "# predict NER tags. This will log the prediction in Rubrix\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Let's read a Rubrix dataset, prepare a training set, save to `.txt` for loading with flair `ColumnCorpus` and train with flair `SequenceTagger` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "import rubrix as rb\n",
    "\n",
    "\n",
    "# 1. Load the dataset from Rubrix (your own NER/token classification task)\n",
    "#    Note: we initiate the 'tars_ner_wnut_17' from \"🔫 Zero-shot Named Entity Recognition with Flair\" tutorial\n",
    "#   (reference: https://rubrix.readthedocs.io/en/stable/tutorials/08-zeroshot_ner.html)\n",
    "train_dataset = rb.load(\"tars_ner_wnut_17\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pre-processing to BIO scheme before saving as .txt file\n",
    "\n",
    "# Use original predictions as annotations for demonstration purposes, in a real use case you would use the `annotations` instead\n",
    "prediction_list = train_dataset.prediction\n",
    "text_list = train_dataset.text\n",
    "\n",
    "annotation_list = []\n",
    "idx = 0\n",
    "for ner_list in prediction_list:\n",
    "    new_ner_list = []\n",
    "    for val in ner_list:\n",
    "        new_ner_list.append((text_list[idx][val[1]:val[2]], val[0]))\n",
    "    annotation_list.append(new_ner_list)\n",
    "    idx += 1\n",
    "\n",
    "\n",
    "ready_data = pd.DataFrame()\n",
    "ready_data['text'] = text_list\n",
    "ready_data['annotation'] = annotation_list\n",
    "\n",
    "\n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "    return match_list, string\n",
    "\n",
    "\n",
    "def mark_sentence(s, match_list):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    word_dict = {}\n",
    "    for word in s.split():\n",
    "        word_dict[word] = 'O'\n",
    "    for start, end, e_type in match_list:\n",
    "        temp_str = s[start:end]\n",
    "        tmp_list = temp_str.split()\n",
    "        if len(tmp_list) > 1:\n",
    "            word_dict[tmp_list[0]] = 'B-' + e_type\n",
    "            for w in tmp_list[1:]:\n",
    "                word_dict[w] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict[temp_str] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def create_data(df, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    '''\n",
    "    with open(filepath, 'w') as f:\n",
    "        for text, annotation in zip(df.text, df.annotation):\n",
    "            text_ = text\n",
    "            match_list = []\n",
    "            for i in annotation:\n",
    "                a, text_ = matcher(text, i[0])\n",
    "                match_list.append((a[0][0], a[0][1], i[1]))\n",
    "            d = mark_sentence(text, match_list)\n",
    "            for i in d.keys():\n",
    "                f.writelines(i + ' ' + d[i] + '\\n')\n",
    "            f.writelines('\\n')\n",
    "\n",
    "\n",
    "# path to save the txt file.\n",
    "filepath = 'train.txt'\n",
    "\n",
    "# creating the file.\n",
    "create_data(ready_data, filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load to Flair ColumnCorpus\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "# directory where the data resides\n",
    "data_folder = './'\n",
    "\n",
    "# initializing the corpus\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='train.txt',\n",
    "                              test_file=None,\n",
    "                              dev_file=None)\n",
    "\n",
    "\n",
    "# 4. Define training parameters\n",
    "\n",
    "# tag to predict\n",
    "label_type = 'ner'\n",
    "\n",
    "# make tag dictionary from the corpus\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "\n",
    "# initialize embeddings\n",
    "embedding_types = [\n",
    "    WordEmbeddings('glove'),\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(\n",
    "    embeddings=embedding_types)\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type=label_type,\n",
    "                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('token-classification',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Let's read a Rubrix dataset, prepare a training set, save to `.csv` for loading with flair `CSVClassificationCorpus` and train with flair `ModelTrainer` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "import rubrix as rb\n",
    "\n",
    "\n",
    "# 1. Load the dataset from Rubrix\n",
    "limit_num = 2048\n",
    "train_dataset = rb.load(\"tweet_eval_emojis\", limit=limit_num)\n",
    "\n",
    "# 2. Pre-processing training pandas dataframe\n",
    "ready_input = [row['text'] for row in train_dataset.inputs]\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df['text'] = ready_input\n",
    "train_df['label'] = train_dataset['annotation']\n",
    "\n",
    "# 3. Save as csv with tab delimiter\n",
    "train_df.to_csv('train.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Read the with CSVClassificationCorpus\n",
    "data_folder = './'\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "label_type = \"label\"\n",
    "column_name_map = {1: \"text\", 2: \"label\"}\n",
    "\n",
    "corpus = CSVClassificationCorpus(\n",
    "    data_folder, column_name_map, skip_header=True, delimiter='\\t', label_type=label_type)\n",
    "\n",
    "# 5. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "\n",
    "\n",
    "# 6. initialize transformer document embeddings (many models are available)\n",
    "document_embeddings = TransformerDocumentEmbeddings(\n",
    "    'distilbert-base-uncased', fine_tune=True)\n",
    "\n",
    "\n",
    "# 7. create the text classifier\n",
    "classifier = TextClassifier(\n",
    "    document_embeddings, label_dictionary=label_dict, label_type=label_type)\n",
    "\n",
    "# 8. initialize trainer with AdamW optimizer\n",
    "trainer = ModelTrainer(classifier, corpus, optimizer=torch.optim.AdamW)\n",
    "\n",
    "\n",
    "# 9. run training with fine-tuning\n",
    "trainer.train('./emojis-classification',\n",
    "              learning_rate=5.0e-5,\n",
    "              mini_batch_size=4,\n",
    "              max_epochs=4,\n",
    "              scheduler=OneCycleLR,\n",
    "              embeddings_storage_mode='none',\n",
    "              weight_decay=0.,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a prediction with flair `TextClassifier` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import TextClassifier\n",
    "\n",
    "classifier = TextClassifier.load('./emojis-classification/best-model.pt')\n",
    "\n",
    "# create example sentence\n",
    "sentence = Sentence('Farewell, Charleston! The memories are sweet #mimosa #dontwannago @ Virginia on King')\n",
    "\n",
    "# predict class and print\n",
    "classifier.predict(sentence)\n",
    "\n",
    "print(sentence.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-shot and Few-shot classifiers\n",
    "\n",
    "Flair enables you to use few-shot and zero-shot learning for text classification with Task-aware representation of sentences (TARS), introduced by Halder et al. (2020), see [Flair's  documentation](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_10_TRAINING_ZERO_SHOT_MODEL.md) for more details. \n",
    "\n",
    "Let's see an example of the base zero-shot TARS model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from flair.models import TARSClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "# Load our pre-trained TARS model for English\n",
    "tars = TARSClassifier.load('tars-base')\n",
    "\n",
    "# Define labels\n",
    "labels = [\"happy\", \"sad\"]\n",
    "\n",
    "# Create a sentence\n",
    "input_text = \"I am so glad you liked it!\"\n",
    "sentence = Sentence(input_text)\n",
    "\n",
    "# Predict for these labels\n",
    "tars.predict_zero_shot(sentence, labels)\n",
    "\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (label, probability)\n",
    "prediction = [(pred.value, pred.score) for pred in sentence.labels]\n",
    "\n",
    "# Building a TextClassificationRecord\n",
    "record = rb.TextClassificationRecord(\n",
    "    inputs=input_text,\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"tars-base\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"en-emotion-zeroshot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom and pre-trained classifiers\n",
    "\n",
    "Let’s see an example with the German offensive language classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "input_text = \"Du erzählst immer Quatsch.\" \n",
    "\n",
    "# Load our pre-trained classifier\n",
    "classifier = TextClassifier.load(\"de-offensive-language\")\n",
    "\n",
    "# Creating Sentence object\n",
    "sentence = Sentence(input_text)\n",
    "\n",
    "# Make the prediction\n",
    "classifier.predict(sentence, return_probabilities_for_all_classes=True)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (label, probability)\n",
    "prediction = [(pred.value, pred.score) for pred in sentence.labels]\n",
    "\n",
    "# Building a TextClassificationRecord\n",
    "record = rb.TextClassificationRecord(\n",
    "    inputs=input_text,\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"de-offensive-language\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"german-offensive-language\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging\n",
    "\n",
    "In the following snippet we will use de multilingual POS tagging model from Flair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "input_text = \"George Washington went to Washington. Dort kaufte er einen Hut.\"\n",
    "\n",
    "# Loading our POS tagging model from flair\n",
    "tagger = SequenceTagger.load(\"flair/upos-multi\")\n",
    "\n",
    "# Creating Sentence object\n",
    "sentence = Sentence(input_text)\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [\n",
    "    (entity.get_labels()[0].value, entity.start_pos, entity.end_pos)\n",
    "    for entity in sentence.get_spans()\n",
    "]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[token.text for token in sentence],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"flair/upos-multi\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"flair-pos-tagging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza\n",
    "\n",
    "[Stanza](https://stanfordnlp.github.io/stanza/) is a collection of efficient tools for many NLP tasks and processes, all in one library. It's maintained by the [Standford NLP Group](https://nlp.stanford.edu). We are going to take a look at a few interactions that can be done with Rubrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "\n",
    "Let's start by using a Sentiment Analysis model to log some `TextClassificationRecords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import stanza\n",
    "\n",
    "input_text = (\n",
    "    \"There are so many NLP libraries available, I don't know which one to choose!\"\n",
    ")\n",
    "\n",
    "# Downloading our model, in case we don't have it cached\n",
    "stanza.download(\"en\")\n",
    "\n",
    "# Creating the pipeline\n",
    "nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,sentiment\")\n",
    "\n",
    "# Analizing the input text\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# This model returns 0 for negative, 1 for neutral and 2 for positive outcome.\n",
    "# We are going to log them into Rubrix using a dictionary to translate numbers to labels.\n",
    "num_to_labels = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "\n",
    "# Build a prediction entities list\n",
    "# Stanza, at the moment, only output the most likely label without probability.\n",
    "# So we will suppouse Stanza predicts the most likely label with 1.0 probability, and the rest with 0.\n",
    "entities = []\n",
    "\n",
    "for _, sentence in enumerate(doc.sentences):\n",
    "    for key in num_to_labels:\n",
    "        if key == sentence.sentiment:\n",
    "            entities.append((num_to_labels[key], 1))\n",
    "        else:\n",
    "            entities.append((num_to_labels[key], 0))\n",
    "\n",
    "# Building a TextClassificationRecord\n",
    "record = rb.TextClassificationRecord(\n",
    "    inputs=input_text,\n",
    "    prediction=entities,\n",
    "    prediction_agent=\"stanza/en\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"stanza-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Classification\n",
    "\n",
    "Stanza offers so many different pretrained language models for Token Classification Tasks, and the list does not stop growing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging\n",
    "\n",
    "We can use one of the many UD models, used for POS tags, morphological features and syntantic relations. UD stands for [Universal Dependencies](https://universaldependencies.org), the framework where these models has been trained. For this example, let's try to extract POS tags of some Catalan lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import stanza\n",
    "\n",
    "# Loading a cool Obrint Pas lyric\n",
    "input_text = \"Viure sempre corrent, avançant amb la gent, rellevant contra el vent, transportant sentiments.\"\n",
    "\n",
    "# Downloading our model, in case we don't have it cached\n",
    "stanza.download(\"ca\")\n",
    "\n",
    "# Creating the pipeline\n",
    "nlp = stanza.Pipeline(lang=\"ca\", processors=\"tokenize,mwt,pos\")\n",
    "\n",
    "# Analizing the input text\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (tag, start_char, end_char)\n",
    "prediction = [\n",
    "    (word.pos, token.start_char, token.end_char)\n",
    "    for sent in doc.sentences\n",
    "    for token in sent.tokens\n",
    "    for word in token.words\n",
    "]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[word.text for sent in doc.sentences for word in sent.words],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"stanza/catalan\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"stanza-catalan-pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER\n",
    "\n",
    "Stanza also offers a list of available pretrained models for NER tasks. So, let's try Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "import stanza\n",
    "\n",
    "input_text = (\n",
    "    \"Герра-и-Пас - одна из моих любимых книг\"  # War and Peace is one my favourite books\n",
    ")\n",
    "\n",
    "# Downloading our model, in case we don't have it cached\n",
    "stanza.download(\"ru\")\n",
    "\n",
    "# Creating the pipeline\n",
    "nlp = stanza.Pipeline(lang=\"ru\", processors=\"tokenize,ner\")\n",
    "\n",
    "# Analizing the input text\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# Creating the prediction entity as a list of tuples (entity, start_char, end_char)\n",
    "prediction = [\n",
    "    (token.ner, token.start_char, token.end_char)\n",
    "    for sent in doc.sentences\n",
    "    for token in sent.tokens\n",
    "]\n",
    "\n",
    "# Building a TokenClassificationRecord\n",
    "record = rb.TokenClassificationRecord(\n",
    "    text=input_text,\n",
    "    tokens=[word.text for sent in doc.sentences for word in sent.words],\n",
    "    prediction=prediction,\n",
    "    prediction_agent=\"flair/russian\",\n",
    ")\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(records=record, name=\"stanza-russian-ner\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b709380ea7d1cb2eb4650c0f11ac7e002ec6a534602815725771481b4784238c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
