{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate new data to improve Misogyny Detection models using Rubrix and biome.text\n",
    "\n",
    "Hey there! In this guide, we will show you how we used Rubrix to annotate new data and use this new data to improve existing Deep Learning models. Our use case was Automatic Misogyny Detection (AMI): NLP models able to detect the underlying misogyny on a given text. Ground-breaking work is being made every year on this subject, with shared tasks and new models that push the performance of these models closer and closer to be implemented in apps, social networks, and other digital environments. \n",
    "\n",
    "Alongside Rubrix, we used our amazing NLP libary [biome.text](https://github.com/recognai/biome-text) to train models with a simple workflow. Rubrix is compatible with almost any library or service, so we were able to work back and forth with both of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our objective\n",
    "\n",
    "One of our team members, Ignacio, was preparing his Bachelor's thesis on the subject of Automatic Misogyny Detection. In this setting, we wanted to use the potential of Rubrix on our favor, going beyond the classic linear workflows that are so common in Deep Learning: gathering some data, preprocessing it, training a model and start making inference. Rubrix breaks that scheme, and allowed us to use a man-in-the-middle approach with retraining and fine-tuning and to add new data in follow-up trainings. \n",
    "\n",
    "We want to talk you more about it in this guide. It is not intended to be a tutorial, even though there will be snippets of code to reproduce our process and get to similar result (we will simplify some aspects of it to keep it light-weighted), but more of a story. We will focus on the process and on what we learnt along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The Bachelor's thesis of Ignacio was focused on the Spanish language, and datasets of misogyny in Spanish text is nothing but scarce. This was the first hardship: finding datasets to start training some models. Luckily, there is a very important community of shared task focused on the matter, covering a lot of non-English languages. We started working with data from [IberEval 2018](https://sites.google.com/view/ibereval-2…), a shared-task that offered a compilation of tweets, analyzed by experts and classified into 5 different misogyny categories. We started training our first model with around three thousand instances of annotated data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Here are some procedures we've made for this guide that were kept on the background. If you want to reproduce all our steps, including the training of models and some extra parts, we will give provide with cells to do so! Feel free to change anything and try new stuff, and tell us if you have some doubts our find something cool at our [Github forum](https://github.com/recognai/rubrix/discussions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our first model\n",
    "\n",
    "To reproduce a simplified version of the first trained model, before annotation, you can execute the following cells. We've already searched for good-enough configurations, so you can skip that step.\n",
    "\n",
    "Let's start by loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets\n",
    "training_ds = Dataset.from_csv('annotation_data/training_full_df.csv', index=False)\n",
    "test_ds = Dataset.from_csv('annotation_data/test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating NLP pipelines with biome.text is quick and convenient! We performed an HPO process on the background, to find suitable hyperparameters for this domain, so let's use them to create our first AMI model. Note that we're making a pipeline with BETO, a Spanish Transformer model, at the head. To learn more about what a Transformer is, please visit the [Transformer guide of biome.text](https://recognai.github.io/biome-text/v3.0.0/documentation/tutorials/4-Using_Transformers_in_biome_text.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {\n",
    "    \"name\": \"AMI_first_model\",\n",
    "    \"features\": {\n",
    "        \"transformers\": {\n",
    "            \"model_name\": \"dccuchile/bert-base-spanish-wwm-cased\", # BETO model\n",
    "            \"trainable\": True,\n",
    "            \"max_length\": 280,  # As we are working with data from Twitter, this is our max length\n",
    "        }\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"type\": \"TextClassification\",\n",
    "        \n",
    "        # These are the possible misogyny categories.\n",
    "        \"labels\": [\n",
    "            'sexual_harassment',\n",
    "             'dominance',\n",
    "             'discredit',\n",
    "             'stereotype',\n",
    "             'derailing',\n",
    "             'non-sexist'\n",
    "        ],\n",
    "        \"pooler\": {\n",
    "            \"type\": \"lstm\",\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_size\": 256,\n",
    "            \"bidirectional\": True,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "pl = Pipeline.from_config(pipeline_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfiguration(\n",
    "    optimizer={\n",
    "        \"type\": \"adamw\",\n",
    "        \"lr\": 0.000023636840436059507,\n",
    "        \"weight_decay\": 0.01438297700463013,\n",
    "    },\n",
    "    batch_size=8,\n",
    "    max_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    pipeline=pl,\n",
    "    train_dataset=training_ds,\n",
    "    valid_dataset=test_ds,\n",
    "    trainer_config=trainer_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `trainer.fit()` stops, the results of the training and the obtained model will be in the output folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make some predictions, and take a look at the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict(\"Las mujeres no deberían tener derecho a voto\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('rubrix': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}