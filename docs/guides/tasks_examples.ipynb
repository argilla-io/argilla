{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tasks Templates\n",
    "\n",
    "Hi there! In this article we wanted to share some examples of our supported tasks, so you can go from zero to hero as fast as possible. We are going to cover those task present in our [supported tasks list](https://docs.rubrix.ml/en/stable/getting_started/supported_tasks.html), so don't forget to stop by and take a look.\n",
    "\n",
    "The tasks are divided into their different category, from text classification to token classification. We will update these article, as well as the supported task list, when a new task gets added to Rubrix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text Classification\n",
    "\n",
    "Text classification deals with predicting in which categories a text fits. As if you’re shown an image you could quickly tell if there’s a dog or a cat in it, we build NLP models to distinguish between a Jane Austen’s novel or a Charlotte Bronte’s poem. It’s all about feeding models with labeled examples and see how it start predicting over the very same labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text Categorization\n",
    "\n",
    "This is a general example for the Text Classification family of tasks. Here, we will try to assign pre-defined categories to sentences and texts. Here, the possibilities are endless! Topic categorization, spam detection, and a vast etcétera.\n",
    "\n",
    "For our example, we are using the [SequeezeBERT](https://huggingface.co/typeform/squeezebert-mnli) zero-shot classifier for predicting the topic of a given text, in three different labels: politics, sports and technology. We are also using [AG](https://huggingface.co/datasets/ag_news), a collections of news."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import rubrix as rb\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Loading our dataset\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[0:20]\")\n",
    "\n",
    "# Define our HuggingFace Pipeline\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"typeform/squeezebert-mnli\",\n",
    "    framework=\"pt\",\n",
    ")\n",
    "\n",
    "records = []\n",
    "\n",
    "for record in dataset:\n",
    "\n",
    "    # Making the prediction\n",
    "    prediction = classifier(\n",
    "        record[\"text\"],\n",
    "        candidate_labels=[\n",
    "            \"politics\",\n",
    "            \"sports\",\n",
    "            \"technology\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Creating the prediction entity as a list of tuples (label, probability)\n",
    "    prediction = list(zip(prediction[\"labels\"], prediction[\"scores\"]))\n",
    "\n",
    "    # Appending to the record list\n",
    "    records.append(\n",
    "        rb.TextClassificationRecord(\n",
    "            inputs=record[\"text\"],\n",
    "            prediction=prediction,\n",
    "            prediction_agent=\"https://huggingface.co/typeform/squeezebert-mnli\",\n",
    "            metadata={\"split\": \"train\"},\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(\n",
    "    records=records,\n",
    "    name=\"text-categorization\",\n",
    "    tags={\n",
    "        \"task\": \"text-categorization\",\n",
    "        \"phase\": \"data-analysis\",\n",
    "        \"dataset\": \"ag_news\",\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "In this kind of projects, we want our models to be able to detect the polarity of an input. Categories like *positive*, *negative* or *neutral* are often used. \n",
    "\n",
    "For this example, we are going to use an [Amazon review polarity dataset](https://huggingface.co/datasets/amazon_polarity), and a sentiment analysis [roBERTa model](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment?text=I+like+you.+I+love+you), which returns `LABEL 0` for positive, `LABEL 1` for neutral and `LABEL 2` for negative. We will handle that in the code."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import rubrix as rb\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Loading our dataset\n",
    "dataset = load_dataset(\"amazon_polarity\", split=\"train[0:20]\")\n",
    "\n",
    "# Define our HuggingFace Pipeline\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    framework=\"pt\",\n",
    "    return_all_scores=True,\n",
    ")\n",
    "\n",
    "# Make a dictionary to translate labels to a friendly-language\n",
    "translate_labels = {\n",
    "    \"LABEL_0\": \"positive\",\n",
    "    \"LABEL_1\": \"neutral\",\n",
    "    \"LABEL_2\": \"negative\",\n",
    "}\n",
    "\n",
    "records = []\n",
    "\n",
    "for record in dataset:\n",
    "\n",
    "    # Making the prediction\n",
    "    predictions = classifier(\n",
    "        record[\"content\"],\n",
    "    )\n",
    "\n",
    "    # Creating the prediction entity as a list of tuples (label, probability)\n",
    "    prediction = [\n",
    "        (translate_labels[prediction[\"label\"]], prediction[\"score\"])\n",
    "        for prediction in predictions[0]\n",
    "    ]\n",
    "\n",
    "    # Appending to the record list\n",
    "    records.append(\n",
    "        rb.TextClassificationRecord(\n",
    "            inputs=record[\"content\"],\n",
    "            prediction=prediction,\n",
    "            prediction_agent=\"https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "            metadata={\"split\": \"train\"},\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(\n",
    "    records=records,\n",
    "    name=\"sentiment-analysis\",\n",
    "    tags={\n",
    "        \"task\": \"sentiment-analysis\",\n",
    "        \"phase\": \"data-annotation\",\n",
    "        \"dataset\": \"amazon-polarity\",\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Semantic Textual Similarity\n",
    "\n",
    "This task is all about how close or far a given text is from any other. We want models that outputs a value of closeness between two inputs.\n",
    "\n",
    "For our example, we will be using [ASSIN dataset](https://huggingface.co/datasets/assin), with pair of sentences from Portuguese and Brazilian newspaper. Our model will be a [multilingual Transformer](https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1) trained specifically for this task. \n",
    "\n",
    "As HuggingFace Transformers does not support natively this task, we will be using the [Sentence Transformer](https://www.sbert.net) framework. For more information about how to make these predictions with HuggingFace Transformer, please visit this [link](https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1#usage-huggingface-transformers)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%pip install -U sentence-transformers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "import rubrix as rb\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Loading our dataset\n",
    "dataset = load_dataset(\"assin\", split=\"train[0:20]\")\n",
    "\n",
    "# Loading the model\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for record in dataset:\n",
    "\n",
    "    # Obtain the embeddings\n",
    "    sentence1 = model.encode(record[\"premise\"], convert_to_tensor=True)\n",
    "    sentence2 = model.encode(record[\"hypothesis\"], convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine-similarits\n",
    "    cosine_scores = util.pytorch_cos_sim(sentence1, sentence2)\n",
    "\n",
    "    # Appending to the record list\n",
    "    records.append(\n",
    "        rb.TextClassificationRecord(\n",
    "            inputs={\n",
    "                \"sentence 1\": record[\"premise\"],\n",
    "                \"sentence 2\": record[\"hypothesis\"],\n",
    "            },\n",
    "            prediction=[(\"similarity\", cosine_scores[0][0].item())],\n",
    "            prediction_agent=\"https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1\",\n",
    "            metadata={\"split\": \"train\"},\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Logging into Rubrix\n",
    "rb.log(\n",
    "    records=records,\n",
    "    name=\"textual-similarity\",\n",
    "    tags={\n",
    "        \"task\": \"similarity\",\n",
    "        \"dataset\": \"assin\",\n",
    "    },\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No config specified, defaulting to: assin/full\n",
      "Reusing dataset assin (/Users/ignacio/.cache/huggingface/datasets/assin/full/1.0.0/348353043dabf6433ce5c3e462055336b38037faffa7760d2f35c0b4c569f4b2)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BulkResponse(dataset='textual-similarity', processed=20, failed=0)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('rubrix': conda)"
  },
  "interpreter": {
   "hash": "4cac2ad44382dcbde9c9d45667b9ac0fec163e57feefe7fa8ea5d11fd16eb612"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}