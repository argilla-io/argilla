{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate new data to improve NLP models using Rubrix and biome.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Hey there! In this guide, we will show you how to use Rubrix to annotate new data and use this new data to improve existing Deep Learning models. Our use case will be Automatic Misogyny Detection (AMI): Deep Learning models able to detect the underlying misogyny on a given text. Ground-breaking work is being made every year on this subject, with shared tasks and new models that push the performance of these models closer and closer to be implemented in apps, social networks, and other digital environments. \n",
    "\n",
    "To train these NLP models we are going to use [biome.text](https://github.com/recognai/biome-text), an open-source library to train models with a simple workflow. Rubrix is compatible with almost any library or service, so we will work back and forth with both of them. \n",
    "\n",
    "The data used to feed the models and make the annotations comes from the [IberEval 2018](https://sites.google.com/view/ibereval-2…) shared task. It's a compilation of tweets, analyzed by experts and classified into 5 different misogyny categories. We are also making the specific datasets used in each step of this guide available, so they can be reproduced in the best way possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "If you want to reproduce this code, make sure that all the libraries needed to run this guide are installed and imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U git+https://github.com/recognai/biome-text\n",
    "%pip install rubrix\n",
    "%pip install pandas\n",
    "exit(0)  # Force restart of the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:erase\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_API_KEY'] = '7bd265df21100baa9767bb9f69108bc417db4b4a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import *\n",
    "import pandas as pd\n",
    "import rubrix as rb\n",
    "\n",
    "#TODO: erase\n",
    "from biome.text import *\n",
    "from biome.text.hpo import TuneExperiment\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray import tune\n",
    "import math\n",
    "\n",
    "import wandb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets\n",
    "\n",
    "Let's load some prepared datasets we've made to quickly train our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets\n",
    "training_ds = Dataset.from_csv('annotation_data/training_full_df.csv')\n",
    "test_ds = Dataset.from_csv('annotation_data/test_df.csv')\n",
    "\n",
    "# Removing non-useful generated columns\n",
    "training_ds = training_ds.map(remove_columns=[\"Unnamed: 0\", \"id\"])\n",
    "test_ds = test_ds.map(remove_columns=[\"Unnamed: 0\", \"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a quick look at the columns and the number of rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the first model\n",
    "\n",
    "Creating NLP pipelines with biome.text is quick and convenient! We performed an HPO process on the background, to find suitable hyperparameters for this domain, so let's use them to create our first AMI model. Note that we're making a pipeline with BETO, a Spanish Transformer model, at the head. To learn more about what a Transformer is, please visit the [Transformer guide of biome.text](https://recognai.github.io/biome-text/v3.0.0/documentation/tutorials/4-Using_Transformers_in_biome_text.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {\n",
    "    \"name\": \"AMI_first_model\",\n",
    "    \"features\": {\n",
    "        \"transformers\": {\n",
    "            \"model_name\": \"dccuchile/bert-base-spanish-wwm-cased\", # BETO model\n",
    "            \"trainable\": True,\n",
    "            \"max_length\": 280,  # As we are working with data from Twitter, this is our max length\n",
    "        }\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"type\": \"TextClassification\",\n",
    "        \n",
    "        # These are the possible misogyny categories. 0 indicates it is non-sexist\n",
    "        \"labels\": [\n",
    "            'sexual_harassment',\n",
    "             'dominance',\n",
    "             'discredit',\n",
    "             'stereotype',\n",
    "             'derailing',\n",
    "             'passive',\n",
    "             'active',\n",
    "             '0'\n",
    "        ],\n",
    "        \"pooler\": {\n",
    "            \"type\": \"lstm\",\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_size\": 256,\n",
    "            \"bidirectional\": True,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "pl = Pipeline.from_config(pipeline_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "trainer_dict = {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"adamw\",\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-4),\n",
    "        \"weight_decay\": tune.loguniform(2e-3, 6e-2 )\n",
    "    },\n",
    "    \"learning_rate_scheduler\": {\n",
    "        \"type\": \"linear_with_warmup\",\n",
    "        \"num_epochs\": 10,\n",
    "        \"num_steps_per_epoch\": int(math.floor(len(training_ds)/batch_size)),\n",
    "        \"warmup_steps\": 100,\n",
    "    },\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": 10,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfiguration(\n",
    "    optimizer={\n",
    "        \"type\": \"adamw\",\n",
    "        \"lr\": 0.000023636840436059507,\n",
    "        \"weight_decay\": 0.01438297700463013,\n",
    "    },\n",
    "    batch_size=8,\n",
    "    max_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    pipeline=pl,\n",
    "    train_dataset=training_ds,\n",
    "    valid_dataset=test_ds,\n",
    "    trainer_config=trainer_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `trainer.fit()` stops, the results of the training and the obtained model will be in the output folder. Nevertheless, we know that this training can long on non-dedicated machines, so we also provide the obtained model to download and import. If you don't want to manually train the model, run the cell below, which downloads and imports the trained model into a biome pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:Descargar e importar código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: eliminar\n",
    "pl = Pipeline.from_pretrained(\"model_annotation_guide.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make some predictions, and take a look at the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict(\"Las mujeres no deberían tener derecho a voto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating as a single agent\n",
    "\n",
    "When we said that we prepared some datasets with the tweets from IberEval 2018, we might have lied a little bit. We prepared the datasets with almost all tweets from IberEval 2018, but we also make a small compilation of 15 instances for you to start annotating. Picture you, after training your first model, trying to push a little bit its performance, or include some new data to cover as many different cases as possible. You came across new instances, and you want to annotate them and include them in a follow-up training. This is where Rubrix comes along. \n",
    "\n",
    "In this chapter of the guide we will show you how to:\n",
    "* Import datasets to Rubrix (in our case, from a csv file).\n",
    "* Annotate datasets using Rubrix.\n",
    "* Export the annotated datasets to use them in your pipelines.\n",
    "\n",
    "And we will cover the scenario of a single annotation agent. In the next chapter, we will give you some insight on how to annotate in teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging a dataset into Rubrix\n",
    "\n",
    "Let's start by logging the dataset into a Rubrix dataset. As these instances were initially annotated by the IberEval team, we can treat them as predictions. In the annotation process, therefore, we will decide if we agree with those predictions or not. If we take raw data, we wouldn't have these predictions to support our annotation process, but that's okay too!\n",
    "\n",
    "The first step is to download the datasets. Then, we will iterate through all the instances, logging them into Rubrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: download to_annotate.csv\n",
    "\n",
    "annotation_ds = Dataset.from_csv('annotation_data/to_annotate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []    # here we will store the TextClassificationRecord objects\n",
    "\n",
    "# Possible labels, used to build the predictions\n",
    "labels = ['sexual_harassment','dominance','discredit','stereotype','derailing','passive','active','0']\n",
    "\n",
    "for record in annotation_ds:\n",
    "\n",
    "    # Prediction list of each record in the dataset\n",
    "    predictions = []\n",
    "\n",
    "    # We build the prediction list with tuples.\n",
    "    for label in labels:\n",
    "\n",
    "        # Ff the label is the one predicted in the dataset, it has a score of 1\n",
    "        if label==record[\"label\"]:\n",
    "            pred = (label, 1)\n",
    "            predictions.append(pred)\n",
    "\n",
    "        # Else, it has a score of 0\n",
    "        else:\n",
    "            pred = (label, 0)\n",
    "            predictions.append(pred)\n",
    "\n",
    "    # Appending the record into the list\n",
    "    records.append(rb.TextClassificationRecord(\n",
    "        inputs=record[\"text\"],\n",
    "        prediction=predictions,\n",
    "        prediction_agent=\"IberEval 2018\",\n",
    "        metadata={'id': record[\"id\"]},\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Logging the records into Rubrix\n",
    "rb.log(records=records, name=\"annotation_misogyny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've logged our annotation dataset into Rubrix, we can start annotating on the UI. We know that first times can be challenging, so here we have some instructions and a GIF to show you around.\n",
    "\n",
    "1. Open Rubrix in your browser. If you're running it locally, it is usually running on [http://localhost:6900](http://localhost:6900).\n",
    "2. Select the `annotation_misogyny` dataset.\n",
    "3. On the upper-right corner, toggle the `Annotation mode`. \n",
    "4. Start selecting the categories that you think fit the input text. If you don't know Spanish, don't worry! 15 instances are not going to change the final model that much, and you will still learn how to annotate.\n",
    "5. For each instance you can annotate a category by pressing it, discarding the record (if you think it does not fit the problem domain), or leave it without an annotation.\n",
    "\n",
    "![Example of Annotation](https://imgur.com/cdZpkXp.gif)\n",
    "\n",
    "If you're wondering why we annotated that instance as 'non-sexist' is because we are trying to make a model capable to differentiate if the input text is being misogynistic or if it is talking about something misogynistic that happened. This second case is considered non-sexist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We have annotated a dataset as a single annotator, and these new data can be used to retrain and fine-tune our NLP model."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('rubrix': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}