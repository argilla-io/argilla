{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate new data to improve NLP models using Rubrix and biome.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Hey there! In this guide, we will show you how to use Rubrix to annotate new data, and use this new data to improve existing Deep Learning models. Our use case will be Automatic Misogyny Detection (AMI): Deep Learning models able to detect the underlying misogyny on a given text. Ground-breaking work is being made every year on this subject, with shared tasks and new models that push the performance of these models closer and closer to be implemented in apps, social networks and other digital environments. \n",
    "\n",
    "To train these NLP models we are going to use [biome.text](https://github.com/recognai/biome-text), an open-source library to train models with a simple workflow. Rubrix is compatible with almost any library or service, so we will work back and forth with both of them. \n",
    "\n",
    "The data used to feed the models and make the annotations comes from the [IberEval 2018](https://sites.google.com/view/ibereval-2â€¦) shared task. It's a compilation of tweets, analyzed by experts and classified in 5 different misogyny categories. We are also making the specific datasets used in each step of this guide available, so it can be reproduced in the best way possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "If you want to reproduce this code, make sure that all the libraries needed to run this guide are installed and imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U git+https://github.com/recognai/biome-text\n",
    "%pip install rubrix\n",
    "%pip install pandas\n",
    "exit(0)  # Force restart of the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:erase\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_API_KEY'] = '7bd265df21100baa9767bb9f69108bc417db4b4a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignaciotalaveracepeda/anaconda3/envs/rubrix/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from biome.text import *\n",
    "import pandas as pd\n",
    "import rubrix as rb\n",
    "\n",
    "#TODO: erase\n",
    "from biome.text import *\n",
    "from biome.text.hpo import TuneExperiment\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray import tune\n",
    "import math\n",
    "\n",
    "import wandb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets\n",
    "\n",
    "Let's load some prepared datasets we made to quickly train our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-408234f3167ff690\n",
      "Reusing dataset csv (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/csv/default-408234f3167ff690/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-ae7b327d976b362b\n",
      "Reusing dataset csv (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/csv/default-ae7b327d976b362b/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "# Loading the datasets\n",
    "training_ds = Dataset.from_csv('annotation_data/training_full_df.csv')\n",
    "test_ds = Dataset.from_csv('annotation_data/test_df.csv')\n",
    "\n",
    "# Removing non-useful generated columns\n",
    "training_ds = training_ds.map(remove_columns=[\"Unnamed: 0\", \"id\"])\n",
    "test_ds = test_ds.map(remove_columns=[\"Unnamed: 0\", \"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a quick look at the columns and the number of rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 3292\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the first model\n",
    "\n",
    "Creating NLP pipelines with biome.text is quick and convenient! We performed a HPO process on the background, to find suitable hyperparameters for this domain, so let's use them to create our first AMI model. Note that we're making a pipeline with BETO, an Spanish Transformer model, at the head. To learn more about what a Transformer is, please visit the [Transformer guide of biome.text](https://recognai.github.io/biome-text/v3.0.0/documentation/tutorials/4-Using_Transformers_in_biome_text.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pipeline_dict = {\n",
    "    \"name\": \"AMI_first_model\",\n",
    "    \"features\": {\n",
    "        \"transformers\": {\n",
    "            \"model_name\": \"dccuchile/bert-base-spanish-wwm-cased\", # BETO model\n",
    "            \"trainable\": True,\n",
    "            \"max_length\": 280,  # As we are working with data from Twitter, this is our max length\n",
    "        }\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"type\": \"TextClassification\",\n",
    "        # These are the possible misogyny categories. 0 indicates it is non-sexist\n",
    "        \"labels\": [\n",
    "            'sexual_harassment',\n",
    "             'dominance',\n",
    "             'discredit',\n",
    "             'stereotype',\n",
    "             'derailing',\n",
    "             'passive',\n",
    "             'active',\n",
    "             '0'\n",
    "        ],\n",
    "        \"pooler\": {\n",
    "            \"type\": \"lstm\",\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_size\": 256,\n",
    "            \"bidirectional\": True,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "pl = Pipeline.from_config(pipeline_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "trainer_dict = {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"adamw\",\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-4),\n",
    "        \"weight_decay\": tune.loguniform(2e-3, 6e-2 )\n",
    "    },\n",
    "    \"learning_rate_scheduler\": {\n",
    "        \"type\": \"linear_with_warmup\",\n",
    "        \"num_epochs\": 10,\n",
    "        \"num_steps_per_epoch\": int(math.floor(len(training_ds)/batch_size)),\n",
    "        \"warmup_steps\": 100,\n",
    "    },\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": 10,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfiguration(\n",
    "    optimizer={\n",
    "        \"type\": \"adamw\",\n",
    "        \"lr\": 0.000023636840436059507,\n",
    "        \"weight_decay\": 0.01438297700463013,\n",
    "    },\n",
    "    batch_size=8,\n",
    "    max_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainerConfiguration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:biome.text.dataset: Reusing cached instances (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/csv/default-408234f3167ff690/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/3b2270605bf44a9b.instance_list)\n",
      "WARNING:biome.text.dataset: Reusing cached instances (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/csv/default-ae7b327d976b362b/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/4895b9fab1348cce.instance_list)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    pipeline=pl,\n",
    "    train_dataset=training_ds,\n",
    "    valid_dataset=test_ds,\n",
    "    trainer_config=trainer_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mignacioct\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">vague-flower-68</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ignacioct/biome\" target=\"_blank\">https://wandb.ai/ignacioct/biome</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ignacioct/biome/runs/i83iyupb\" target=\"_blank\">https://wandb.ai/ignacioct/biome/runs/i83iyupb</a><br/>\n",
       "                Run data is saved locally in <code>/Users/ignaciotalaveracepeda/Documents/RecognAI/rubrix/docs/guides/training_logs/wandb/run-20210701_110145-i83iyupb</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type               | Params\n",
      "---------------------------------------------\n",
      "0 | _head | TextClassification | 111 M \n",
      "---------------------------------------------\n",
      "111 M     Trainable params\n",
      "0         Non-trainable params\n",
      "111 M     Total params\n",
      "447.825   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layoutâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignaciotalaveracepeda/anaconda3/envs/rubrix/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/Users/ignaciotalaveracepeda/anaconda3/envs/rubrix/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c984b2de38e44ff9f26f95fb16e6796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
