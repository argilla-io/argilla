{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate new data to improve NLP models using Rubrix and biome.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Hey there! In this guide, we will show you how to use Rubrix to annotate new data, and use this new data to improve existing Deep Learning models. Our use case will be Automatic Misogyny Detection (AMI): Deep Learning models able to detect the underlying misogyny on a given text. Ground-breaking work is being made every year on this subject, with shared tasks and new models that push the performance of these models closer and closer to be implemented in apps, social networks and other digital environments. \n",
    "\n",
    "To train these NLP models we are going to use [biome.text](https://github.com/recognai/biome-text), an open-source library to train models with a simple workflow. Rubrix is compatible with almost any library or service, so we will work back and forth with both of them. \n",
    "\n",
    "The data used to feed the models and make the annotations comes from the [IberEval 2018](https://sites.google.com/view/ibereval-2…) shared task. We are also making the specific datasets used in each step of this guide available, so it can be reproduced in the best way possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "If you want to reproduce this code, make sure that all the libraries needed to run this guide are installed and imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U git+https://github.com/recognai/biome-text\n",
    "%pip install rubrix\n",
    "%pip install pandas\n",
    "exit(0)  # Force restart of the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['WANDB_API_KEY'] = '7bd265df21100baa9767bb9f69108bc417db4b4a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import *\n",
    "import pandas as pd\n",
    "import rubrix as rb\n",
    "\n",
    "#TODO: erase\n",
    "from biome.text import *\n",
    "from biome.text.hpo import TuneExperiment\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray import tune\n",
    "import math\n",
    "\n",
    "import wandb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the first model\n",
    "\n",
    "Hey there! In this guide, we will show you how to use Rubrix to annotate new data, and use this new data to improve existing Deep Learning models. Our use case will be Automatic Misogyny Detection (AMI): Deep Learning models able to detect the underlying misogyny on a given text. Ground-breaking work is being made every year on this subject, with shared-tasks and new models that push the performance of this model closer and closer to be implemented in apps, social networks and other digital environments. \n",
    "\n",
    "To train these NLP models we are going to use [biome.text](https://github.com/recognai/biome-text), an open-source library to train models with a simple workflow. Rubrix is compatible with almost any library or service, so we will work back and forth with both of them. \n",
    "\n",
    "The data used to feed the models and make the annotations comes from the [IberEval 2018](https://sites.google.com/view/ibereval-2…) shared-task. We are also making the specific datasets used in each step of this guide available, so it can be reproduced in the best way possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO (wont be included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-dd9545aa755b36d8\n",
      "Reusing dataset csv (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/csv/default-dd9545aa755b36d8/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-22ae5ced3fbbbbb1\n",
      "Reusing dataset csv (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/csv/default-22ae5ced3fbbbbb1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-ae7b327d976b362b\n",
      "Reusing dataset csv (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/csv/default-ae7b327d976b362b/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-408234f3167ff690\n",
      "Reusing dataset csv (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/csv/default-408234f3167ff690/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "training_ds = Dataset.from_csv('annotation_data/training_df.csv')\n",
    "validation_ds = Dataset.from_csv('annotation_data/validation_df.csv')\n",
    "test_ds = Dataset.from_csv('annotation_data/test_df.csv')\n",
    "training_full_ds = Dataset.from_csv('annotation_data/training_full_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'id', 'text', 'label'],\n",
       "    num_rows: 2810\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {\n",
    "    \"name\": \"rubrix_guide\",\n",
    "    \"features\": {\n",
    "        \"transformers\": {\n",
    "            \"model_name\": \"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "            \"trainable\": True,\n",
    "            \"max_length\": 280,  #twitter characters cap\n",
    "        }\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"type\": \"TextClassification\",\n",
    "        \"multilabel\": True,\n",
    "        \"labels\": [\n",
    "            'sexual_harassment',\n",
    "             'dominance',\n",
    "             'discredit',\n",
    "             'stereotype',\n",
    "             'derailing',\n",
    "             'passive',\n",
    "             'active',\n",
    "             '0'\n",
    "        ],\n",
    "        \"pooler\": {\n",
    "            \"type\": tune.choice([\"gru\", \"lstm\"]),\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_size\": tune.choice([32,64,128,256]),\n",
    "            \"bidirectional\": tune.choice([True, False]),\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "trainer_config = TrainerConfiguration(\n",
    "    optimizer={\n",
    "        \"type\": \"adamw\",\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-4),\n",
    "        \"weight_decay\": tune.loguniform(2e-3, 6e-2 )\n",
    "    },\n",
    "    max_epochs=10,\n",
    "    batch_size= batch_size,\n",
    "    monitor=\"validation_macro/fscore\",\n",
    "    monitor_mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_alg = HyperOptSearch(metric=\"validation_macro/fscore\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_exp = TuneExperiment(\n",
    "    pipeline_config=pipeline_dict, \n",
    "    trainer_config=trainer_config,\n",
    "    train_dataset=training_ds,\n",
    "    valid_dataset=validation_ds,\n",
    "    name=\"rubrix_guide\",\n",
    "    # parameters for tune.run\n",
    "    num_samples=100,\n",
    "    local_dir=\"tune_runs\",\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.3/8.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.29 GiB heap, 0.0/1.14 GiB objects<br>Result logdir: /Users/ignaciotalaveracepeda/Documents/RecognAI/rubrix/docs/guides/tune_runs/rubrix_guide<br>Number of trials: 1/100 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th>metrics  </th><th>mlflow  </th><th>mlflow_tracking_uri                                                             </th><th>name        </th><th style=\"text-align: right;\">  pipeline_config/features/transformers/max_length</th><th>pipeline_config/features/transformers/model_name  </th><th>pipeline_config/features/transformers/trainable  </th><th>pipeline_config/head/labels                                                                         </th><th>pipeline_config/head/multilabel  </th><th>pipeline_config/head/pooler/bidirectional  </th><th style=\"text-align: right;\">  pipeline_config/head/pooler/hidden_size</th><th style=\"text-align: right;\">  pipeline_config/head/pooler/num_layers</th><th>pipeline_config/head/pooler/type  </th><th>pipeline_config/head/type  </th><th>pipeline_config/name  </th><th>train_dataset_path                                          </th><th style=\"text-align: right;\">  trainer_config/accumulate_grad_batches</th><th>trainer_config/add_csv_logger  </th><th>trainer_config/add_early_stopping  </th><th>trainer_config/add_lr_monitor  </th><th>trainer_config/add_tensorboard_logger  </th><th>trainer_config/add_wandb_logger  </th><th>trainer_config/auto_lr_find  </th><th>trainer_config/auto_scale_batch_size  </th><th style=\"text-align: right;\">  trainer_config/batch_size</th><th>trainer_config/callbacks  </th><th style=\"text-align: right;\">  trainer_config/check_val_every_n_epoch</th><th>trainer_config/checkpoint_callback  </th><th>trainer_config/default_root_dir                                                 </th><th>trainer_config/fast_dev_run  </th><th style=\"text-align: right;\">  trainer_config/flush_logs_every_n_steps</th><th>trainer_config/gpus  </th><th style=\"text-align: right;\">  trainer_config/gradient_clip_val</th><th style=\"text-align: right;\">  trainer_config/limit_test_batches</th><th style=\"text-align: right;\">  trainer_config/limit_train_batches</th><th style=\"text-align: right;\">  trainer_config/limit_val_batches</th><th style=\"text-align: right;\">  trainer_config/log_every_n_steps</th><th>trainer_config/logger  </th><th>trainer_config/lr_decay  </th><th style=\"text-align: right;\">  trainer_config/max_epochs</th><th>trainer_config/max_steps  </th><th>trainer_config/min_epochs  </th><th>trainer_config/min_steps  </th><th>trainer_config/monitor  </th><th>trainer_config/monitor_mode  </th><th style=\"text-align: right;\">  trainer_config/num_sanity_val_steps</th><th style=\"text-align: right;\">  trainer_config/num_workers_for_dataloader</th><th style=\"text-align: right;\">  trainer_config/optimizer/lr</th><th>trainer_config/optimizer/type  </th><th style=\"text-align: right;\">  trainer_config/optimizer/weight_decay</th><th style=\"text-align: right;\">  trainer_config/overfit_batches</th><th style=\"text-align: right;\">  trainer_config/patience</th><th style=\"text-align: right;\">  trainer_config/precision</th><th>trainer_config/progress_bar_refresh_rate  </th><th>trainer_config/resume_from_checkpoint  </th><th style=\"text-align: right;\">  trainer_config/save_top_k_checkpoints</th><th>trainer_config/stochastic_weight_avg  </th><th>trainer_config/terminate_on_nan  </th><th style=\"text-align: right;\">  trainer_config/val_check_interval</th><th style=\"text-align: right;\">  trainer_config/warmup_steps</th><th>trainer_config/weights_save_path  </th><th>valid_dataset_path                                          </th><th>wandb  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>_default_trainable_15bf5a4a</td><td>PENDING </td><td>     </td><td>         </td><td>True    </td><td>file:///Users/ignaciotalaveracepeda/Documents/RecognAI/rubrix/docs/guides/mlruns</td><td>rubrix_guide</td><td style=\"text-align: right;\">                                               280</td><td>dccuchile/bert-base-spanish-wwm-cased             </td><td>True                                             </td><td>(&#x27;sexual_harassment&#x27;, &#x27;dominance&#x27;, &#x27;discredit&#x27;, &#x27;stereotype&#x27;, &#x27;derailing&#x27;, &#x27;passive&#x27;, &#x27;active&#x27;, &#x27;0&#x27;)</td><td>True                             </td><td>False                                      </td><td style=\"text-align: right;\">                                      256</td><td style=\"text-align: right;\">                                       1</td><td>gru                               </td><td>TextClassification         </td><td>rubrix_guide          </td><td>/var/folders/mb/lvj4fyds5757cy_7swmlpt_00000gn/T/tmpz2x55gq7</td><td style=\"text-align: right;\">                                       1</td><td>True                           </td><td>True                               </td><td>                               </td><td>True                                   </td><td>True                             </td><td>False                        </td><td>False                                 </td><td style=\"text-align: right;\">                         16</td><td>                          </td><td style=\"text-align: right;\">                                       1</td><td>True                                </td><td>/Users/ignaciotalaveracepeda/Documents/RecognAI/rubrix/docs/guides/training_logs</td><td>False                        </td><td style=\"text-align: right;\">                                      100</td><td>                     </td><td style=\"text-align: right;\">                                 0</td><td style=\"text-align: right;\">                                  1</td><td style=\"text-align: right;\">                                   1</td><td style=\"text-align: right;\">                                 1</td><td style=\"text-align: right;\">                                50</td><td>True                   </td><td>                         </td><td style=\"text-align: right;\">                         10</td><td>                          </td><td>                           </td><td>                          </td><td>validation_macro/fscore </td><td>max                          </td><td style=\"text-align: right;\">                                    2</td><td style=\"text-align: right;\">                                          0</td><td style=\"text-align: right;\">                  5.49823e-05</td><td>adamw                          </td><td style=\"text-align: right;\">                             0.00597915</td><td style=\"text-align: right;\">                               0</td><td style=\"text-align: right;\">                        3</td><td style=\"text-align: right;\">                        32</td><td>                                          </td><td>                                       </td><td style=\"text-align: right;\">                                      1</td><td>False                                 </td><td>False                            </td><td style=\"text-align: right;\">                                  1</td><td style=\"text-align: right;\">                            0</td><td>                                  </td><td>/var/folders/mb/lvj4fyds5757cy_7swmlpt_00000gn/T/tmpp148g_y0</td><td>True   </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-30 13:40:24,044\tERROR tune.py:545 -- Trials did not complete: [_default_trainable_15bf5a4a]\n",
      "2021-06-30 13:40:24,047\tINFO tune.py:549 -- Total run time: 374.91 seconds (374.46 seconds for the tuning loop).\n",
      "2021-06-30 13:40:24,050\tWARNING tune.py:553 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    }
   ],
   "source": [
    "analysis_frozen = tune.run(\n",
    "    tune_exp,\n",
    "    scheduler=tune.schedulers.ASHAScheduler(), \n",
    "    config=tune_exp.config,\n",
    "    metric=\"validation_macro/fscore\",\n",
    "    search_alg=search_alg,\n",
    "    mode=\"max\",\n",
    "    progress_reporter=tune.JupyterNotebookReporter(overwrite=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
