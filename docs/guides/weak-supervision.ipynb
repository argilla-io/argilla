{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020957ce-46fa-4799-8321-716e62900269",
   "metadata": {},
   "source": [
    "# Weak supervision\n",
    "\n",
    "\n",
    "This guide gives you a brief introduction to weak supervision with Rubrix.\n",
    "\n",
    "Rubrix currently supports weak supervision for multi-class text classification use cases, but we'll be adding support for multilabel text classification and token classification (e.g., Named Entity Recognition) soon.\n",
    "\n",
    "![Labeling workflow](https://raw.githubusercontent.com/recognai/rubrix-materials/main/tutorials/weak_supervision/weak_supervision.svg \"Labeling workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb86995-f158-4392-86b7-eb5305a9ec3b",
   "metadata": {},
   "source": [
    "## Rubrix weak supervision in a nutshell\n",
    "\n",
    "The recommended workflow for weak supervision is:\n",
    "\n",
    "- Log an unlabelled dataset into Rubrix\n",
    "- Use the `Annotate` mode for hand- and/or bulk-labelling a test set. This test is key to measure the quality and performance of your rules.\n",
    "- Use the `Define rules` mode for testing and defining rules. Rules are defined with search queries (using ES query string DSL).\n",
    "- Use the Python client for reading rules, defining additional rules if needed, and train a label (for building a training set) or a downstream model (for building an end classifier).\n",
    "\n",
    "The next sections cover the main components of this workflow. If you want to jump into a practical tutorial, check the [news classification tutorial](../tutorials/weak-supervision-with-rubrix.ipynb).\n",
    "\n",
    "### Weak labeling using the UI\n",
    "\n",
    "Since version 0.8.0 you can find and define rules directly in the UI. \n",
    "The [Define rules mode](../reference/webapp/define_rules.md) is found in the right side bar of the [Dataset page](../reference/webapp/dataset.md).\n",
    "The video below shows how you can interactively find and save rules with the UI. \n",
    "For a full example check the [Weak supervision tutorial](../tutorials/weak-supervision-with-rubrix.ipynb).\n",
    "\n",
    "<video width=\"100%\" controls><source src=\"../_static/tutorials/weak-supervision-with-rubrix/ws_news.mp4\" type=\"video/mp4\"></video>\n",
    "\n",
    "\n",
    "### Weak supervision from Python\n",
    "\n",
    "Doing weak supervision with Rubrix should be straightforward. Keeping the same spirit as other parts of the library, you can virtually use any weak supervision library or method, such as Snorkel or Flyingsquid. \n",
    "\n",
    "Rubrix weak supervision support is built around two basic abstractions:\n",
    "\n",
    "\n",
    "### `Rule`\n",
    "A rule encodes an heuristic for labeling a record.\n",
    "\n",
    "Heuristics can be defined using [Elasticsearch's queries](../reference/webapp/search_records.md):\n",
    "\n",
    "```python\n",
    "plz = Rule(query=\"plz OR please\", label=\"SPAM\")\n",
    "```\n",
    "\n",
    "or with Python functions (similar to Snorkel's labeling functions, which you can use as well):\n",
    "\n",
    "```python\n",
    "def contains_http(record: rb.TextClassificationRecord) -> Optional[str]:\n",
    "    if \"http\" in record.inputs[\"text\"]:\n",
    "        return \"SPAM\"\n",
    "```\n",
    "\n",
    "Besides textual features, Python labeling functions can exploit metadata features:\n",
    "\n",
    "```python\n",
    "def author_channel(record: rb.TextClassificationRecord) -> Optional[str]:\n",
    "    # the word channel appears in the comment author name\n",
    "    if \"channel\" in record.metadata[\"author\"]:\n",
    "        return \"SPAM\"\n",
    "```\n",
    "\n",
    "A rule should either return a string value, that is a weak label, or a `None` type in case of abstention.\n",
    "\n",
    "\n",
    "### `Weak Labels`\n",
    "\n",
    "Weak Labels objects bundle and apply a set of rules to the records of a Rubrix dataset. Applying a rule to a record means assigning a weak label or abstaining.\n",
    "\n",
    "This abstraction provides you with the building blocks for training and testing weak supervision \"denoising\", \"label\" or even \"end\" models:\n",
    "\n",
    "```python\n",
    "rules = [contains_http, author_channel]\n",
    "weak_labels = WeakLabels(\n",
    "    rules=rules, \n",
    "    dataset=\"weak_supervision_yt\"\n",
    ")\n",
    "\n",
    "# returns a summary of the applied rules\n",
    "weak_labels.summary()\n",
    "```\n",
    "\n",
    "More information about these abstractions can be found in [the Python Labeling module docs](../reference/python/python_labeling.rst).\n",
    "\n",
    "## Built-in label models\n",
    "\n",
    "To make things even easier for you, we provide wrapper classes around the most common label models, that directly consume a `WeakLabels` object.\n",
    "This makes working with those models a breeze.\n",
    "Take a look at the list of built-in models in the [labeling module docs](../reference/python/python_labeling.rst#rubrix.labeling.text_classification.label_models.LabelModel).\n",
    "\n",
    "\n",
    "## Detailed Workflow\n",
    "\n",
    "A typical workflow to use weak supervision is:\n",
    "\n",
    "1. Create a Rubrix dataset with your raw dataset. If you actually have some labelled data you can log it into the the same dataset.\n",
    "2. Define a set of weak labeling rules with the Rules definition mode in the UI.\n",
    "3. Create a `WeakLabels` object and apply the rules. You can load the rules from your dataset and add additional rules and labeling functions using Python. Typically, you'll iterate between this step and step 2.\n",
    "4. Once you are satisfied with your weak labels, use the matrix of the `WeakLabels` instance with your library/method of choice to build a training set or even train a downstream text classification model.\n",
    "\n",
    "\n",
    "This guide shows you an end-to-end example using Snorkel, Flyingsquid and Weasel. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52dd8f-fb8e-4909-9f6d-0a8856afa999",
   "metadata": {},
   "source": [
    "## Example dataset\n",
    "\n",
    "We'll be using a well-known dataset for weak supervision examples, the [YouTube Spam Collection](http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/) dataset, which is a binary classification task for detecting spam comments in Youtube videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1e00af-c6f9-42aa-81aa-2976c9591b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Alessandro leite</td>\n",
       "      <td>2014-11-05T22:21:36</td>\n",
       "      <td>pls http://www10.vakinha.com.br/VaquinhaE.aspx...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Salim Tayara</td>\n",
       "      <td>2014-11-02T14:33:30</td>\n",
       "      <td>if your like drones, plz subscribe to Kamal Ta...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Phuc Ly</td>\n",
       "      <td>2014-01-20T15:27:47</td>\n",
       "      <td>go here to check the views :3﻿</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DropShotSk8r</td>\n",
       "      <td>2014-01-19T04:27:18</td>\n",
       "      <td>Came here to check the views, goodbye.﻿</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>css403</td>\n",
       "      <td>2014-11-07T14:25:48</td>\n",
       "      <td>i am 2,126,492,636 viewer :D﻿</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            author                 date  \\\n",
       "0           0  Alessandro leite  2014-11-05T22:21:36   \n",
       "1           1      Salim Tayara  2014-11-02T14:33:30   \n",
       "2           2           Phuc Ly  2014-01-20T15:27:47   \n",
       "3           3      DropShotSk8r  2014-01-19T04:27:18   \n",
       "4           4            css403  2014-11-07T14:25:48   \n",
       "\n",
       "                                                text  label  video  \n",
       "0  pls http://www10.vakinha.com.br/VaquinhaE.aspx...   -1.0      1  \n",
       "1  if your like drones, plz subscribe to Kamal Ta...   -1.0      1  \n",
       "2                     go here to check the views :3﻿   -1.0      1  \n",
       "3            Came here to check the views, goodbye.﻿   -1.0      1  \n",
       "4                      i am 2,126,492,636 viewer :D﻿   -1.0      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "train_df = pd.read_csv('../tutorials/data/yt_comments_train.csv')\n",
    "test_df = pd.read_csv('../tutorials/data/yt_comments_test.csv')\n",
    "\n",
    "# preview data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fcd253-f8b5-40ba-a9a2-9d068e19c1cc",
   "metadata": {},
   "source": [
    "## 1. Create a Rubrix dataset with unlabelled data and test data\n",
    "\n",
    "Let's load the train (non-labelled) and the test (containing labels) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78571aa4-314d-4b4e-b933-c3292213b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "\n",
    "# build records from the train dataset\n",
    "records = [\n",
    "    rb.TextClassificationRecord(\n",
    "        inputs=row.text,\n",
    "        metadata={\"video\":row.video, \"author\": row.author}\n",
    "    )\n",
    "    for i,row in train_df.iterrows()\n",
    "]\n",
    "\n",
    "# build records from the test dataset with annotation\n",
    "labels = [\"HAM\", \"SPAM\"]\n",
    "records += [\n",
    "    rb.TextClassificationRecord(\n",
    "        inputs=row.text,\n",
    "        annotation=labels[row.label],\n",
    "        metadata={\"video\":row.video, \"author\": row.author}\n",
    "    )\n",
    "    for i,row in test_df.iterrows()\n",
    "]\n",
    "\n",
    "# log records to Rubrix\n",
    "rb.log(records, name=\"weak_supervision_yt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafda4f-45c0-49d6-9c37-7473c6888ebe",
   "metadata": {},
   "source": [
    "After this step, you have a fully browsable dataset available that you can access via the [Rubrix web app](../reference/webapp/index.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde95ce0-6e1e-4c9e-aff2-ac12643b9a48",
   "metadata": {},
   "source": [
    "## 2. Defining rules\n",
    "\n",
    "Let's now define some of the rules proposed in the tutorial [Snorkel Intro Tutorial: Data Labeling](https://www.snorkel.org/use-cases/01-spam-tutorial). \n",
    "Most of these rules can be defined directly with our web app in the [Define rules mode](../reference/webapp/define_rules.md) and [Elasticsearch's query strings](../reference/webapp/search_records.md). \n",
    "Afterward, you can conveniently load them into your notebook with the [load_rules function](../reference/python/python_labeling.rst#rubrix.labeling.text_classification.rule.load_rules).\n",
    "\n",
    "Rules can also be defined programmatically as shown below. Depending on your use case and team structure you can mix and match both interfaces (UI or Python).\n",
    "\n",
    "Let's see here some programmatic rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045700ef-62b7-44e2-95f9-0f966236303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubrix.labeling.text_classification import Rule, WeakLabels\n",
    "\n",
    "#  rules defined as Elasticsearch queries\n",
    "check_out = Rule(query=\"check out\", label=\"SPAM\")\n",
    "plz = Rule(query=\"plz OR please\", label=\"SPAM\")\n",
    "subscribe = Rule(query=\"subscribe\", label=\"SPAM\")\n",
    "my = Rule(query=\"my\", label=\"SPAM\")\n",
    "song = Rule(query=\"song\", label=\"HAM\")\n",
    "love = Rule(query=\"love\", label=\"HAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d4d7f-9a95-4905-a38b-3907d5293c4f",
   "metadata": {},
   "source": [
    "You can also define plain Python labeling functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b3554c-9646-4f2b-ab33-13518566bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# rules defined as Python labeling functions\n",
    "def contains_http(record: rb.TextClassificationRecord):\n",
    "    if \"http\" in record.inputs[\"text\"]:\n",
    "        return \"SPAM\"\n",
    "\n",
    "def short_comment(record: rb.TextClassificationRecord):\n",
    "    return \"HAM\" if len(record.inputs[\"text\"].split()) < 5 else None\n",
    "\n",
    "def regex_check_out(record: rb.TextClassificationRecord):\n",
    "    return \"SPAM\" if re.search(r\"check.*out\", record.inputs[\"text\"], flags=re.I) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c93ec-b136-43cf-af50-96c956b65f12",
   "metadata": {},
   "source": [
    "## 3. Building and analizing weak labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf60f4-c5f5-492d-ac0d-52a25cfb5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubrix.labeling.text_classification import load_rules\n",
    "\n",
    "# bundle our rules in a list\n",
    "rules = [check_out, plz, subscribe, my, song, love, contains_http, short_comment, regex_check_out]\n",
    "\n",
    "# optionally add the rules defined in the web app UI\n",
    "rules += load_rules(dataset=\"weak_supervision_yt\")\n",
    "\n",
    "# apply the rules to a dataset to obtain the weak labels\n",
    "weak_labels = WeakLabels(\n",
    "    rules=rules, \n",
    "    dataset=\"weak_supervision_yt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0584b8b-4b0d-4857-9e8b-9823d9172634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>coverage</th>\n",
       "      <th>overlaps</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check out</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.242919</td>\n",
       "      <td>0.235839</td>\n",
       "      <td>0.029956</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plz OR please</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.090414</td>\n",
       "      <td>0.081155</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subscribe</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.106754</td>\n",
       "      <td>0.083878</td>\n",
       "      <td>0.028867</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.190632</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.049564</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>0.872340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>song</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.132898</td>\n",
       "      <td>0.079521</td>\n",
       "      <td>0.033769</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.092048</td>\n",
       "      <td>0.070261</td>\n",
       "      <td>0.031590</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_http</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.106209</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.049564</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.245098</td>\n",
       "      <td>0.110566</td>\n",
       "      <td>0.064270</td>\n",
       "      <td>84</td>\n",
       "      <td>8</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.226580</td>\n",
       "      <td>0.226035</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>{SPAM, HAM}</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.448802</td>\n",
       "      <td>0.120915</td>\n",
       "      <td>338</td>\n",
       "      <td>30</td>\n",
       "      <td>0.918478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    polarity  coverage  overlaps  conflicts  correct  \\\n",
       "check out             {SPAM}  0.242919  0.235839   0.029956       45   \n",
       "plz OR please         {SPAM}  0.090414  0.081155   0.019608       20   \n",
       "subscribe             {SPAM}  0.106754  0.083878   0.028867       30   \n",
       "my                    {SPAM}  0.190632  0.166667   0.049564       41   \n",
       "song                   {HAM}  0.132898  0.079521   0.033769       39   \n",
       "love                   {HAM}  0.092048  0.070261   0.031590       28   \n",
       "contains_http         {SPAM}  0.106209  0.073529   0.049564        6   \n",
       "short_comment          {HAM}  0.245098  0.110566   0.064270       84   \n",
       "regex_check_out       {SPAM}  0.226580  0.226035   0.027778       45   \n",
       "total            {SPAM, HAM}  0.754902  0.448802   0.120915      338   \n",
       "\n",
       "                 incorrect  precision  \n",
       "check out                0   1.000000  \n",
       "plz OR please            0   1.000000  \n",
       "subscribe                0   1.000000  \n",
       "my                       6   0.872340  \n",
       "song                     9   0.812500  \n",
       "love                     7   0.800000  \n",
       "contains_http            0   1.000000  \n",
       "short_comment            8   0.913043  \n",
       "regex_check_out          0   1.000000  \n",
       "total                   30   0.918478  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show some stats about the rules, see the `summary()` docstring for details\n",
    "weak_labels.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4364c17-4a5d-464b-8f2b-36f50c81b174",
   "metadata": {},
   "source": [
    "## 4. Using the weak labels\n",
    "\n",
    "At this step you have at least two options:\n",
    "\n",
    "1. Use the weak labels for training a \"denoising\" or label model to build a less noisy training set. Highly popular options for this are [Snorkel](https://snorkel.org/) or [Flyingsquid](https://github.com/HazyResearch/flyingsquid). After this step, you can train a downstream model with the \"clean\" labels.\n",
    "\n",
    "2. Use the weak labels directly with recent \"end-to-end\" (e.g., [Weasel](https://github.com/autonlab/weasel)) or joint models (e.g., [COSINE](https://github.com/yueyu1030/COSINE)).\n",
    "\n",
    "\n",
    "Let's see some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb78718",
   "metadata": {},
   "source": [
    "### A simple majority vote\n",
    "\n",
    "As a first example we will show you, how to use the `WeakLabels` object together with a simple majority vote model.\n",
    "For this we will take the implementation by Snorkel, and evaluate it with the help of sklearn's metrics module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install snorkel scikit-learn -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb2b66",
   "metadata": {},
   "source": [
    "The majority vote model is arguably the most straightforward label model.\n",
    "On a per-record basis, it simply counts the votes for each label returned by the rules, and takes the majority vote.\n",
    "Snorkel provides a neat implementation of this logic in its `MajorityLabelVoter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15888261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "# instantiate the majority vote label model\n",
    "majority_model = MajorityLabelVoter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d163f-ba08-424c-93de-ef0420531ca9",
   "metadata": {},
   "source": [
    "Let's go on and evaluate this baseline.\n",
    "To break ties when there is no majority vote, we choose the _\"random\"_ policy that randomly selects one of the tied labels. \n",
    "In this way we avoid a bias towards label models that produce fewer but more certain weak labels, and makes the comparison between the different label models fairer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d588f00-c8e1-4bec-8e99-bd78639257b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "majority_model.score(\n",
    "    L=weak_labels.matrix(has_annotation=True),\n",
    "    Y=weak_labels.annotation(),\n",
    "    tie_break_policy=\"random\",\n",
    ")\n",
    "# {'accuracy': 0.844}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619247f-3d9d-44ae-bb9f-d07c9120c1dc",
   "metadata": {},
   "source": [
    "As we will see further down, an accuracy of 0.844 is a very decent baseline.\n",
    "Choosing to simply ignore tiebreaks and abstentions (by setting the tiebreak policy to _\"abstain\"_), we would obtain an accuracy of nearly 0.96.\n",
    "\n",
    "When predicting weak labels to train a down-stream model, you probably want to discard the abstentions and tiebreaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de05c3-84ef-40de-8f73-0157a4aa1074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for training a down-stream model\n",
    "predictions = majority_model.predict(L=weak_labels.matrix(has_annotation=False))\n",
    "\n",
    "# records for training\n",
    "training_records = weak_labels.records(has_annotation=False)\n",
    "\n",
    "# mask to ignore abstentions/tiebreaks\n",
    "idx = predictions != -1\n",
    "\n",
    "# combine records and predictions\n",
    "training_data = pd.DataFrame(\n",
    "    [\n",
    "        {\"text\": rec.inputs[\"text\"], \"label\": weak_labels.int2label[label]} \n",
    "        for rec, label in zip(training_records, predictions)\n",
    "    ]\n",
    ")[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "39e06fd0-caa6-4707-a667-030b52ad4be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey I&amp;#39;m a British youtuber!!&lt;br /&gt;I upload...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOKIA spotted﻿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dance :)﻿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You guys should check out this EXTRAORDINARY w...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Need money ? check my channel and subscribe,so...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>Please check out my acoustic cover channel :) ...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>PLEASE SUBSCRIBE ME!!!!!!!!!!!!!!!!!!!!!!!!!!!...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>&lt;a href=\"http://www.gofundme.com/Helpmypitbull...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>I love this song so much!:-D I've heard it so ...</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>Check out this video on YouTube:﻿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1055 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label\n",
       "0     Hey I&#39;m a British youtuber!!<br />I upload...  SPAM\n",
       "1                                        NOKIA spotted﻿   HAM\n",
       "2                                             Dance :)﻿   HAM\n",
       "3     You guys should check out this EXTRAORDINARY w...  SPAM\n",
       "4     Need money ? check my channel and subscribe,so...  SPAM\n",
       "...                                                 ...   ...\n",
       "1579  Please check out my acoustic cover channel :) ...  SPAM\n",
       "1580  PLEASE SUBSCRIBE ME!!!!!!!!!!!!!!!!!!!!!!!!!!!...  SPAM\n",
       "1581  <a href=\"http://www.gofundme.com/Helpmypitbull...  SPAM\n",
       "1582  I love this song so much!:-D I've heard it so ...   HAM\n",
       "1585                  Check out this video on YouTube:﻿  SPAM\n",
       "\n",
       "[1055 rows x 2 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview training data\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186a612-57a4-4118-97fb-927ed87df96d",
   "metadata": {},
   "source": [
    "### Label model with Snorkel\n",
    "\n",
    "Snorkel's label model is by far the most popular option for using weak supervision, and Rubrix provides built-in support for it. \n",
    "Using Snorkel with Rubrix's `WeakLabels` is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af30dfa-b401-4e96-984d-c59753411557",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install snorkel -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb50bb-edfb-40a2-9e4c-094e22284df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubrix.labeling.text_classification import Snorkel\n",
    "\n",
    "# we pass our WeakLabels instance to our Snorkel label model\n",
    "snorkel_model = Snorkel(weak_labels)\n",
    "\n",
    "# we fit the model\n",
    "snorkel_model.fit(lr=0.001, n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6346d-1a34-4efa-87de-f3e69b7550ca",
   "metadata": {},
   "source": [
    "When fitting the snorkel model, we recommend performing a quick grid search for the learning rate `lr` and the number of epochs `n_epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82339f-561d-4c59-89ee-c49ddbe8da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check its performance\n",
    "snorkel_model.score(tie_break_policy=\"abstain\")\n",
    "# {'accuracy': 0.848, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075463f3-8695-4d28-af24-8860b2efd691",
   "metadata": {},
   "source": [
    "When choosing to simply ignore tiebreaks and abstentions in the score (by setting the tiebreak policy to _\"abstain\"_), we would obtain an accuracy of about 0.95.\n",
    "\n",
    "After fitting your label model, you can quickly explore its predictions, before building a training set for training a downstream text classifier. \n",
    "This step is useful for validation, manual revision, or defining score thresholds for accepting labels from your label model (for example, only considering labels with a score greater then 0.8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3fd76-8b58-4ae3-bc58-9d82c039bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your training records with the predictions of the label model\n",
    "records_for_training = snorkel_model.predict()\n",
    "\n",
    "# optional: log the records to a new dataset in Rubrix\n",
    "rb.log(records_for_training, name=\"snorkel_results\")\n",
    "\n",
    "# extract training data\n",
    "training_data = pd.DataFrame(\n",
    "    [\n",
    "        {\"text\": rec.inputs[\"text\"], \"label\": rec.prediction[0][0]} \n",
    "        for rec in records_for_training\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4e18b365-5412-47c7-b0e6-e140bd0c2dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey I&amp;#39;m a British youtuber!!&lt;br /&gt;I upload...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOKIA spotted﻿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dance :)﻿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You guys should check out this EXTRAORDINARY w...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Need money ? check my channel and subscribe,so...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>Please check out my acoustic cover channel :) ...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>PLEASE SUBSCRIBE ME!!!!!!!!!!!!!!!!!!!!!!!!!!!...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>&lt;a href=\"http://www.gofundme.com/Helpmypitbull...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>I love this song so much!:-D I've heard it so ...</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>Check out this video on YouTube:﻿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1177 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label\n",
       "0     Hey I&#39;m a British youtuber!!<br />I upload...  SPAM\n",
       "1                                        NOKIA spotted﻿   HAM\n",
       "2                                             Dance :)﻿   HAM\n",
       "3     You guys should check out this EXTRAORDINARY w...  SPAM\n",
       "4     Need money ? check my channel and subscribe,so...  SPAM\n",
       "...                                                 ...   ...\n",
       "1172  Please check out my acoustic cover channel :) ...  SPAM\n",
       "1173  PLEASE SUBSCRIBE ME!!!!!!!!!!!!!!!!!!!!!!!!!!!...  SPAM\n",
       "1174  <a href=\"http://www.gofundme.com/Helpmypitbull...  SPAM\n",
       "1175  I love this song so much!:-D I've heard it so ...   HAM\n",
       "1176                  Check out this video on YouTube:﻿  SPAM\n",
       "\n",
       "[1177 rows x 2 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview training data\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0123c-a7a5-4a35-82fc-a5a616cd5000",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "For an example of how to use the `WeakLabels` object with Snorkel's raw `LabelModel` class, you can check out the [WeakLabels reference](../reference/python/python_labeling.rst#rubrix.labeling.text_classification.weak_labels.WeakLabels).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f69fc-7f2f-4b57-8262-a0a6c2f92329",
   "metadata": {},
   "source": [
    "### Label model with FlyingSquid\n",
    "\n",
    "FlyingSquid is a powerful method developed by [Hazy Research](https://hazyresearch.stanford.edu/), a research group from Stanford behind ground-breaking work on programmatic data labeling, including Snorkel.\n",
    "FlyingSquid uses a closed-form solution for fitting the label model with great speed gains and similar performance.\n",
    "Just like for Snorkel, Rubrix provides built-in support for FlyingSquid, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55482c-02f6-4e7a-8637-0aa50e4e9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flyingsquid pgmpy -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ede78a-0fe6-4f2b-87b1-3d0f06f0c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubrix.labeling.text_classification import FlyingSquid\n",
    "\n",
    "# we pass our WeakLabels instance to our FlyingSquid label model\n",
    "flyingsquid_model = FlyingSquid(weak_labels)\n",
    "\n",
    "# we fit the model\n",
    "flyingsquid_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d81dbf5-edf9-4c8c-a876-5fcb16d73090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check its performance\n",
    "flyingsquid_model.score(tie_break_policy=\"random\")\n",
    "# {'accuracy': 0.832, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282da18-cc57-437a-bfd1-c13a9ac1aec4",
   "metadata": {},
   "source": [
    "When choosing to simply ignore tiebreaks and abstentions in the score (by setting the tiebreak policy to _\"abstain\"_), we would obtain an accuracy of about 0.93.\n",
    "\n",
    "After fitting your label model, you can quickly explore its predictions, before building a training set for training a downstream text classifier. \n",
    "This step is useful for validation, manual revision, or defining score thresholds for accepting labels from your label model (for example, only considering labels with a score greater then 0.8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff30478c-fdda-445f-ad49-cac01b8bde71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your training records with the predictions of the label model\n",
    "records_for_training = flyingsquid_model.predict()\n",
    "\n",
    "# log the records to a new dataset in Rubrix\n",
    "rb.log(records_for_training, name=\"flyingsquid_results\")\n",
    "\n",
    "# extract training data\n",
    "training_data = pd.DataFrame(\n",
    "    [\n",
    "        {\"text\": rec.inputs[\"text\"], \"label\": rec.prediction[0][0]} \n",
    "        for rec in records_for_training\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0d641340-f82b-4af8-b86b-7c06eaf59f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey I&amp;#39;m a British youtuber!!&lt;br /&gt;I upload...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOKIA spotted﻿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dance :)﻿</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You guys should check out this EXTRAORDINARY w...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Need money ? check my channel and subscribe,so...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>Please check out my acoustic cover channel :) ...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>PLEASE SUBSCRIBE ME!!!!!!!!!!!!!!!!!!!!!!!!!!!...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>&lt;a href=\"http://www.gofundme.com/Helpmypitbull...</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>I love this song so much!:-D I've heard it so ...</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>Check out this video on YouTube:﻿</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1177 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label\n",
       "0     Hey I&#39;m a British youtuber!!<br />I upload...  SPAM\n",
       "1                                        NOKIA spotted﻿   HAM\n",
       "2                                             Dance :)﻿   HAM\n",
       "3     You guys should check out this EXTRAORDINARY w...  SPAM\n",
       "4     Need money ? check my channel and subscribe,so...  SPAM\n",
       "...                                                 ...   ...\n",
       "1172  Please check out my acoustic cover channel :) ...  SPAM\n",
       "1173  PLEASE SUBSCRIBE ME!!!!!!!!!!!!!!!!!!!!!!!!!!!...  SPAM\n",
       "1174  <a href=\"http://www.gofundme.com/Helpmypitbull...  SPAM\n",
       "1175  I love this song so much!:-D I've heard it so ...   HAM\n",
       "1176                  Check out this video on YouTube:﻿  SPAM\n",
       "\n",
       "[1177 rows x 2 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview training data\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cec1b4-97c8-45a0-9fc4-06e663b9ee4d",
   "metadata": {},
   "source": [
    "### Joint Model with Weasel\n",
    "\n",
    "[Weasel](https://github.com/autonlab/weasel) lets you train downstream models end-to-end using directly weak labels.\n",
    "In contrast to Snorkel or FlyingSquid, which are two-stage approaches, Weasel is a one-stage method that jointly trains the label and the end model at the same time.\n",
    "For more details check out the [End-to-End Weak Supervision paper](https://arxiv.org/abs/2107.02233) presented at NeurIPS 2021.\n",
    "\n",
    "In this guide we will show you, how you can **train a Hugging Face transformers** model directly **with weak labels using Weasel**.\n",
    "Since Weasel uses [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) for the training, some basic knowledge of PyTorch is helpful, but not strictly necessary.\n",
    "\n",
    "Let's start with installing the Weasel python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac1d61-5a40-44dc-a340-46bbdb8852eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install git+https://github.com/autonlab/weasel#egg=weasel[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffab015-2204-4986-b415-98ba95228736",
   "metadata": {},
   "source": [
    "The first step is to obtain our weak labels.\n",
    "For this we use the same rules and data set as in the examples above (Snorkel and FlyingSquid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17f25f-a4c2-47a3-b8b2-ba9bf1966b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain our weak labels\n",
    "weak_labels = WeakLabels(\n",
    "    rules=rules, \n",
    "    dataset=\"weak_supervision_yt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6273d8-5765-4979-b3eb-3b648b789796",
   "metadata": {},
   "source": [
    "In a second step we instantiate our end model, which in our case will be a pre-trained transformer from the Hugging Face Hub.\n",
    "Here we choose the small ELECTRA model by Google that shows excellent performance given its moderate number of parameters.\n",
    "Due to its size, you can fine-tune it on your CPU within a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598eac51-5ae9-4d16-aeca-df8c10a00f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weasel.models.downstream_models.transformers import Transformers\n",
    "\n",
    "# instantiate our transformers end model\n",
    "end_model = Transformers(\"google/electra-small-discriminator\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcec749-929f-439b-969a-620380319363",
   "metadata": {},
   "source": [
    "With our end-model at hand, we can now instantiate the Weasel model.\n",
    "Apart from the end-model, it also includes a neural encoder that tries to estimate latent labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8cc63-aed4-490d-9930-72e474c8836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weasel.models import Weasel\n",
    "\n",
    "# instantiate our weasel end-to-end model\n",
    "weasel = Weasel(\n",
    "    end_model=end_model,\n",
    "    num_LFs=len(weak_labels.rules),\n",
    "    n_classes=2,\n",
    "    encoder={'hidden_dims': [32, 10]},\n",
    "    optim_encoder={'name': 'adam', 'lr': 1e-4},\n",
    "    optim_end_model={'name': 'adam', 'lr': 5e-5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d48a81-2a0c-4958-9b49-093c48beb0d5",
   "metadata": {},
   "source": [
    "Afterwards, we wrap our data in the `TransformersDataModule`, so that Weasel and PyTorch Lightning can work with it.\n",
    "In this step we also tokenize the data. \n",
    "Here we need to be careful to use the corresponding tokenizer to our end model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa1b28-cf3a-4c4e-9ccf-1ac67777ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from weasel.datamodules.transformers_datamodule import TransformersDataModule, TransformersCollator\n",
    "\n",
    "# tokenizer for our transformers end model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "# tokenize train and test data\n",
    "X_train = [\n",
    "    tokenizer(rec.inputs[\"text\"], truncation=True) \n",
    "    for rec in weak_labels.records(has_annotation=False)\n",
    "]\n",
    "X_test = [\n",
    "    tokenizer(rec.inputs[\"text\"], truncation=True)\n",
    "    for rec in weak_labels.records(has_annotation=True)\n",
    "]\n",
    "\n",
    "# instantiate data module\n",
    "datamodule = TransformersDataModule(\n",
    "    label_matrix=weak_labels.matrix(has_annotation=False),\n",
    "    X_train=X_train,\n",
    "    collator=TransformersCollator(tokenizer),\n",
    "    X_test=X_test,\n",
    "    Y_test=weak_labels.annotation(),\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b35f33-a9e1-4289-84d5-bcc1d4c1c200",
   "metadata": {},
   "source": [
    "Now we have everything ready to start the training of our Weasel model.\n",
    "For the training process, Weasel relies on the excellent [PyTorch Lightning Trainer](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html).\n",
    "It provides tons of options and features to optimize the training process, but the defaults below should give you reasonable results.\n",
    "Keep in mind that you are fine-tuning a full-blown transformer model, albeit a small one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90b1ed-f2c6-4993-bf62-c15964cec3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# instantiate the pytorch-lightning trainer\n",
    "trainer = pl.Trainer(\n",
    "    gpus=0,  # >= 1 to use GPU(s)\n",
    "    max_epochs=2,\n",
    "    logger=None,\n",
    "    callbacks=[pl.callbacks.ModelCheckpoint(monitor=\"Val/accuracy\", mode=\"max\")]\n",
    ")\n",
    "\n",
    "# fit the model end-to-end\n",
    "trainer.fit(\n",
    "    model=weasel,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f22be-7555-431b-b416-d1b57ef46128",
   "metadata": {},
   "source": [
    "After the training we can call the `Trainer.test` method to check the final performance. \n",
    "The model should achieve a test accuracy of around 0.94."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06a5e6-680f-47e1-bb74-1e97917c59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()\n",
    "# {'accuracy': 0.94, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ef7cc-9c4f-444a-a6a0-5e4bac36c04f",
   "metadata": {},
   "source": [
    "To use the model for inference, you can either use its *predict* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9daafb6-937a-46d4-932d-4fd7296b9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text for the inference\n",
    "text = \"In my head this is like 2 years ago.. Time FLIES\"\n",
    "\n",
    "# Get predictions for the example text\n",
    "predicted_probs, predicted_label = weasel.predict(\n",
    "    tokenizer(text, return_tensors=\"pt\")\n",
    ")\n",
    "\n",
    "# Map predicted int to label\n",
    "weak_labels.int2label[int(predicted_label)]  # HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aabc852-4e91-4d4b-86b3-da1e9eea4f04",
   "metadata": {},
   "source": [
    "Or you can instantiate one of the popular transformers pipelines, providing directly the end-model and the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8aed5-6618-4170-b144-87bd8863e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# modify the id2label mapping of the model\n",
    "weasel.end_model.model.config.id2label = weak_labels.int2label\n",
    "\n",
    "# create transformers pipeline\n",
    "classifier = pipeline(\"text-classification\", model=weasel.end_model.model, tokenizer=tokenizer)\n",
    "\n",
    "# use pipeline for predictions\n",
    "classifier(text)  # [{'label': 'HAM', 'score': 0.6110987663269043}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
