{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020957ce-46fa-4799-8321-716e62900269",
   "metadata": {},
   "source": [
    "# Weak supervision\n",
    "\n",
    "\n",
    "This guide gives you a brief introduction to weak supervision with Rubrix.\n",
    "\n",
    "Rubrix currently supports weak supervision for text classification use cases, but we'll be adding support for token classification (e.g., Named Entity Recognition) soon.\n",
    "\n",
    "\n",
    ".. nbinfo::\n",
    "   This feature is experimental, you can expect some changes in the Python API. Please report on [Github](https://github.com/recognai/rubrix) any issue you encounter.\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "![Labeling workflow](https://raw.githubusercontent.com/recognai/rubrix-materials/main/tutorials/weak_supervision/weak_supervision.svg \"Labeling workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb86995-f158-4392-86b7-eb5305a9ec3b",
   "metadata": {},
   "source": [
    "## Rubrix weak supervision in a nutshell\n",
    "\n",
    "Doing weak supervision with Rubrix should be straightforward. Keeping the same spirit as other parts of the library, you can virtually use any weak supervision library or method, such as Snorkel or Flyingsquid. \n",
    "\n",
    "Rubrix weak supervision support is built around two basic abstractions:\n",
    "\n",
    "\n",
    "### `Rule`\n",
    "A rule encodes an heuristic for labeling a record.\n",
    "\n",
    "Heuristics can be defined using [Elasticsearch's queries](../reference/rubrix_webapp_reference.rst#search-input):\n",
    "\n",
    "```python\n",
    "plz = Rule(query=\"plz OR please\", label=\"SPAM\")\n",
    "```\n",
    "\n",
    "or with Python functions (similar to Snorkel's labeling functions, which you can use as well):\n",
    "\n",
    "```python\n",
    "def contains_http(record: rb.TextClassificationRecord) -> Optional[str]:\n",
    "    if \"http\" in record.inputs[\"text\"]:\n",
    "        return \"SPAM\"\n",
    "```\n",
    "\n",
    "Besides textual features, Python labeling functions can exploit metadata features:\n",
    "\n",
    "```python\n",
    "def author_channel(record: rb.TextClassificationRecord) -> Optional[str]:\n",
    "    # the word channel appears in the comment author name\n",
    "    if \"channel\" in record.metadata[\"author\"]:\n",
    "        return \"SPAM\"\n",
    "```\n",
    "\n",
    "A rule should either return a string value, that is a weak label, or a `None` type in case of abstention.\n",
    "\n",
    "\n",
    "### `Weak Labels`\n",
    "\n",
    "Weak Labels objects bundle and apply a set of rules to the records of a Rubrix dataset. Applying a rule to a record means assigning a weak label or abstaining.\n",
    "\n",
    "This abstraction provides you with the building blocks for training and testing weak supervision \"denoising\", \"label\" or even \"end\" models:\n",
    "\n",
    "```python\n",
    "rules = [contains_http, author_channel]\n",
    "weak_labels = WeakLabels(\n",
    "    rules=rules, \n",
    "    dataset=\"weak_supervision_yt\"\n",
    ")\n",
    "\n",
    "# returns a summary of the applied rules\n",
    "weak_labels.summary()\n",
    "```\n",
    "\n",
    "More information about these abstractions can be found in [the Python Labeling module docs](../reference/python/python_labeling.rst).\n",
    "\n",
    "## Built-in label models\n",
    "\n",
    "To make things even easier for you, we provide wrapper classes around the most common label models, that directly consume a `WeakLabels` object.\n",
    "This makes working with those models a breeze.\n",
    "Take a look at the list of built-in models in the [labeling module docs](../reference/python/python_labeling.rst#rubrix.labeling.text_classification.label_models.LabelModel).\n",
    "\n",
    "\n",
    "## Workflow\n",
    "\n",
    "A typical workflow to use weak supervision is:\n",
    "\n",
    "1. Create a Rubrix dataset with your raw dataset. If you actually have some labelled data you can log it into the the same dataset.\n",
    "2. Define a set of rules, exploring and trying out different things directly in the Rubrix web app.\n",
    "3. Create a `WeakLabels` object and apply the rules. Typically, you'll iterate between this step and step 2.\n",
    "4. Once you are satisfied with your weak labels, use the matrix of the `WeakLabels` instance with your library/method of choice to build a training set or even train a downstream text classification model.\n",
    "\n",
    "\n",
    "This guide shows you an end-to-end example using Snorkel and Flyingsquid. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52dd8f-fb8e-4909-9f6d-0a8856afa999",
   "metadata": {},
   "source": [
    "## Example dataset\n",
    "\n",
    "We'll be using a well-known dataset for weak supervision examples, the [YouTube Spam Collection](http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/) dataset, which is a binary classification task for detecting spam comments in Youtube videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b1e00af-c6f9-42aa-81aa-2976c9591b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Alessandro leite</td>\n",
       "      <td>2014-11-05T22:21:36</td>\n",
       "      <td>pls http://www10.vakinha.com.br/VaquinhaE.aspx...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Salim Tayara</td>\n",
       "      <td>2014-11-02T14:33:30</td>\n",
       "      <td>if your like drones, plz subscribe to Kamal Ta...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Phuc Ly</td>\n",
       "      <td>2014-01-20T15:27:47</td>\n",
       "      <td>go here to check the views :3﻿</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DropShotSk8r</td>\n",
       "      <td>2014-01-19T04:27:18</td>\n",
       "      <td>Came here to check the views, goodbye.﻿</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>css403</td>\n",
       "      <td>2014-11-07T14:25:48</td>\n",
       "      <td>i am 2,126,492,636 viewer :D﻿</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            author                 date  \\\n",
       "0           0  Alessandro leite  2014-11-05T22:21:36   \n",
       "1           1      Salim Tayara  2014-11-02T14:33:30   \n",
       "2           2           Phuc Ly  2014-01-20T15:27:47   \n",
       "3           3      DropShotSk8r  2014-01-19T04:27:18   \n",
       "4           4            css403  2014-11-07T14:25:48   \n",
       "\n",
       "                                                text  label  video  \n",
       "0  pls http://www10.vakinha.com.br/VaquinhaE.aspx...   -1.0      1  \n",
       "1  if your like drones, plz subscribe to Kamal Ta...   -1.0      1  \n",
       "2                     go here to check the views :3﻿   -1.0      1  \n",
       "3            Came here to check the views, goodbye.﻿   -1.0      1  \n",
       "4                      i am 2,126,492,636 viewer :D﻿   -1.0      1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "train_df = pd.read_csv('../tutorials/data/yt_comments_train.csv')\n",
    "test_df = pd.read_csv('../tutorials/data/yt_comments_test.csv')\n",
    "\n",
    "# preview data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fcd253-f8b5-40ba-a9a2-9d068e19c1cc",
   "metadata": {},
   "source": [
    "## 1. Create a Rubrix dataset with unlabelled data and test data\n",
    "\n",
    "Let's load the train (non-labelled) and the test (containing labels) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78571aa4-314d-4b4e-b933-c3292213b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "\n",
    "# build records from the train dataset\n",
    "records = [\n",
    "    rb.TextClassificationRecord(\n",
    "        inputs=row.text,\n",
    "        metadata={\"video\":row.video, \"author\": row.author}\n",
    "    )\n",
    "    for i,row in train_df.iterrows()\n",
    "]\n",
    "\n",
    "# build records from the test dataset\n",
    "labels = [\"HAM\", \"SPAM\"]\n",
    "records += [\n",
    "    rb.TextClassificationRecord(\n",
    "        inputs=row.text,\n",
    "        annotation=labels[row.label],\n",
    "        metadata={\"video\":row.video, \"author\": row.author}\n",
    "    )\n",
    "    for i,row in test_df.iterrows()\n",
    "]\n",
    "\n",
    "# log records to Rubrix\n",
    "rb.log(records, name=\"weak_supervision_yt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafda4f-45c0-49d6-9c37-7473c6888ebe",
   "metadata": {},
   "source": [
    "After this step, you have a fully browsable dataset available at `http://localhost:6900/weak_supervision_yt` (or the base URL where your Rubrix instance is hosted)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde95ce0-6e1e-4c9e-aff2-ac12643b9a48",
   "metadata": {},
   "source": [
    "## 2. Defining rules\n",
    "\n",
    "Let's now define some of the rules proposed in the tutorial [Snorkel Intro Tutorial: Data Labeling](https://www.snorkel.org/use-cases/01-spam-tutorial).\n",
    "\n",
    "\n",
    "Remember you can use [Elasticsearch's query string DSL](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html) and test your queries directly in the web app. Available fields in the query are described in [the Rubrix web app reference](../reference/rubrix_webapp_reference.rst#search-input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045700ef-62b7-44e2-95f9-0f966236303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubrix.labeling.text_classification import Rule, WeakLabels\n",
    "\n",
    "#  rules defined as Elasticsearch queries\n",
    "check_out = Rule(query=\"check out\", label=\"SPAM\")\n",
    "plz = Rule(query=\"plz OR please\", label=\"SPAM\")\n",
    "subscribe = Rule(query=\"subscribe\", label=\"SPAM\")\n",
    "my = Rule(query=\"my\", label=\"SPAM\")\n",
    "song = Rule(query=\"song\", label=\"HAM\")\n",
    "love = Rule(query=\"love\", label=\"HAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f45410-3bb4-450a-8162-5bd8ed5814e8",
   "metadata": {},
   "source": [
    "Besides using the UI, if you want to quickly see the effect of a rule, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d408aa69-ca11-4847-bdb6-d8c1725a78d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'text': 'Thank you. Please give your email. ﻿'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'text': 'HUH HYUCK HYUCK IM SPECIAL WHO&amp;#39;S WATCHING THIS IN 2015 IM FROM AUSTRALIA OR SOMETHING GIVE ME ATTENTION PLEASE IM JUST A RAPPER WITH A DREAM IM GONNA SHARE THIS ON GOOGLE PLUS BECAUSE IM SO COOL.﻿'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'text': 'Media is Evil! Please see and share: W W W. THE FARRELL REPORT. NET  Top Ex UK Police Intelligence Analyst turned Whistleblower Tony Farrell exposes a horrific monstrous cover-up perpetrated by criminals operating crimes from inside Mainstream Entertainment and Media Law firms. Beware protect your children!! These devils brutally target innocent people. These are the real criminals linked to London&amp;#39;s 7/7 attacks 2005.  MUST SEE AND MAKE VIRAL!!! Also see UK Column video on 31st January 2013.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'text': 'hey guys if you guys can please SUBSCRIBE to my channel ,i&amp;#39;m a young rapper really dedicated i post a video everyday ,i post a verse (16 bars)(part of a song)everyday to improve i&amp;#39;m doing this for 365 days ,right now i&amp;#39;m on day 41  i&amp;#39;m doing it for a whole year without missing one day if you guys can please SUBSCRIBE and follow me on my journey to my dream watch me improve, it really means a lot to me  thank you (:, i won&amp;#39;t let you down i promise(: i&amp;#39;m lyrical i keep it real!'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'text': 'Please do buy these new Christmas shirts! You can buy at any time before  December 4th and they are sold worldwide! Don't miss out:  http://teespring.com/treechristmas﻿'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>{'text': 'Please subscribe to us and thank you﻿'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>{'text': 'My honest opinion. It's a very mediocre song. Nothing unique or special  about her music, lyrics or voice. Nothing memorable like Billie Jean or  Beat It. Before her millions of fans reply with hate comments, i know this  is a democracy and people are free to see what they want. But then don't I  have the right to express my opinion? Please don't reply with dumb comments  lie \"if you don't like it don't watch it\". I just came here to see what's  the buzz about(661 million views??) and didn't like what i saw. OK?﻿'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>{'text': 'EVERYONE PLEASE GO SUBSCRIBE TO MY CHANNEL OR JUST LOON AT MY VIDEOS﻿'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>{'text': 'please suscribe i am bored of 5 subscribers try to get it to 20!﻿'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>{'text': 'https://www.facebook.com/eeccon/posts/733949243353321?comment_id=734237113324534&amp;amp;offset=0&amp;amp;total_comments=74   please like frigea marius gabriel comment :D﻿'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 inputs\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {'text': 'Thank you. Please give your email. ﻿'}\n",
       "1                                                                                                                                                                                                                                                                                                                                  {'text': 'HUH HYUCK HYUCK IM SPECIAL WHO&#39;S WATCHING THIS IN 2015 IM FROM AUSTRALIA OR SOMETHING GIVE ME ATTENTION PLEASE IM JUST A RAPPER WITH A DREAM IM GONNA SHARE THIS ON GOOGLE PLUS BECAUSE IM SO COOL.﻿'}\n",
       "2                      {'text': 'Media is Evil! Please see and share: W W W. THE FARRELL REPORT. NET  Top Ex UK Police Intelligence Analyst turned Whistleblower Tony Farrell exposes a horrific monstrous cover-up perpetrated by criminals operating crimes from inside Mainstream Entertainment and Media Law firms. Beware protect your children!! These devils brutally target innocent people. These are the real criminals linked to London&#39;s 7/7 attacks 2005.  MUST SEE AND MAKE VIRAL!!! Also see UK Column video on 31st January 2013.'}\n",
       "3                  {'text': 'hey guys if you guys can please SUBSCRIBE to my channel ,i&#39;m a young rapper really dedicated i post a video everyday ,i post a verse (16 bars)(part of a song)everyday to improve i&#39;m doing this for 365 days ,right now i&#39;m on day 41  i&#39;m doing it for a whole year without missing one day if you guys can please SUBSCRIBE and follow me on my journey to my dream watch me improve, it really means a lot to me  thank you (:, i won&#39;t let you down i promise(: i&#39;m lyrical i keep it real!'}\n",
       "4                                                                                                                                                                                                                                                                                                                                                                  {'text': 'Please do buy these new Christmas shirts! You can buy at any time before  December 4th and they are sold worldwide! Don't miss out:  http://teespring.com/treechristmas﻿'}\n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ...\n",
       "181                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'text': 'Please subscribe to us and thank you﻿'}\n",
       "182  {'text': 'My honest opinion. It's a very mediocre song. Nothing unique or special  about her music, lyrics or voice. Nothing memorable like Billie Jean or  Beat It. Before her millions of fans reply with hate comments, i know this  is a democracy and people are free to see what they want. But then don't I  have the right to express my opinion? Please don't reply with dumb comments  lie \"if you don't like it don't watch it\". I just came here to see what's  the buzz about(661 million views??) and didn't like what i saw. OK?﻿'}\n",
       "183                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'text': 'EVERYONE PLEASE GO SUBSCRIBE TO MY CHANNEL OR JUST LOON AT MY VIDEOS﻿'}\n",
       "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'text': 'please suscribe i am bored of 5 subscribers try to get it to 20!﻿'}\n",
       "185                                                                                                                                                                                                                                                                                                                                                                     {'text': 'https://www.facebook.com/eeccon/posts/733949243353321?comment_id=734237113324534&amp;offset=0&amp;total_comments=74   please like frigea marius gabriel comment :D﻿'}\n",
       "\n",
       "[186 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display full length text\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# get the subset for the rule query\n",
    "rb.load(name=\"weak_supervision_yt\", query=\"plz OR please\")[['inputs']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d4d7f-9a95-4905-a38b-3907d5293c4f",
   "metadata": {},
   "source": [
    "You can also define plain Python labeling functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b3554c-9646-4f2b-ab33-13518566bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# rules defined as Python labeling functions\n",
    "def contains_http(record: rb.TextClassificationRecord):\n",
    "    if \"http\" in record.inputs[\"text\"]:\n",
    "        return \"SPAM\"\n",
    "\n",
    "def short_comment(record: rb.TextClassificationRecord):\n",
    "    return \"HAM\" if len(record.inputs[\"text\"].split()) < 5 else None\n",
    "\n",
    "def regex_check_out(record: rb.TextClassificationRecord):\n",
    "    return \"SPAM\" if re.search(r\"check.*out\", record.inputs[\"text\"], flags=re.I) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c93ec-b136-43cf-af50-96c956b65f12",
   "metadata": {},
   "source": [
    "## 3. Building and analizing weak labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf60f4-c5f5-492d-ac0d-52a25cfb5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle our rules in a list\n",
    "rules = [check_out, plz, subscribe, my, song, love, contains_http, short_comment, regex_check_out]\n",
    "\n",
    "# apply the rules to a dataset to obtain the weak labels\n",
    "weak_labels = WeakLabels(\n",
    "    rules=rules, \n",
    "    dataset=\"weak_supervision_yt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0584b8b-4b0d-4857-9e8b-9823d9172634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>coverage</th>\n",
       "      <th>overlaps</th>\n",
       "      <th>conflicts</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check out</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.235379</td>\n",
       "      <td>0.229147</td>\n",
       "      <td>0.028763</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plz OR please</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.089166</td>\n",
       "      <td>0.079099</td>\n",
       "      <td>0.019175</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subscribe</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.108341</td>\n",
       "      <td>0.084372</td>\n",
       "      <td>0.028763</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.190316</td>\n",
       "      <td>0.167306</td>\n",
       "      <td>0.050815</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>0.872340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>song</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.139981</td>\n",
       "      <td>0.085331</td>\n",
       "      <td>0.034995</td>\n",
       "      <td>78</td>\n",
       "      <td>18</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.097795</td>\n",
       "      <td>0.075743</td>\n",
       "      <td>0.032119</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_http</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.096357</td>\n",
       "      <td>0.066155</td>\n",
       "      <td>0.045062</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>{HAM}</td>\n",
       "      <td>0.259827</td>\n",
       "      <td>0.113135</td>\n",
       "      <td>0.058965</td>\n",
       "      <td>168</td>\n",
       "      <td>16</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>{SPAM}</td>\n",
       "      <td>0.220997</td>\n",
       "      <td>0.220518</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>{SPAM, HAM}</td>\n",
       "      <td>0.764621</td>\n",
       "      <td>0.447267</td>\n",
       "      <td>0.116970</td>\n",
       "      <td>676</td>\n",
       "      <td>60</td>\n",
       "      <td>0.918478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    polarity  coverage  overlaps  conflicts  correct  \\\n",
       "check out             {SPAM}  0.235379  0.229147   0.028763       90   \n",
       "plz OR please         {SPAM}  0.089166  0.079099   0.019175       40   \n",
       "subscribe             {SPAM}  0.108341  0.084372   0.028763       60   \n",
       "my                    {SPAM}  0.190316  0.167306   0.050815       82   \n",
       "song                   {HAM}  0.139981  0.085331   0.034995       78   \n",
       "love                   {HAM}  0.097795  0.075743   0.032119       56   \n",
       "contains_http         {SPAM}  0.096357  0.066155   0.045062       12   \n",
       "short_comment          {HAM}  0.259827  0.113135   0.058965      168   \n",
       "regex_check_out       {SPAM}  0.220997  0.220518   0.026846       90   \n",
       "total            {SPAM, HAM}  0.764621  0.447267   0.116970      676   \n",
       "\n",
       "                 incorrect  precision  \n",
       "check out                0   1.000000  \n",
       "plz OR please            0   1.000000  \n",
       "subscribe                0   1.000000  \n",
       "my                      12   0.872340  \n",
       "song                    18   0.812500  \n",
       "love                    14   0.800000  \n",
       "contains_http            0   1.000000  \n",
       "short_comment           16   0.913043  \n",
       "regex_check_out          0   1.000000  \n",
       "total                   60   0.918478  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show some stats about the rules, see the `summary()` docstring for details\n",
    "weak_labels.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4364c17-4a5d-464b-8f2b-36f50c81b174",
   "metadata": {},
   "source": [
    "## 4. Using the weak labels\n",
    "\n",
    "At this step you have at least two options:\n",
    "\n",
    "1. Use the weak labels for training a \"denoising\" or label model to build a less noisy training set. Highly popular options for this are [Snorkel](https://snorkel.org/) or [Flyingsquid](https://github.com/HazyResearch/flyingsquid). After this step, you can train a downstream model with the \"clean\" labels.\n",
    "\n",
    "2. Use the weak labels directly with recent \"end-to-end\" (e.g., [Weasel](https://github.com/autonlab/weasel)) or joint models (e.g., [COSINE](https://github.com/yueyu1030/COSINE)).\n",
    "\n",
    "\n",
    "Let's see some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186a612-57a4-4118-97fb-927ed87df96d",
   "metadata": {},
   "source": [
    "### Label model with Snorkel\n",
    "\n",
    "Snorkel is by far the most popular option for using weak supervision, and Rubrix provides built-in support for it. \n",
    "Using Snorkel with Rubrix's `WeakLabels` is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af30dfa-b401-4e96-984d-c59753411557",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install snorkel -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb50bb-edfb-40a2-9e4c-094e22284df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubrix.labeling.text_classification import Snorkel\n",
    "\n",
    "# we pass our WeakLabels instance to our Snorkel label model\n",
    "label_model = Snorkel(weak_labels)\n",
    "\n",
    "# we train the model\n",
    "label_model.fit()\n",
    "\n",
    "# we check its performance\n",
    "label_model.score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075463f3-8695-4d28-af24-8860b2efd691",
   "metadata": {},
   "source": [
    "After fitting your label model, you can quickly explore its predictions, before building a training set for training a downstream text classifier. \n",
    "\n",
    "This step is useful for validation, manual revision, or defining score thresholds for accepting labels from your label model (for example, only considering labels with a score greater then 0.8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3fd76-8b58-4ae3-bc58-9d82c039bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your training records with the predictions of the label model\n",
    "records_for_training = label_model.predict()\n",
    "\n",
    "# log the records to a new dataset in Rubrix\n",
    "rb.log(records_for_training, name=\"snorkel_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f69fc-7f2f-4b57-8262-a0a6c2f92329",
   "metadata": {},
   "source": [
    "### Label model with FlyingSquid\n",
    "\n",
    "FlyingSquid is a powerful method developed by [Hazy Research](https://hazyresearch.stanford.edu/), a research group from Stanford behind ground-breaking work on programmatic data labeling, including Snorkel. FlyingSquid uses a closed-form solution for fitting the label model with great speed gains and similar performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55482c-02f6-4e7a-8637-0aa50e4e9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flyingsquid pgmpy -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8629a-6eff-4620-b116-50abd0b4ee2b",
   "metadata": {},
   "source": [
    "By default, the `WeakLabels` class uses `-1` as value for an abstention. \n",
    "FlyingSquid, though, expects a value of `0`.\n",
    "With Rubrix you can define a custom `label2int` mapping like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c911f2-b9e5-42cb-8d52-1e5f796a782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_labels = WeakLabels(rules=rules, dataset=\"weak_supervision_yt\", label2int={None: 0, 'SPAM': -1, 'HAM': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ede78a-0fe6-4f2b-87b1-3d0f06f0c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flyingsquid.label_model import LabelModel\n",
    "\n",
    "# train our label model\n",
    "label_model = LabelModel(len(weak_labels.rules))\n",
    "label_model.fit(L_train=weak_labels.matrix(has_annotation=False),verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282da18-cc57-437a-bfd1-c13a9ac1aec4",
   "metadata": {},
   "source": [
    "After fitting your label model, you can quickly explore its predictions, before building a training set for training a downstream text classifier. \n",
    "\n",
    "This step is useful for validation, manual revision, or defining score thresholds for accepting labels from your label model (for example, only considering labels with a score greater then 0.8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff30478c-fdda-445f-ad49-cac01b8bde71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the part of the weak label matrix that has no corresponding annotation\n",
    "train_matrix = weak_labels.matrix(has_annotation=False)\n",
    "\n",
    "# get predictions from our label model\n",
    "predictions = label_model.predict_proba(L_matrix=train_matrix)\n",
    "predicted_labels = label_model.predict(L_matrix=train_matrix)\n",
    "preds = [[('SPAM', pred[0]), ('HAM', pred[1])] for pred in predictions]\n",
    "\n",
    "# get the records that do not have an annotation\n",
    "train_records = weak_labels.records(has_annotation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17e492-0c4f-40ea-833f-67753b4a2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the predictions to the records\n",
    "def add_prediction(record, prediction):\n",
    "    record.prediction = prediction\n",
    "    return record\n",
    "\n",
    "train_records_with_lm_prediction = [\n",
    "    add_prediction(rec, pred)\n",
    "    for rec, pred, label in zip(train_records, preds, predicted_labels)\n",
    "    if label != weak_labels.label2int[None] # exclude records where the label model abstains\n",
    "]\n",
    "\n",
    "# log a new dataset to Rubrix\n",
    "rb.log(train_records_with_lm_prediction, name=\"flyingsquid_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cec1b4-97c8-45a0-9fc4-06e663b9ee4d",
   "metadata": {},
   "source": [
    "### Joint Model with Weasel\n",
    "\n",
    "[Weasel](https://github.com/autonlab/weasel) lets you train downstream models end-to-end using directly weak labels.\n",
    "In contrast to Snorkel or FlyingSquid, which are two-stage approaches, Weasel is a one-stage method that jointly trains the label and the end model at the same time.\n",
    "For more details check out the [End-to-End Weak Supervision paper](https://arxiv.org/abs/2107.02233) presented at NeurIPS 2021.\n",
    "\n",
    "In this guide we will show you, how you can **train a Hugging Face transformers** model directly **with weak labels using Weasel**.\n",
    "Since Weasel uses [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) for the training, some basic knowledge of PyTorch is helpful, but not strictly necessary.\n",
    "\n",
    "First, we need to install the Weasel python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac1d61-5a40-44dc-a340-46bbdb8852eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install git+https://github.com/autonlab/weasel#egg=weasel[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4f949-71d9-4200-b37f-42c709427228",
   "metadata": {},
   "source": [
    "Before we get started, we need to define some classes, that wrap our data and our end model in a way Weasel can work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28afb473-217d-4b94-be9e-ad8cab1ce9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weasel.datamodules.base_datamodule import AbstractWeaselDataset, AbstractDownstreamDataset\n",
    "from weasel.models.downstream_models.base_model import DownstreamBaseModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "class TrainDataset(AbstractWeaselDataset):\n",
    "    def __init__(self, L, inputs):\n",
    "        super().__init__(L, None)\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        if self.L.shape[0] != len(self.inputs):\n",
    "            raise ValueError(\"L and inputs have different number of samples\")\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return self.L[item], self.inputs[item]\n",
    "    \n",
    "\n",
    "class TestDataset(AbstractDownstreamDataset):\n",
    "    def __init__(self, inputs, Y):\n",
    "        super().__init__(None, Y)\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        if len(self.Y) != len(self.inputs):\n",
    "            raise ValueError(\"inputs and Y have different number of samples\")\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return self.inputs[item], self.Y[item]\n",
    "    \n",
    "class TrainCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self._tokenizer = tokenizer\n",
    "    def __call__(self, batch):\n",
    "        L = torch.stack([b[0] for b in batch])\n",
    "        inputs = {key: [b[1][key] for b in batch] for key in batch[0][1]}\n",
    "        return L, self._tokenizer.pad(inputs, return_tensors=\"pt\")\n",
    "\n",
    "    \n",
    "class TestCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self._tokenizer = tokenizer\n",
    "    def __call__(self, batch):\n",
    "        Y = torch.stack([b[1] for b in batch])\n",
    "        inputs = {key: [b[0][key] for b in batch] for key in batch[0][0]}\n",
    "        return self._tokenizer.pad(inputs, return_tensors=\"pt\"), Y\n",
    "\n",
    "    \n",
    "class TransformersEndModel(DownstreamBaseModel):\n",
    "    def __init__(self, name: str, num_labels: int = 2):\n",
    "        super().__init__()\n",
    "        self.out_dim = num_labels\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(name, num_labels=num_labels)\n",
    "        \n",
    "    def forward(self, kwargs):\n",
    "        model_output = self.model(**kwargs)\n",
    "        return model_output[\"logits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffab015-2204-4986-b415-98ba95228736",
   "metadata": {},
   "source": [
    "The first step is to obtain our weak labels.\n",
    "For this we use the same rules and data set as in the examples above (Snorkel and FlyingSquid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17f25f-a4c2-47a3-b8b2-ba9bf1966b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain our weak labels\n",
    "weak_labels = WeakLabels(\n",
    "    rules=rules, \n",
    "    dataset=\"weak_supervision_yt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6273d8-5765-4979-b3eb-3b648b789796",
   "metadata": {},
   "source": [
    "In a second step we instantiate our end model, which in our case will be a pre-trained transformer from the Hugging Face Hub.\n",
    "Here we choose the small ELECTRA model by Google that shows excellent performance given its moderate number of parameters.\n",
    "Due to its size, you can fine-tune it on your CPU within a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598eac51-5ae9-4d16-aeca-df8c10a00f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate our transformers end model\n",
    "end_model = TransformersEndModel(\"google/electra-small-discriminator\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcec749-929f-439b-969a-620380319363",
   "metadata": {},
   "source": [
    "With our end-model at hand, we can now instantiate the Weasel model.\n",
    "Apart from the end-model, it also includes a neural encoder that tries to estimate latent labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8cc63-aed4-490d-9930-72e474c8836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weasel.models import Weasel\n",
    "\n",
    "# instantiate our weasel end-to-end model\n",
    "weasel = Weasel(\n",
    "    end_model=end_model,\n",
    "    num_LFs=len(weak_labels.rules),\n",
    "    n_classes=2,\n",
    "    encoder={'hidden_dims': [32, 10]},\n",
    "    optim_encoder={'name': 'adam', 'lr': 1e-4},\n",
    "    optim_end_model={'name': 'adam', 'lr': 5e-5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d48a81-2a0c-4958-9b49-093c48beb0d5",
   "metadata": {},
   "source": [
    "Afterwards, we wrap our data in torch `Dataset`s and `DataLoader`s, so that Weasel and PyTorch Lightning can work with it.\n",
    "In this step we also tokenize the data. \n",
    "Here we need to be careful to use the corresponding tokenizer to our end model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d938d9-7aae-4882-a4b3-6877a1e3226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer for our transformers end model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "# torch data set of our training data\n",
    "train_ds = TrainDataset(\n",
    "    L=weak_labels.matrix(has_annotation=False), \n",
    "    inputs=[tokenizer(rec.inputs[\"text\"], truncation=True) \n",
    "          for rec in weak_labels.records(has_annotation=False)], \n",
    ")\n",
    "\n",
    "# torch data set of our test data \n",
    "test_ds = TestDataset(\n",
    "    inputs=[tokenizer(rec.inputs[\"text\"], truncation=True)\n",
    "          for rec in weak_labels.records(has_annotation=True)],\n",
    "    Y=weak_labels.annotation(),\n",
    ")\n",
    "\n",
    "# torch data loader for our training data\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    collate_fn=TrainCollator(tokenizer),\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "# torch data loader for our test data\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    collate_fn=TestCollator(tokenizer),\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b35f33-a9e1-4289-84d5-bcc1d4c1c200",
   "metadata": {},
   "source": [
    "Now we have everything ready to start the training of our Weasel model.\n",
    "For the training process, Weasel relies on the excellent [PyTorch Lightning Trainer](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html).\n",
    "It provides tons of options and features to optimize the training process, but the defaults below should give you reasonable results.\n",
    "Keep in mind that you are fine-tuning a full-blown transformer model, albeit a small one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90b1ed-f2c6-4993-bf62-c15964cec3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# instantiate the pytorch-lightning trainer\n",
    "trainer = pl.Trainer(\n",
    "    gpus=0,  # >= 1 to use GPU(s)\n",
    "    max_epochs=2,\n",
    "    logger=None,\n",
    "    callbacks=[pl.callbacks.ModelCheckpoint(monitor=\"Val/accuracy\", mode=\"max\")]\n",
    ")\n",
    "\n",
    "# fit the model end-to-end\n",
    "trainer.fit(\n",
    "    model=weasel, \n",
    "    train_dataloaders=train_loader, \n",
    "    val_dataloaders=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f22be-7555-431b-b416-d1b57ef46128",
   "metadata": {},
   "source": [
    "After the training we can call the `Trainer.test` method to check the final performance. \n",
    "The model should have achieved an accuracy of around 0.94."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06a5e6-680f-47e1-bb74-1e97917c59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(dataloaders=test_loader)  # List of test metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ef7cc-9c4f-444a-a6a0-5e4bac36c04f",
   "metadata": {},
   "source": [
    "To use the model for inference, you can either use its *predict* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9daafb6-937a-46d4-932d-4fd7296b9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text for the inference\n",
    "text = \"In my head this is like 2 years ago.. Time FLIES\"\n",
    "\n",
    "# Get predictions for the example text\n",
    "predicted_probs, predicted_label = weasel.predict(\n",
    "    tokenizer(text, return_tensors=\"pt\")\n",
    ")\n",
    "\n",
    "# Map predicted int to label\n",
    "weak_labels.int2label[int(predicted_label)]  # HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aabc852-4e91-4d4b-86b3-da1e9eea4f04",
   "metadata": {},
   "source": [
    "Or you can instantiate one of the popular transformers pipelines, providing directly the end-model and the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8aed5-6618-4170-b144-87bd8863e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# modify the id2label mapping of the model\n",
    "weasel.end_model.model.config.id2label = weak_labels.int2label\n",
    "\n",
    "# create transformers pipeline\n",
    "classifier = pipeline(\"text-classification\", model=weasel.end_model.model, tokenizer=tokenizer)\n",
    "\n",
    "# use pipeline for predictions\n",
    "classifier(text)  # [{'label': 'HAM', 'score': 0.6110987663269043}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
