{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate new data to improve NLP models using Rubrix and biome.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Hey there! In this guide, we will show you how to use Rubrix to annotate new data and use this new data to improve existing Deep Learning models. Our use case will be Automatic Misogyny Detection (AMI): Deep Learning models able to detect the underlying misogyny on a given text. Ground-breaking work is being made every year on this subject, with shared tasks and new models that push the performance of these models closer and closer to be implemented in apps, social networks, and other digital environments. \n",
    "\n",
    "To train these NLP models we are going to use [biome.text](https://github.com/recognai/biome-text), an open-source library to train models with a simple workflow. Rubrix is compatible with almost any library or service, so we will work back and forth with both of them. \n",
    "\n",
    "The data used to feed the models and make the annotations comes from the [IberEval 2018](https://sites.google.com/view/ibereval-2…) shared task. It's a compilation of tweets, analyzed by experts and classified into 5 different misogyny categories. We are also making the specific datasets used in each step of this guide available, so they can be reproduced in the best way possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "If you want to reproduce this code, make sure that all the libraries needed to run this guide are installed and imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U git+https://github.com/recognai/biome-text\n",
    "%pip install rubrix\n",
    "%pip install pandas\n",
    "exit(0)  # Force restart of the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:erase\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_API_KEY'] = '7bd265df21100baa9767bb9f69108bc417db4b4a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignaciotalaveracepeda/anaconda3/envs/rubrix/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from biome.text import *\n",
    "import pandas as pd\n",
    "import rubrix as rb\n",
    "\n",
    "#TODO: erase\n",
    "from biome.text import *\n",
    "from biome.text.hpo import TuneExperiment\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray import tune\n",
    "import math\n",
    "\n",
    "import wandb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets\n",
    "\n",
    "Let's load some prepared datasets we've made to quickly train our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets\n",
    "training_ds = Dataset.from_csv('annotation_data/training_full_df.csv')\n",
    "test_ds = Dataset.from_csv('annotation_data/test_df.csv')\n",
    "\n",
    "# TODO: remove el mapeo, hacer que se guarde bien\n",
    "# Removing non-useful generated columns\n",
    "training_ds = training_ds.map(remove_columns=[\"Unnamed: 0\", \"id\"])\n",
    "test_ds = test_ds.map(remove_columns=[\"Unnamed: 0\", \"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the first model\n",
    "\n",
    "Creating NLP pipelines with biome.text is quick and convenient! We performed an HPO process on the background, to find suitable hyperparameters for this domain, so let's use them to create our first AMI model. Note that we're making a pipeline with BETO, a Spanish Transformer model, at the head. To learn more about what a Transformer is, please visit the [Transformer guide of biome.text](https://recognai.github.io/biome-text/v3.0.0/documentation/tutorials/4-Using_Transformers_in_biome_text.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {\n",
    "    \"name\": \"AMI_first_model\",\n",
    "    \"features\": {\n",
    "        \"transformers\": {\n",
    "            \"model_name\": \"dccuchile/bert-base-spanish-wwm-cased\", # BETO model\n",
    "            \"trainable\": True,\n",
    "            \"max_length\": 280,  # As we are working with data from Twitter, this is our max length\n",
    "        }\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"type\": \"TextClassification\",\n",
    "        \n",
    "        # These are the possible misogyny categories. 0 indicates it is non-sexist\n",
    "        \"labels\": [\n",
    "            'sexual_harassment',\n",
    "             'dominance',\n",
    "             'discredit',\n",
    "             'stereotype',\n",
    "             'derailing',\n",
    "             'passive',\n",
    "             'active',\n",
    "             '0'\n",
    "        ],\n",
    "        \"pooler\": {\n",
    "            \"type\": \"lstm\",\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_size\": 256,\n",
    "            \"bidirectional\": True,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "pl = Pipeline.from_config(pipeline_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "trainer_dict = {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"adamw\",\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-4),\n",
    "        \"weight_decay\": tune.loguniform(2e-3, 6e-2 )\n",
    "    },\n",
    "    \"learning_rate_scheduler\": {\n",
    "        \"type\": \"linear_with_warmup\",\n",
    "        \"num_epochs\": 10,\n",
    "        \"num_steps_per_epoch\": int(math.floor(len(training_ds)/batch_size)),\n",
    "        \"warmup_steps\": 100,\n",
    "    },\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": 10,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfiguration(\n",
    "    optimizer={\n",
    "        \"type\": \"adamw\",\n",
    "        \"lr\": 0.000023636840436059507,\n",
    "        \"weight_decay\": 0.01438297700463013,\n",
    "    },\n",
    "    batch_size=8,\n",
    "    max_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    pipeline=pl,\n",
    "    train_dataset=training_ds,\n",
    "    valid_dataset=test_ds,\n",
    "    trainer_config=trainer_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `trainer.fit()` stops, the results of the training and the obtained model will be in the output folder. Nevertheless, we know that this training can long on non-dedicated machines, so we also provide the obtained model to download and import. If you don't want to manually train the model, run the cell below, which downloads and imports the trained model into a biome pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:Descargar e importar código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#TODO: eliminar\n",
    "pl = Pipeline.from_pretrained(\"model_annotation_guide.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make some predictions, and take a look at the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['discredit',\n",
       "  'derailing',\n",
       "  '0',\n",
       "  'stereotype',\n",
       "  'dominance',\n",
       "  'sexual_harassment',\n",
       "  'passive',\n",
       "  'active'],\n",
       " 'probabilities': [0.7752683162689209,\n",
       "  0.10697121918201447,\n",
       "  0.056650321930646896,\n",
       "  0.039366547018289566,\n",
       "  0.012474359944462776,\n",
       "  0.005319079849869013,\n",
       "  0.0026962445117533207,\n",
       "  0.001253939582966268]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.predict(\"Las mujeres no deberían tener derecho a voto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating as a single agent\n",
    "\n",
    "When we said that we prepared some datasets with the tweets from IberEval 2018, we might have lied a little bit. We prepared the datasets with almost all tweets from IberEval 2018, but we also make a small compilation of 15 instances for you to start annotating. Picture you, after training your first model, trying to push a little bit its performance, or include some new data to cover as many different cases as possible. You came across new instances, and you want to annotate them and include them in a follow-up training. This is where Rubrix comes along. \n",
    "\n",
    "In this chapter of the guide we will show you how to:\n",
    "* Import datasets to Rubrix (in our case, from a csv file).\n",
    "* Annotate datasets using Rubrix.\n",
    "* Export the annotated datasets to use them in your pipelines.\n",
    "\n",
    "And we will cover the scenario of a single annotation agent. In the next chapter, we will give you some insight on how to annotate in teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging a dataset into Rubrix\n",
    "\n",
    "Let's start by logging the dataset into a Rubrix dataset. As these instances were initially annotated by the IberEval team, we can treat them as predictions. In the annotation process, therefore, we will decide if we agree with those predictions or not. If we take raw data, we wouldn't have these predictions to support our annotation process, but that's okay too!\n",
    "\n",
    "The first step is to download the datasets. Then, we will iterate through all the instances, logging them into Rubrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-28845c1d061ee336\n",
      "Reusing dataset csv (/Users/ignaciotalaveracepeda/.cache/huggingface/datasets/csv/default-28845c1d061ee336/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "#TODO: download to_annotate.csv\n",
    "\n",
    "annotation_ds = Dataset.from_csv('annotation_data/to_annotate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='annotation_misogyny_celia', processed=15, failed=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = []    # here we will store the TextClassificationRecord objects\n",
    "\n",
    "# Possible labels, used to build the predictions\n",
    "labels = ['sexual_harassment','dominance','discredit','stereotype','derailing','0']\n",
    "\n",
    "for record in annotation_ds:\n",
    "\n",
    "    # Prediction list of each record in the dataset\n",
    "    predictions = []\n",
    "\n",
    "    # We build the prediction list with tuples.\n",
    "    for label in labels:\n",
    "\n",
    "        # If the label is the one predicted in the dataset, it has a score of 1\n",
    "        if label==record[\"label\"]:\n",
    "            pred = (label, 1)\n",
    "            predictions.append(pred)\n",
    "\n",
    "        # Else, it has a score of 0\n",
    "        else:\n",
    "            pred = (label, 0)\n",
    "            predictions.append(pred)\n",
    "\n",
    "    # Appending the record into the list\n",
    "    records.append(rb.TextClassificationRecord(\n",
    "        id=record[\"id\"],\n",
    "        inputs=record[\"text\"],\n",
    "        prediction=predictions,\n",
    "        prediction_agent=\"IberEval 2018\",\n",
    "        metadata={'id': record[\"id\"]},\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Logging the records into Rubrix\n",
    "rb.log(records=records, name=\"annotation_misogyny_celia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotating in the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've logged our annotation dataset into Rubrix, we can start annotating on the UI. We know that first times can be challenging, so here we have some instructions and a GIF to show you around.\n",
    "\n",
    "1. Open Rubrix in your browser. If you're running it locally, it is usually running on [http://localhost:6900](http://localhost:6900).\n",
    "2. Select the `annotation_misogyny` dataset.\n",
    "3. On the upper-right corner, toggle the `Annotation mode`. \n",
    "4. Start selecting the categories that you think fit the input text. If you don't know Spanish, don't worry! 15 instances are not going to change the final model that much, and you will still learn how to annotate.\n",
    "5. For each instance you can annotate a category by pressing it, discarding the record (if you think it does not fit the problem domain), or leave it without an annotation.\n",
    "\n",
    "![Example of Annotation](https://imgur.com/e2ntn5c.gif)\n",
    "\n",
    "If you're wondering why we annotated that instance as 'non-sexist' is because we are trying to make a model capable to differentiate if the input text is being misogynistic or if it is talking about something misogynistic that happened. This second case is considered non-sexist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We have annotated a dataset as a single annotator, and these new data can be used to retrain and fine-tune our NLP model. To save the annotated dataset, just run the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_final = rb.load(\"annotation_misogyny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating with multiple agents\n",
    "\n",
    "There will be times in which you won't annotate your data alone. Usually, it comes down to two reasons: there is a lot of data, or you want a certain grade of agreement in your annotations. When annotating sexist tweets, for example, it is crucial that you try to encode as few biases as possible. How can you fight against that? Well, one of the most effective ways to do it is by gathering a diverse, multicultural team to annotate. \n",
    "\n",
    "In this case, we need a way to merge several annotations of the same instance into one, preserving the will of the majority, and that's when the Inter-Annotator Agreement (IAA) comes in handy. There many different types of IAAs, some based on rules and others based on statistics. For our case, we want to show you how to implement a simple, rule-based IAA. Once you know how to merge annotations from different sources, you can do it the way you want, with more complex rules or using statistic indicators like Cohen's kappa.\n",
    "\n",
    "Our simple rule system will consist on these few rules for three annotators:\n",
    "* For an instance to be annotated with a category, there must be consensus of, at least, two annotators.\n",
    "* If there's consensus in an sexism category, and other annotators find there's no sexism in the instance, it will be discarded.\n",
    "\n",
    "We have loaded and annotated three datasets, one for each of our annotators, called `annotation_misogyn_anna`, `annotation_misogyn_bob` and `annotation_misogyn_celia`. Each one of them have annotated their instances, and now it's time to merge them into a single, annotated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the annotated datasets\n",
    "\n",
    "Once our agents have done their annotations, we should start by saving those datasets, extracting them from Rubrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_anna = rb.load(\"annotation_misogyny_anna\").set_index(\"id\").sort_index()\n",
    "\n",
    "annotation_bob = rb.load(\"annotation_misogyny_bob\").set_index(\"id\").sort_index()\n",
    "\n",
    "annotation_celia = rb.load(\"annotation_misogyny_celia\").set_index(\"id\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rb.load()`returns a Pandas Dataframe. We will use this library to merge our annotations into a single dataset.\n",
    "\n",
    "Now, let's create the DataFrame that will hold our merged annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_final = pd.DataFrame(columns=['id','text', 'prediction', 'prediction_agent', 'annotation', 'annotation_agent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we are going to make our main loop. In it, we will extract the annotations made by the three different annotators, and apply the rules we discussed before. We also added two new statuses, `non-annotated` if no annotator has annotated the record, and `no-consensus` if no category could be annotated, according to our rules. Take into consideration these special categories when reintroducing the dataset into a training pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this tool to count ocurrences in list\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through the datasets, all of them has the same length\n",
    "for i in range(len(annotation_anna)):\n",
    "    \n",
    "    # Extracting the annotated categories by each annotator\n",
    "    category_annotated_anna = annotation_anna.iloc[i][\"annotation\"]\n",
    "    category_annotated_bob = annotation_bob.iloc[i][\"annotation\"]\n",
    "    category_annotated_celia = annotation_celia.iloc[i][\"annotation\"]\n",
    "    \n",
    "    # Merging the annotations into a list\n",
    "    annotated_categories = [category_annotated_anna, category_annotated_bob, category_annotated_celia]\n",
    "    # Flattening the list (if there is annotation, it is saved as an individual list)\n",
    "    if not None in annotated_categories:\n",
    "        annotated_categories = [item for sublist in annotated_categories for item in sublist] \n",
    "    \n",
    "    # If all the elements in the list are None, we can return 'non-annotated'\n",
    "    if all(annotation is None for annotation in annotated_categories):\n",
    "        merged_annotation = 'non-annotated'    \n",
    "    \n",
    "    # Counting the annotations\n",
    "    counted_annotations = Counter(annotated_categories)\n",
    "    \n",
    "    # Checking if the element with the most number of annotations follows the rules to be annotated\n",
    "    if counted_annotations[max(counted_annotations, key=counted_annotations.get)] >= 2 and \"0\" not in counted_annotations:\n",
    "        merged_annotation = max(counted_annotations, key=counted_annotations.get)\n",
    "        \n",
    "    else:\n",
    "        merged_annotation = 'no-consensus'\n",
    "        \n",
    "        \n",
    "    # As all elements in each row of the DataFrame except the annotations are the same, we can\n",
    "    # retrieve information from any of the annotators. In our case is Anna.\n",
    "    annotation_final = annotation_final.append({\n",
    "        'id': annotation_anna.iloc[i][\"metadata\"][\"id\"],\n",
    "        'text': annotation_anna.iloc[i][\"inputs\"][\"text\"],\n",
    "        'prediction': annotation_anna.iloc[i][\"prediction\"],\n",
    "        'prediction_agent': annotation_anna.iloc[i][\"prediction_agent\"],\n",
    "        'annotation': merged_annotation,\n",
    "        'annotation_agent': 'Anna, Bob and Celia',\n",
    "    }, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
